{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "10rFQ6qgEfwG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10rFQ6qgEfwG",
    "outputId": "f1eb8986-5c3b-4ae1-8c9a-767bffd653ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "JxgMNAW8EjHe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxgMNAW8EjHe",
    "outputId": "996ef455-5621-4268-8359-7e2101546f00"
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "emet_2xgEjLL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emet_2xgEjLL",
    "outputId": "88cc1f0e-389f-48c4-d2ba-6eaccc5580b7"
   },
   "outputs": [],
   "source": [
    "# pip install podium-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "1a8d4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "VDPtYtvQElF0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDPtYtvQElF0",
    "outputId": "ee94560a-7cb6-440a-a027-cac624e26a64"
   },
   "outputs": [],
   "source": [
    "# pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "PCRcR-b_ExaN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCRcR-b_ExaN",
    "outputId": "7d142e0b-73c2-4433-dd65-1dbbdf71d0c5"
   },
   "outputs": [],
   "source": [
    "# pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "37c2bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wordsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "463d0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ekphrasis -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "d7b9ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install contextualSpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "77de93e4",
   "metadata": {
    "id": "77de93e4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bertopic import BERTopic\n",
    "from podium import Vocab, Field, LabelField\n",
    "from podium.datasets import TabularDataset\n",
    "from podium.vectorizers import GloVe\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from copy import deepcopy\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from time import time\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "\n",
    "import contextualSpellCheck\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import emoji\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "EnG-1KsiOp7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnG-1KsiOp7d",
    "outputId": "9668126a-955e-45f9-8a47-810b3809b270"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "NgNYHv8MPOgK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgNYHv8MPOgK",
    "outputId": "8b773729-8c76-4b84-82d4-238cf443f712"
   },
   "outputs": [],
   "source": [
    "# %cd drive/MyDrive/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfb886",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "548a72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "08931560",
   "metadata": {
    "id": "08931560"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train/SemEval2018-T3-train-taskA.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')\n",
    "df_test = pd.read_csv(\"goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')\n",
    "df_replace = pd.read_csv(\"test_TaskA/SemEval2018-T3_input_test_taskA.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "8cd6bf9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)\n",
    "df_test.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)\n",
    "df_replace.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0a023",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "64b38f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=True)\n",
    "df_tmp = df['Tweet text'].apply(tokenizer.tokenize)\n",
    "df_test_tmp = df['Tweet text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "63f5631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet text'] = df['Tweet text'].apply(emoji.demojize)\n",
    "df_test['Tweet text'] = df_test['Tweet text'].apply(emoji.demojize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "f935fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_counter(s):\n",
    "    return len(emoji.emoji_lis(emoji.emojize(s)))\n",
    "\n",
    "df['emoji_count'] = df['Tweet text'].apply(emoji_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "0656d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_emojis(s):\n",
    "#     print(f'S: {s}')\n",
    "#     print()\n",
    "#     d = emoji.emoji_lis(emoji.emojize(s))\n",
    "#     s_new = ''\n",
    "#     if d != []:\n",
    "#         for i, el in enumerate(d):\n",
    "#             print(i, el)\n",
    "#             if i == 0:\n",
    "#                 s_new += s[:el['location']]\n",
    "#             elif i < len(d):\n",
    "#                 up_to = sum([k['location'] + len(emoji.demojize(k['emoji'])) for k in d[:i]])\n",
    "#                 len_text = el['location'] - (d[i-1]['location'] + 1)\n",
    "#                 s_new += s[up_to : up_to + len_text]\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "94a03a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "# seg_eng = Segmenter(corpus=\"english\") \n",
    "seg_tw = Segmenter(corpus=\"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "702478ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_hashtags(s):\n",
    "    '''Removes the hashtag sign and segments the hashtag text.'''\n",
    "    hashtags = []\n",
    "    \n",
    "    l = []\n",
    "    for i, s_i in enumerate(s):\n",
    "        if s_i.startswith('#'):\n",
    "            tmp = tokenizer.tokenize(seg_tw.segment(s_i.replace('#', '')))\n",
    "            l.extend(tmp)\n",
    "        else:\n",
    "            l.append(s_i)\n",
    "    return l\n",
    "            \n",
    "df_tmp = df_tmp.apply(separate_hashtags)\n",
    "df_test_tmp = df_test_tmp.apply(separate_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "e5badc5e",
   "metadata": {
    "id": "e5badc5e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_words(s):\n",
    "    '''\n",
    "    Removes tags, emojis, links, smiley faces, | signs, stopwords and changes the case to lower.\n",
    "    '''\n",
    "    ret_list = []\n",
    "\n",
    "    smiley_regex = r'([\\:\\;\\=][()PDO\\/\\]\\[p|]+)+'\n",
    "    \n",
    "    is_tag = lambda w: w.startswith('@')\n",
    "    is_vertical_line = lambda w: w.startswith('|')\n",
    "    is_emoji = lambda w: w != ':' and w.startswith(':') and w.endswith(':')\n",
    "    remove_emoji = lambda w: w[:w.index(':')] + w[w.rindex(':')+1:] if ':' in w and w.index(':') != w.rindex(':') else w\n",
    "    is_link = lambda w: w.startswith(\"http\") or w.startswith(\"https\")\n",
    "    is_hashtag = lambda w: w.startswith(\"#\")\n",
    "    is_smiley = lambda w: re.match(smiley_regex, w)\n",
    "\n",
    "    w2 = []\n",
    "    for i, w in enumerate(s):\n",
    "        if is_tag(w) or is_emoji(w) or is_link(w) or is_vertical_line(w):\n",
    "            continue\n",
    "\n",
    "        elif is_hashtag(w):\n",
    "            w_tmp = w.replace('#', '')\n",
    "            if w_tmp != '':\n",
    "                lower_append(w_tmp, w2)\n",
    "\n",
    "        elif is_smiley(w):\n",
    "            w_tmp = re.sub(smiley_regex, '', w)\n",
    "            if w_tmp != '':\n",
    "                lower_append(w_tmp, w2)\n",
    "\n",
    "        else:\n",
    "            w_tmp = w.replace('#', '')\n",
    "            w_tmp = w_tmp.replace('|', '')\n",
    "            w_tmp = w_tmp.replace('_', '')\n",
    "            w_tmp = w_tmp.replace('...', '')\n",
    "            if w_tmp != '':\n",
    "                lower_append(w_tmp, w2)\n",
    "\n",
    "    return ' '.join([i for i in w2 if len(i) > 2])\n",
    "\n",
    "def lower_append(w, l):\n",
    "    l.append(w.lower())\n",
    "\n",
    "df['clean_text'] = df_tmp.apply(preprocess_words)\n",
    "df_test['clean_text'] = df_test_tmp.apply(preprocess_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "4606db1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "      <td>0</td>\n",
       "      <td>sweet united nations video just time for chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "      <td>0</td>\n",
       "      <td>are rumored have talked erv's agent and the an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey there nice see you minnesota winter weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td>0</td>\n",
       "      <td>episodes left i'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "      <td>0</td>\n",
       "      <td>can't breathe was chosen the most notable quot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>You're never too old for Footie Pajamas. http:...</td>\n",
       "      <td>0</td>\n",
       "      <td>you're never too old for footie pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Nothing makes me happier then getting on the h...</td>\n",
       "      <td>0</td>\n",
       "      <td>nothing makes happier then getting the highway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 an opening my first beer now gonna be a l...</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 opening first beer now gonna long night day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>@Adam_Klug do you think you would support a gu...</td>\n",
       "      <td>0</td>\n",
       "      <td>you think you would support guy who knocked ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@samcguigan544 You are not allowed to open tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>you are not allowed open that until christmas day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh, thank GOD - our entire office email system...</td>\n",
       "      <td>0</td>\n",
       "      <td>thank god our entire office email system down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>But instead, I'm scrolling through Facebook, I...</td>\n",
       "      <td>0</td>\n",
       "      <td>but instead i'm scrolling through facebook ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>@TargetZonePT :pouting_face: no he bloody isn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>outingface bloody isn't was upstairs getting c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>cold warmth both suffuse one's cheeks with pin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Just great when you're mobile bill arrives by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>just great when you're mobile bill arrives text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tweet index  Label                                         Tweet text  \\\n",
       "0             1      1  Sweet United Nations video. Just in time for C...   \n",
       "1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n",
       "2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n",
       "3             4      0                3 episodes left I'm dying over here   \n",
       "4             5      1  I can't breathe! was chosen as the most notabl...   \n",
       "5             6      0  You're never too old for Footie Pajamas. http:...   \n",
       "6             7      1  Nothing makes me happier then getting on the h...   \n",
       "7             8      0  4:30 an opening my first beer now gonna be a l...   \n",
       "8             9      0  @Adam_Klug do you think you would support a gu...   \n",
       "9            10      0  @samcguigan544 You are not allowed to open tha...   \n",
       "10           11      1  Oh, thank GOD - our entire office email system...   \n",
       "11           12      0  But instead, I'm scrolling through Facebook, I...   \n",
       "12           13      0  @TargetZonePT :pouting_face: no he bloody isn'...   \n",
       "13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n",
       "14           15      1  Just great when you're mobile bill arrives by ...   \n",
       "\n",
       "    emoji_count                                         clean_text  \n",
       "0             0  sweet united nations video just time for chris...  \n",
       "1             0  are rumored have talked erv's agent and the an...  \n",
       "2             0    hey there nice see you minnesota winter weather  \n",
       "3             0                  episodes left i'm dying over here  \n",
       "4             0  can't breathe was chosen the most notable quot...  \n",
       "5             0            you're never too old for footie pajamas  \n",
       "6             0  nothing makes happier then getting the highway...  \n",
       "7             0   4:30 opening first beer now gonna long night day  \n",
       "8             0  you think you would support guy who knocked ou...  \n",
       "9             0  you are not allowed open that until christmas day  \n",
       "10            0  thank god our entire office email system down ...  \n",
       "11            0  but instead i'm scrolling through facebook ins...  \n",
       "12            1  outingface bloody isn't was upstairs getting c...  \n",
       "13            0  cold warmth both suffuse one's cheeks with pin...  \n",
       "14            0    just great when you're mobile bill arrives text  "
      ]
     },
     "execution_count": 746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf8c4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def simple_preprocessing(s):\n",
    "#     '''Lowercases, tokenizes, de-accents, removes words shorter than 3 and longer than 14 characters'''\n",
    "#     return [' '.join(simple_preprocess(s_i)) for s_i in s]\n",
    "\n",
    "# df['clean_text'] = df[['clean_text']].apply(simple_preprocessing)\n",
    "# df_test['clean_text'] = df_test[['clean_text']].apply(simple_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "79dead99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "      <td>0</td>\n",
       "      <td>sweet united nations video just time for chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "      <td>0</td>\n",
       "      <td>are rumored have talked erv's agent and the an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey there nice see you minnesota winter weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td>0</td>\n",
       "      <td>episodes left i'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "      <td>0</td>\n",
       "      <td>can't breathe was chosen the most notable quot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>You're never too old for Footie Pajamas. http:...</td>\n",
       "      <td>0</td>\n",
       "      <td>you're never too old for footie pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Nothing makes me happier then getting on the h...</td>\n",
       "      <td>0</td>\n",
       "      <td>nothing makes happier then getting the highway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 an opening my first beer now gonna be a l...</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 opening first beer now gonna long night day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>@Adam_Klug do you think you would support a gu...</td>\n",
       "      <td>0</td>\n",
       "      <td>you think you would support guy who knocked ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@samcguigan544 You are not allowed to open tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>you are not allowed open that until christmas day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh, thank GOD - our entire office email system...</td>\n",
       "      <td>0</td>\n",
       "      <td>thank god our entire office email system down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>But instead, I'm scrolling through Facebook, I...</td>\n",
       "      <td>0</td>\n",
       "      <td>but instead i'm scrolling through facebook ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>@TargetZonePT :pouting_face: no he bloody isn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>outingface bloody isn't was upstairs getting c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>cold warmth both suffuse one's cheeks with pin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Just great when you're mobile bill arrives by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>just great when you're mobile bill arrives text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tweet index  Label                                         Tweet text  \\\n",
       "0             1      1  Sweet United Nations video. Just in time for C...   \n",
       "1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n",
       "2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n",
       "3             4      0                3 episodes left I'm dying over here   \n",
       "4             5      1  I can't breathe! was chosen as the most notabl...   \n",
       "5             6      0  You're never too old for Footie Pajamas. http:...   \n",
       "6             7      1  Nothing makes me happier then getting on the h...   \n",
       "7             8      0  4:30 an opening my first beer now gonna be a l...   \n",
       "8             9      0  @Adam_Klug do you think you would support a gu...   \n",
       "9            10      0  @samcguigan544 You are not allowed to open tha...   \n",
       "10           11      1  Oh, thank GOD - our entire office email system...   \n",
       "11           12      0  But instead, I'm scrolling through Facebook, I...   \n",
       "12           13      0  @TargetZonePT :pouting_face: no he bloody isn'...   \n",
       "13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n",
       "14           15      1  Just great when you're mobile bill arrives by ...   \n",
       "\n",
       "    emoji_count                                         clean_text  \n",
       "0             0  sweet united nations video just time for chris...  \n",
       "1             0  are rumored have talked erv's agent and the an...  \n",
       "2             0    hey there nice see you minnesota winter weather  \n",
       "3             0                  episodes left i'm dying over here  \n",
       "4             0  can't breathe was chosen the most notable quot...  \n",
       "5             0            you're never too old for footie pajamas  \n",
       "6             0  nothing makes happier then getting the highway...  \n",
       "7             0   4:30 opening first beer now gonna long night day  \n",
       "8             0  you think you would support guy who knocked ou...  \n",
       "9             0  you are not allowed open that until christmas day  \n",
       "10            0  thank god our entire office email system down ...  \n",
       "11            0  but instead i'm scrolling through facebook ins...  \n",
       "12            1  outingface bloody isn't was upstairs getting c...  \n",
       "13            0  cold warmth both suffuse one's cheeks with pin...  \n",
       "14            0    just great when you're mobile bill arrives text  "
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "e48acc39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "      <td>0</td>\n",
       "      <td>sweet united nations video just time for chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "      <td>0</td>\n",
       "      <td>are rumored have talked erv's agent and the an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey there nice see you minnesota winter weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td>0</td>\n",
       "      <td>episodes left i'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "      <td>0</td>\n",
       "      <td>can't breathe was chosen the most notable quot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>You're never too old for Footie Pajamas. http:...</td>\n",
       "      <td>0</td>\n",
       "      <td>you're never too old for footie pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Nothing makes me happier then getting on the h...</td>\n",
       "      <td>0</td>\n",
       "      <td>nothing makes happier then getting the highway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 an opening my first beer now gonna be a l...</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 opening first beer now gonna long night day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>@Adam_Klug do you think you would support a gu...</td>\n",
       "      <td>0</td>\n",
       "      <td>you think you would support guy who knocked ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@samcguigan544 You are not allowed to open tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>you are not allowed open that until christmas day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh, thank GOD - our entire office email system...</td>\n",
       "      <td>0</td>\n",
       "      <td>thank god our entire office email system down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>But instead, I'm scrolling through Facebook, I...</td>\n",
       "      <td>0</td>\n",
       "      <td>but instead i'm scrolling through facebook ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>@TargetZonePT :pouting_face: no he bloody isn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>outingface bloody isn't was upstairs getting c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>cold warmth both suffuse one's cheeks with pin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Just great when you're mobile bill arrives by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>just great when you're mobile bill arrives text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tweet index  Label                                         Tweet text  \\\n",
       "0             1      1  Sweet United Nations video. Just in time for C...   \n",
       "1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n",
       "2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n",
       "3             4      0                3 episodes left I'm dying over here   \n",
       "4             5      1  I can't breathe! was chosen as the most notabl...   \n",
       "5             6      0  You're never too old for Footie Pajamas. http:...   \n",
       "6             7      1  Nothing makes me happier then getting on the h...   \n",
       "7             8      0  4:30 an opening my first beer now gonna be a l...   \n",
       "8             9      0  @Adam_Klug do you think you would support a gu...   \n",
       "9            10      0  @samcguigan544 You are not allowed to open tha...   \n",
       "10           11      1  Oh, thank GOD - our entire office email system...   \n",
       "11           12      0  But instead, I'm scrolling through Facebook, I...   \n",
       "12           13      0  @TargetZonePT :pouting_face: no he bloody isn'...   \n",
       "13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n",
       "14           15      1  Just great when you're mobile bill arrives by ...   \n",
       "\n",
       "    emoji_count                                         clean_text  \n",
       "0             0  sweet united nations video just time for chris...  \n",
       "1             0  are rumored have talked erv's agent and the an...  \n",
       "2             0    hey there nice see you minnesota winter weather  \n",
       "3             0                  episodes left i'm dying over here  \n",
       "4             0  can't breathe was chosen the most notable quot...  \n",
       "5             0            you're never too old for footie pajamas  \n",
       "6             0  nothing makes happier then getting the highway...  \n",
       "7             0   4:30 opening first beer now gonna long night day  \n",
       "8             0  you think you would support guy who knocked ou...  \n",
       "9             0  you are not allowed open that until christmas day  \n",
       "10            0  thank god our entire office email system down ...  \n",
       "11            0  but instead i'm scrolling through facebook ins...  \n",
       "12            1  outingface bloody isn't was upstairs getting c...  \n",
       "13            0  cold warmth both suffuse one's cheeks with pin...  \n",
       "14            0    just great when you're mobile bill arrives text  "
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_tuple_characters(s):\n",
    "    return [re.sub(r'(.)\\1{2,}', r'\\1', w) for w in s]\n",
    "\n",
    "df['clean_text'] = df[['clean_text']].apply(remove_tuple_characters)\n",
    "df_test['clean_text'] = df_test[['clean_text']].apply(remove_tuple_characters)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "bb677a12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lemmatize(s):\n",
    "    '''Lemmatizes the words in the sentences and returns them if theyre not stopwords or punctuation'''\n",
    "    return [[w.lemma_.lower() for w in nlp(s_i) if w.lemma_.lower() not in nlp.Defaults.stop_words and not w.is_punct] for s_i in s]\n",
    "\n",
    "df['lemmas'] = df[['clean_text']].apply(lemmatize)\n",
    "df_test['lemmas'] = df_test[['clean_text']].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "f2d460e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "      <td>0</td>\n",
       "      <td>sweet united nations video just time for chris...</td>\n",
       "      <td>[sweet, united, nations, video, time, christma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "      <td>0</td>\n",
       "      <td>are rumored have talked erv's agent and the an...</td>\n",
       "      <td>[rumor, talk, erv, agent, angel, ask, escobar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey there nice see you minnesota winter weather</td>\n",
       "      <td>[hey, nice, minnesota, winter, weather]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td>0</td>\n",
       "      <td>episodes left i'm dying over here</td>\n",
       "      <td>[episode, leave, die]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "      <td>0</td>\n",
       "      <td>can't breathe was chosen the most notable quot...</td>\n",
       "      <td>[breathe, choose, notable, quote, year, annual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>You're never too old for Footie Pajamas. http:...</td>\n",
       "      <td>0</td>\n",
       "      <td>you're never too old for footie pajamas</td>\n",
       "      <td>[old, footie, pajama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Nothing makes me happier then getting on the h...</td>\n",
       "      <td>0</td>\n",
       "      <td>nothing makes happier then getting the highway...</td>\n",
       "      <td>[happy, highway, break, light, light, like, ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 an opening my first beer now gonna be a l...</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 opening first beer now gonna long night day</td>\n",
       "      <td>[4:30, open, beer, long, night, day]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>@Adam_Klug do you think you would support a gu...</td>\n",
       "      <td>0</td>\n",
       "      <td>you think you would support guy who knocked ou...</td>\n",
       "      <td>[think, support, guy, knock, daughter, rice, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@samcguigan544 You are not allowed to open tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>you are not allowed open that until christmas day</td>\n",
       "      <td>[allow, open, christmas, day]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh, thank GOD - our entire office email system...</td>\n",
       "      <td>0</td>\n",
       "      <td>thank god our entire office email system down ...</td>\n",
       "      <td>[thank, god, entire, office, email, system, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>But instead, I'm scrolling through Facebook, I...</td>\n",
       "      <td>0</td>\n",
       "      <td>but instead i'm scrolling through facebook ins...</td>\n",
       "      <td>[instead, scroll, facebook, instagram, twitter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>@TargetZonePT :pouting_face: no he bloody isn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>outingface bloody isn't was upstairs getting c...</td>\n",
       "      <td>[outingface, bloody, upstairs, getting, change]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>cold warmth both suffuse one's cheeks with pin...</td>\n",
       "      <td>[cold, warmth, suffuse, cheek, pink, colour, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Just great when you're mobile bill arrives by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>just great when you're mobile bill arrives text</td>\n",
       "      <td>[great, mobile, bill, arrive, text]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tweet index  Label                                         Tweet text  \\\n",
       "0             1      1  Sweet United Nations video. Just in time for C...   \n",
       "1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n",
       "2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n",
       "3             4      0                3 episodes left I'm dying over here   \n",
       "4             5      1  I can't breathe! was chosen as the most notabl...   \n",
       "5             6      0  You're never too old for Footie Pajamas. http:...   \n",
       "6             7      1  Nothing makes me happier then getting on the h...   \n",
       "7             8      0  4:30 an opening my first beer now gonna be a l...   \n",
       "8             9      0  @Adam_Klug do you think you would support a gu...   \n",
       "9            10      0  @samcguigan544 You are not allowed to open tha...   \n",
       "10           11      1  Oh, thank GOD - our entire office email system...   \n",
       "11           12      0  But instead, I'm scrolling through Facebook, I...   \n",
       "12           13      0  @TargetZonePT :pouting_face: no he bloody isn'...   \n",
       "13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n",
       "14           15      1  Just great when you're mobile bill arrives by ...   \n",
       "\n",
       "    emoji_count                                         clean_text  \\\n",
       "0             0  sweet united nations video just time for chris...   \n",
       "1             0  are rumored have talked erv's agent and the an...   \n",
       "2             0    hey there nice see you minnesota winter weather   \n",
       "3             0                  episodes left i'm dying over here   \n",
       "4             0  can't breathe was chosen the most notable quot...   \n",
       "5             0            you're never too old for footie pajamas   \n",
       "6             0  nothing makes happier then getting the highway...   \n",
       "7             0   4:30 opening first beer now gonna long night day   \n",
       "8             0  you think you would support guy who knocked ou...   \n",
       "9             0  you are not allowed open that until christmas day   \n",
       "10            0  thank god our entire office email system down ...   \n",
       "11            0  but instead i'm scrolling through facebook ins...   \n",
       "12            1  outingface bloody isn't was upstairs getting c...   \n",
       "13            0  cold warmth both suffuse one's cheeks with pin...   \n",
       "14            0    just great when you're mobile bill arrives text   \n",
       "\n",
       "                                               lemmas  \n",
       "0   [sweet, united, nations, video, time, christma...  \n",
       "1   [rumor, talk, erv, agent, angel, ask, escobar,...  \n",
       "2             [hey, nice, minnesota, winter, weather]  \n",
       "3                               [episode, leave, die]  \n",
       "4   [breathe, choose, notable, quote, year, annual...  \n",
       "5                               [old, footie, pajama]  \n",
       "6   [happy, highway, break, light, light, like, ch...  \n",
       "7                [4:30, open, beer, long, night, day]  \n",
       "8   [think, support, guy, knock, daughter, rice, d...  \n",
       "9                       [allow, open, christmas, day]  \n",
       "10  [thank, god, entire, office, email, system, da...  \n",
       "11  [instead, scroll, facebook, instagram, twitter...  \n",
       "12    [outingface, bloody, upstairs, getting, change]  \n",
       "13  [cold, warmth, suffuse, cheek, pink, colour, t...  \n",
       "14                [great, mobile, bill, arrive, text]  "
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "1419464a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### baseline features ###\n",
    "tokenizer2 = TweetTokenizer()\n",
    "def word_counter(s):\n",
    "    return len([x for x in tokenizer.tokenize(s) if not x.startswith((\"@\", \"#\", \"http\"))])\n",
    "\n",
    "def char_counter(s):\n",
    "    return len(s.replace(' ', ''))\n",
    "\n",
    "def all_uppercase_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.isupper() and not x.startswith((\"@\", \"#\", \"http\"))])\n",
    "\n",
    "def all_lowercase_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.islower() and not x.startswith((\"@\", \"#\", \"http\"))])\n",
    "\n",
    "def capitalised_counter(s):\n",
    "    return sum([i.istitle() for i in tokenizer2.tokenize(s)])\n",
    "\n",
    "def digit_counter(s):\n",
    "    return sum([i.isdigit() for i in s])\n",
    "\n",
    "\n",
    "\n",
    "### other features ###\n",
    "def tag_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.startswith(\"@\")])\n",
    "\n",
    "def hashtag_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.startswith(\"#\")])\n",
    "\n",
    "def link_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.startswith(('http:', 'https:'))])\n",
    "\n",
    "def smiley_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if re.match(r'([\\:\\;\\=][()PDO\\/\\]\\[p|]+)+', x)])\n",
    "\n",
    "def exclamation_mark_counter(s):\n",
    "    return s.count('!')\n",
    "\n",
    "def question_mark_counter(s):\n",
    "    return s.count('?')\n",
    "\n",
    "def ellipsis_counter(s):\n",
    "    return s.count('...')\n",
    "    \n",
    "\n",
    "\n",
    "### NER ###\n",
    "def ORG_tag_counter(s):\n",
    "    doc = nlp(s)\n",
    "    return len([d.text for d in doc.ents if d.label_ == 'ORG'])\n",
    "\n",
    "def NORP_tag_counter(s):\n",
    "    doc = nlp(s)\n",
    "    return len([d.text for d in doc.ents if d.label_ == 'NORP'])\n",
    "\n",
    "def GPE_tag_counter(s):\n",
    "    doc = nlp(s)\n",
    "    return len([d.text for d in doc.ents if d.label_ == 'GPE'])\n",
    "\n",
    "def PERSON_tag_counter(s):\n",
    "    doc = nlp(s)\n",
    "    return len([d.text for d in doc.ents if d.label_ == 'PERSON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "1d8d398f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(some_df):\n",
    "    some_df['word_count'] = df['clean_text'].apply(word_counter)\n",
    "    some_df['char_count'] = df['clean_text'].apply(char_counter)\n",
    "    some_df['all_uppercase_count'] = df['Tweet text'].apply(all_uppercase_counter)\n",
    "    some_df['all_lowercase_count'] = df['Tweet text'].apply(all_lowercase_counter)\n",
    "    some_df['capitalised_count'] = df['Tweet text'].apply(capitalised_counter)\n",
    "    some_df['digit_count'] = df['Tweet text'].apply(digit_counter)\n",
    "    \n",
    "    some_df['tag_count'] = df['Tweet text'].apply(tag_counter)\n",
    "    some_df['hashtag_count'] = df['Tweet text'].apply(hashtag_counter)\n",
    "    some_df['link_count'] = df['Tweet text'].apply(link_counter)\n",
    "    some_df['smiley_count'] = df['Tweet text'].apply(smiley_counter)\n",
    "    \n",
    "    some_df['exclamation_mark_count'] = df['Tweet text'].apply(exclamation_mark_counter)\n",
    "    some_df['question_mark_count'] = df['Tweet text'].apply(question_mark_counter)\n",
    "    some_df['ellipsis_count'] = df['Tweet text'].apply(ellipsis_counter)\n",
    "    \n",
    "    some_df['ORG_tag_count'] = df['Tweet text'].apply(ORG_tag_counter)\n",
    "    some_df['NORP_tag_count'] = df['Tweet text'].apply(NORP_tag_counter)\n",
    "    some_df['GPE_tag_count'] = df['Tweet text'].apply(GPE_tag_counter)\n",
    "    some_df['PERSON_tag_count'] = df['Tweet text'].apply(PERSON_tag_counter)\n",
    "    \n",
    "add_features(df)\n",
    "add_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "0b415a2b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>all_uppercase_count</th>\n",
       "      <th>all_lowercase_count</th>\n",
       "      <th>...</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>link_count</th>\n",
       "      <th>smiley_count</th>\n",
       "      <th>exclamation_mark_count</th>\n",
       "      <th>question_mark_count</th>\n",
       "      <th>ellipsis_count</th>\n",
       "      <th>ORG_tag_count</th>\n",
       "      <th>NORP_tag_count</th>\n",
       "      <th>GPE_tag_count</th>\n",
       "      <th>PERSON_tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "      <td>0</td>\n",
       "      <td>sweet united nations video just time for chris...</td>\n",
       "      <td>[sweet, united, nations, video, time, christma...</td>\n",
       "      <td>10</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "      <td>0</td>\n",
       "      <td>are rumored have talked erv's agent and the an...</td>\n",
       "      <td>[rumor, talk, erv, agent, angel, ask, escobar,...</td>\n",
       "      <td>15</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey there nice see you minnesota winter weather</td>\n",
       "      <td>[hey, nice, minnesota, winter, weather]</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td>0</td>\n",
       "      <td>episodes left i'm dying over here</td>\n",
       "      <td>[episode, leave, die]</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "      <td>0</td>\n",
       "      <td>can't breathe was chosen the most notable quot...</td>\n",
       "      <td>[breathe, choose, notable, quote, year, annual...</td>\n",
       "      <td>16</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label                                         Tweet text  \\\n",
       "0            1      1  Sweet United Nations video. Just in time for C...   \n",
       "1            2      1  @mrdahl87 We are rumored to have talked to Erv...   \n",
       "2            3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n",
       "3            4      0                3 episodes left I'm dying over here   \n",
       "4            5      1  I can't breathe! was chosen as the most notabl...   \n",
       "\n",
       "   emoji_count                                         clean_text  \\\n",
       "0            0  sweet united nations video just time for chris...   \n",
       "1            0  are rumored have talked erv's agent and the an...   \n",
       "2            0    hey there nice see you minnesota winter weather   \n",
       "3            0                  episodes left i'm dying over here   \n",
       "4            0  can't breathe was chosen the most notable quot...   \n",
       "\n",
       "                                              lemmas  word_count  char_count  \\\n",
       "0  [sweet, united, nations, video, time, christma...          10          58   \n",
       "1  [rumor, talk, erv, agent, angel, ask, escobar,...          15          78   \n",
       "2            [hey, nice, minnesota, winter, weather]           8          40   \n",
       "3                              [episode, leave, die]           6          28   \n",
       "4  [breathe, choose, notable, quote, year, annual...          16          88   \n",
       "\n",
       "   all_uppercase_count  all_lowercase_count  ...  hashtag_count  link_count  \\\n",
       "0                    0                    4  ...              2           1   \n",
       "1                    0                   14  ...              0           0   \n",
       "2                    1                    4  ...              0           0   \n",
       "3                    0                    5  ...              0           0   \n",
       "4                    1                   20  ...              0           0   \n",
       "\n",
       "   smiley_count  exclamation_mark_count  question_mark_count  ellipsis_count  \\\n",
       "0             0                       0                    0               0   \n",
       "1             1                       0                    0               2   \n",
       "2             0                       1                    0               0   \n",
       "3             0                       0                    0               0   \n",
       "4             0                       1                    0               0   \n",
       "\n",
       "   ORG_tag_count  NORP_tag_count  GPE_tag_count  PERSON_tag_count  \n",
       "0              1               0              0                 0  \n",
       "1              0               0              0                 2  \n",
       "2              0               0              1                 0  \n",
       "3              0               0              0                 0  \n",
       "4              1               0              0                 0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "af74098d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7)\n",
    "df_validation = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "1a5cd369",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>all_uppercase_count</th>\n",
       "      <th>all_lowercase_count</th>\n",
       "      <th>...</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>link_count</th>\n",
       "      <th>smiley_count</th>\n",
       "      <th>exclamation_mark_count</th>\n",
       "      <th>question_mark_count</th>\n",
       "      <th>ellipsis_count</th>\n",
       "      <th>ORG_tag_count</th>\n",
       "      <th>NORP_tag_count</th>\n",
       "      <th>GPE_tag_count</th>\n",
       "      <th>PERSON_tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>2698</td>\n",
       "      <td>0</td>\n",
       "      <td>#OPEC #chief #defends #policy, #says #group to...</td>\n",
       "      <td>0</td>\n",
       "      <td>opec chief defends policy says group try ride ...</td>\n",
       "      <td>[opec, chief, defend, policy, group, try, ride...</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide ANY source that shows it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide any source that shows</td>\n",
       "      <td>[provide, source]</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>0</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>[sick]</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>1770</td>\n",
       "      <td>1</td>\n",
       "      <td>Quel domage  RT @CFL_News: No Canada: Bills in...</td>\n",
       "      <td>0</td>\n",
       "      <td>quel domage canada bills series which was colo...</td>\n",
       "      <td>[quel, domage, canada, bills, series, colossal...</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>2590</td>\n",
       "      <td>0</td>\n",
       "      <td>@PaulGoonerW yeah stressed isnt the word could...</td>\n",
       "      <td>1</td>\n",
       "      <td>yeah stressed isnt the word could easily punch...</td>\n",
       "      <td>[yeah, stressed, word, easily, punch, face, ri...</td>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tweet index  Label                                         Tweet text  \\\n",
       "2694         2698      0  #OPEC #chief #defends #policy, #says #group to...   \n",
       "1184         1185      0  yet you cant provide ANY source that shows it ...   \n",
       "1267         1268      1                                   also sick names    \n",
       "1766         1770      1  Quel domage  RT @CFL_News: No Canada: Bills in...   \n",
       "2586         2590      0  @PaulGoonerW yeah stressed isnt the word could...   \n",
       "\n",
       "      emoji_count                                         clean_text  \\\n",
       "2694            0  opec chief defends policy says group try ride ...   \n",
       "1184            0         yet you cant provide any source that shows   \n",
       "1267            0                                    also sick names   \n",
       "1766            0  quel domage canada bills series which was colo...   \n",
       "2586            1  yeah stressed isnt the word could easily punch...   \n",
       "\n",
       "                                                 lemmas  word_count  \\\n",
       "2694  [opec, chief, defend, policy, group, try, ride...          12   \n",
       "1184                                  [provide, source]           8   \n",
       "1267                                             [sick]           3   \n",
       "1766  [quel, domage, canada, bills, series, colossal...          14   \n",
       "2586  [yeah, stressed, word, easily, punch, face, ri...          16   \n",
       "\n",
       "      char_count  all_uppercase_count  all_lowercase_count  ...  \\\n",
       "2694          53                    0                    3  ...   \n",
       "1184          35                    1                    8  ...   \n",
       "1267          13                    0                    3  ...   \n",
       "1766          69                    2                   11  ...   \n",
       "2586         101                    0                   17  ...   \n",
       "\n",
       "      hashtag_count  link_count  smiley_count  exclamation_mark_count  \\\n",
       "2694             11           1             0                       0   \n",
       "1184              0           0             0                       0   \n",
       "1267              0           0             0                       0   \n",
       "1766              2           1             0                       0   \n",
       "2586              0           0             0                       0   \n",
       "\n",
       "      question_mark_count  ellipsis_count  ORG_tag_count  NORP_tag_count  \\\n",
       "2694                    0               0              2               0   \n",
       "1184                    0               0              1               0   \n",
       "1267                    0               0              0               0   \n",
       "1766                    0               0              3               0   \n",
       "2586                    0               0              0               0   \n",
       "\n",
       "      GPE_tag_count  PERSON_tag_count  \n",
       "2694              1                 0  \n",
       "1184              0                 0  \n",
       "1267              0                 0  \n",
       "1766              0                 0  \n",
       "2586              0                 0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "138ca2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_df_with_all_features(df1, df2):\n",
    "    cols_to_add = list(set(df2.columns.tolist()) - set(df1.columns.tolist()))\n",
    "    if 'Tweet index' in cols_to_add:\n",
    "        cols_to_add.remove('Tweet index')\n",
    "    new_df = pd.concat((df1.copy(), df2[cols_to_add]), axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "2d560761",
   "metadata": {
    "id": "2d560761"
   },
   "outputs": [],
   "source": [
    "def join_docs(s):\n",
    "    '''Joins the strings inside the inner list of a nested list'''\n",
    "    return ' '.join(s)\n",
    "\n",
    "df_train['topic_text'] = df_train['lemmas'].apply(join_docs)\n",
    "df_validation['topic_text'] = df_validation['lemmas'].apply(join_docs)\n",
    "df_test['topic_text'] = df_test['lemmas'].apply(join_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "c6cb21d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2672, 24)"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "6ca36c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>all_uppercase_count</th>\n",
       "      <th>all_lowercase_count</th>\n",
       "      <th>...</th>\n",
       "      <th>link_count</th>\n",
       "      <th>smiley_count</th>\n",
       "      <th>exclamation_mark_count</th>\n",
       "      <th>question_mark_count</th>\n",
       "      <th>ellipsis_count</th>\n",
       "      <th>ORG_tag_count</th>\n",
       "      <th>NORP_tag_count</th>\n",
       "      <th>GPE_tag_count</th>\n",
       "      <th>PERSON_tag_count</th>\n",
       "      <th>topic_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>2698</td>\n",
       "      <td>0</td>\n",
       "      <td>#OPEC #chief #defends #policy, #says #group to...</td>\n",
       "      <td>0</td>\n",
       "      <td>opec chief defends policy says group try ride ...</td>\n",
       "      <td>[opec, chief, defend, policy, group, try, ride...</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>opec chief defend policy group try ride price ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide ANY source that shows it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide any source that shows</td>\n",
       "      <td>[provide, source]</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>provide source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>0</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>[sick]</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>1770</td>\n",
       "      <td>1</td>\n",
       "      <td>Quel domage  RT @CFL_News: No Canada: Bills in...</td>\n",
       "      <td>0</td>\n",
       "      <td>quel domage canada bills series which was colo...</td>\n",
       "      <td>[quel, domage, canada, bills, series, colossal...</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>quel domage canada bills series colossal flop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>2590</td>\n",
       "      <td>0</td>\n",
       "      <td>@PaulGoonerW yeah stressed isnt the word could...</td>\n",
       "      <td>1</td>\n",
       "      <td>yeah stressed isnt the word could easily punch...</td>\n",
       "      <td>[yeah, stressed, word, easily, punch, face, ri...</td>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah stressed word easily punch face right smi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tweet index  Label                                         Tweet text  \\\n",
       "2694         2698      0  #OPEC #chief #defends #policy, #says #group to...   \n",
       "1184         1185      0  yet you cant provide ANY source that shows it ...   \n",
       "1267         1268      1                                   also sick names    \n",
       "1766         1770      1  Quel domage  RT @CFL_News: No Canada: Bills in...   \n",
       "2586         2590      0  @PaulGoonerW yeah stressed isnt the word could...   \n",
       "\n",
       "      emoji_count                                         clean_text  \\\n",
       "2694            0  opec chief defends policy says group try ride ...   \n",
       "1184            0         yet you cant provide any source that shows   \n",
       "1267            0                                    also sick names   \n",
       "1766            0  quel domage canada bills series which was colo...   \n",
       "2586            1  yeah stressed isnt the word could easily punch...   \n",
       "\n",
       "                                                 lemmas  word_count  \\\n",
       "2694  [opec, chief, defend, policy, group, try, ride...          12   \n",
       "1184                                  [provide, source]           8   \n",
       "1267                                             [sick]           3   \n",
       "1766  [quel, domage, canada, bills, series, colossal...          14   \n",
       "2586  [yeah, stressed, word, easily, punch, face, ri...          16   \n",
       "\n",
       "      char_count  all_uppercase_count  all_lowercase_count  ...  link_count  \\\n",
       "2694          53                    0                    3  ...           1   \n",
       "1184          35                    1                    8  ...           0   \n",
       "1267          13                    0                    3  ...           0   \n",
       "1766          69                    2                   11  ...           1   \n",
       "2586         101                    0                   17  ...           0   \n",
       "\n",
       "      smiley_count  exclamation_mark_count  question_mark_count  \\\n",
       "2694             0                       0                    0   \n",
       "1184             0                       0                    0   \n",
       "1267             0                       0                    0   \n",
       "1766             0                       0                    0   \n",
       "2586             0                       0                    0   \n",
       "\n",
       "      ellipsis_count  ORG_tag_count  NORP_tag_count  GPE_tag_count  \\\n",
       "2694               0              2               0              1   \n",
       "1184               0              1               0              0   \n",
       "1267               0              0               0              0   \n",
       "1766               0              3               0              0   \n",
       "2586               0              0               0              0   \n",
       "\n",
       "      PERSON_tag_count                                         topic_text  \n",
       "2694                 0  opec chief defend policy group try ride price ...  \n",
       "1184                 0                                     provide source  \n",
       "1267                 0                                               sick  \n",
       "1766                 0  quel domage canada bills series colossal flop ...  \n",
       "2586                 0  yeah stressed word easily punch face right smi...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98404b14",
   "metadata": {
    "id": "98404b14"
   },
   "source": [
    "## Topic modeling baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127a752",
   "metadata": {
    "id": "7127a752"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    min_df=20,\n",
    "    max_df=0.5,\n",
    ")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    min_df=20,\n",
    "    max_df=0.5,\n",
    ")\n",
    "\n",
    "tweet_text_count_train = count_vectorizer.fit_transform(df_train['topic_text'])\n",
    "tweet_text_count_validation = count_vectorizer.transform(df_validation['topic_text'])\n",
    "tweet_text_count_test = count_vectorizer.transform(df_test['topic_text'])\n",
    "\n",
    "tweet_text_tfidf_train = tfidf_vectorizer.fit_transform(df_train['topic_text'])\n",
    "tweet_text_tfidf_validation = tfidf_vectorizer.transform(df_validation['topic_text'])\n",
    "tweet_text_tfidf_test = tfidf_vectorizer.transform(df_test['topic_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08150dae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "08150dae",
    "outputId": "03815e50-e2ab-48a5-a465-97b6c0e7571e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "range_ = list(range(2, 60))\n",
    "for i in range_:\n",
    "    model = KMeans(i)\n",
    "    model.fit(tweet_text_count_train)\n",
    "    inertia.append(model.inertia_)\n",
    "    \n",
    "plt.plot(range_, inertia)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d0b6e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(17)\n",
    "model.fit(tweet_text_count_train)\n",
    "\n",
    "kmeans_count_labels_train = model.predict(tweet_text_count_train)\n",
    "kmeans_count_labels_validation = model.predict(tweet_text_count_validation)\n",
    "kmeans_count_labels_test = model.predict(tweet_text_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f11fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "fe5f11fc",
    "outputId": "8000ac3b-1375-4905-8bca-179acf2ac778",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for i in range_:\n",
    "    model = KMeans(i)\n",
    "    model.fit(tweet_text_tfidf_train)\n",
    "    inertia.append(model.inertia_)\n",
    "plt.plot(range_, inertia)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72eb5b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(22)\n",
    "model.fit(tweet_text_tfidf_train)\n",
    "\n",
    "kmeans_tfidf_labels_train = model.predict(tweet_text_tfidf_train)\n",
    "kmeans_tfidf_labels_validation = model.predict(tweet_text_tfidf_validation)\n",
    "kmeans_tfidf_labels_test = model.predict(tweet_text_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0vdf4uM_F2cg",
   "metadata": {
    "id": "0vdf4uM_F2cg"
   },
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "0fdbed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_validation.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "NRdFJDRVF2m2",
   "metadata": {
    "id": "NRdFJDRVF2m2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.361565351486206\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "topic_model = BERTopic(top_n_words=10, min_topic_size=20)\n",
    "topics, probs = topic_model.fit_transform(df_train['topic_text'])\n",
    "end = time()\n",
    "print(end-start)\n",
    "pred_train = topic_model.transform(df_train['topic_text'])\n",
    "pred_validation = topic_model.transform(df_validation['topic_text'])\n",
    "pred_test = topic_model.transform(df_test['topic_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "W9IK-LziICsL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "W9IK-LziICsL",
    "outputId": "76f344ac-14f1-47a9-fc10-eef060d23b5a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1172</td>\n",
       "      <td>-1_like_day_love_work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>0_money_police_black_people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>1_game_win_team_play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>2_love_closedeye_facethrowingakiss_flushedface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "      <td>3_mean_know_funny_glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>102</td>\n",
       "      <td>4_christmas_gift_holiday_sweater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>97</td>\n",
       "      <td>5_sleep_wake_morning_hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>92</td>\n",
       "      <td>6_twitter_tweet_talk_retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>7_day_week_monday_friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "      <td>8_drink_food_turkey_hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>9_cold_winter_weather_freeze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>52</td>\n",
       "      <td>10_study_school_math_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>40</td>\n",
       "      <td>11_mom_kid_woman_act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>12_music_song_listen_spin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>13_facewithtearsofjoy_evilmonkey_facial_clinki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>14_love_life_pain_hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>15_wear_stitch_sock_fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>16_hear_luv_edition_aye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>17_time_watch_bit_wrong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1   1172                              -1_like_day_love_work\n",
       "1       0    343                        0_money_police_black_people\n",
       "2       1    157                               1_game_win_team_play\n",
       "3       2    121     2_love_closedeye_facethrowingakiss_flushedface\n",
       "4       3    111                             3_mean_know_funny_glad\n",
       "5       4    102                   4_christmas_gift_holiday_sweater\n",
       "6       5     97                          5_sleep_wake_morning_hour\n",
       "7       6     92                       6_twitter_tweet_talk_retweet\n",
       "8       7     76                           7_day_week_monday_friday\n",
       "9       8     72                            8_drink_food_turkey_hot\n",
       "10      9     66                       9_cold_winter_weather_freeze\n",
       "11     10     52                          10_study_school_math_test\n",
       "12     11     40                               11_mom_kid_woman_act\n",
       "13     12     35                          12_music_song_listen_spin\n",
       "14     13     32  13_facewithtearsofjoy_evilmonkey_facial_clinki...\n",
       "15     14     31                             14_love_life_pain_hurt\n",
       "16     15     28                        15_wear_stitch_sock_fashion\n",
       "17     16     24                            16_hear_luv_edition_aye\n",
       "18     17     21                            17_time_watch_bit_wrong"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca352aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_bertopic = df_train.copy()\n",
    "# df_validation_bertopic = df_validation.copy()\n",
    "# df_test_bertopic = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "7e68e524",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train['topic'] = pred_train[0]\n",
    "df_validation['topic'] = pred_validation[0]\n",
    "df_test['topic'] = pred_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_bertopic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad485a",
   "metadata": {
    "id": "6cad485a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "cc4e8fed",
   "metadata": {
    "id": "cc4e8fed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_embedding_train = df_train[['topic_text', 'Label']]\n",
    "tweet_embedding_validation = df_validation[['topic_text', 'Label']]\n",
    "tweet_embedding_test = df_test[['topic_text', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "97c7341c",
   "metadata": {
    "id": "97c7341c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_embedding_train.reset_index(drop=True, inplace=True)\n",
    "tweet_embedding_validation.reset_index(drop=True, inplace=True)\n",
    "tweet_embedding_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "c409d8cc",
   "metadata": {
    "id": "c409d8cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 862M/862M [02:58<00:00, 4.82MB/s]\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 10_000\n",
    "vocab = Vocab(max_size=max_vocab_size, min_freq=20)\n",
    "\n",
    "TWEET = Field('text', numericalizer=vocab)\n",
    "LABEL = LabelField('Label')\n",
    "\n",
    "fields = [TWEET, LABEL]\n",
    "\n",
    "train = TabularDataset.from_pandas(df_train[['topic_text', 'Label']], fields)\n",
    "validation = TabularDataset.from_pandas(df_validation[['topic_text', 'Label']], fields)\n",
    "test = TabularDataset.from_pandas(df_test[['topic_text', 'Label']], fields)\n",
    "train.finalize_fields()\n",
    "\n",
    "glove = GloVe()\n",
    "embeddings = glove.load_vocab(vocab)\n",
    "\n",
    "train_batch = train.batch(add_padding=True)\n",
    "validation_batch = validation.batch(add_padding=True)\n",
    "test_batch = test.batch(add_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "e707fbc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e707fbc3",
    "outputId": "68f44c29-513e-40cf-a62d-f02ee2cb168c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0, 41,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [46,  0, 67,  0,  0, 56, 21,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch['text'].astype(int)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "df0d0eed",
   "metadata": {
    "id": "df0d0eed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_train = embeddings[train_batch['text'].astype(int)]\n",
    "tweet_validation = embeddings[validation_batch['text'].astype(int)]\n",
    "tweet_test = embeddings[test_batch['text'].astype(int)]\n",
    "\n",
    "# Mean\n",
    "tweet_train_mean = tweet_train.mean(axis=1)\n",
    "tweet_validation_mean = tweet_validation.mean(axis=1)\n",
    "tweet_test_mean = tweet_test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "cf850683",
   "metadata": {
    "id": "cf850683",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_train_mean_df = pd.DataFrame(tweet_train_mean)\n",
    "df_train = pd.merge(df_train, embeddings_train_mean_df, left_index=True, right_index=True)\n",
    "\n",
    "embeddings_validation_mean_df = pd.DataFrame(tweet_validation_mean)\n",
    "df_validation = pd.merge(df_validation, embeddings_validation_mean_df, left_index=True, right_index=True)\n",
    "\n",
    "embeddings_test_mean_df = pd.DataFrame(tweet_test_mean)\n",
    "df_test = pd.merge(df_test, embeddings_test_mean_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "470341bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "470341bb",
    "outputId": "e580e876-4810-4056-a94c-62109a8a1e33",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>all_uppercase_count</th>\n",
       "      <th>all_lowercase_count</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2698</td>\n",
       "      <td>0</td>\n",
       "      <td>#OPEC #chief #defends #policy, #says #group to...</td>\n",
       "      <td>0</td>\n",
       "      <td>opec chief defends policy says group try ride ...</td>\n",
       "      <td>[opec, chief, defend, policy, group, try, ride...</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133291</td>\n",
       "      <td>0.373946</td>\n",
       "      <td>-0.605286</td>\n",
       "      <td>-0.488337</td>\n",
       "      <td>-1.580123</td>\n",
       "      <td>0.299884</td>\n",
       "      <td>0.277527</td>\n",
       "      <td>1.056188</td>\n",
       "      <td>-0.905541</td>\n",
       "      <td>-0.476995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide ANY source that shows it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide any source that shows</td>\n",
       "      <td>[provide, source]</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.498801</td>\n",
       "      <td>0.739389</td>\n",
       "      <td>-0.782420</td>\n",
       "      <td>-0.765940</td>\n",
       "      <td>-1.578567</td>\n",
       "      <td>0.666944</td>\n",
       "      <td>-0.028898</td>\n",
       "      <td>0.991089</td>\n",
       "      <td>-1.259082</td>\n",
       "      <td>-0.359712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>0</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>[sick]</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554514</td>\n",
       "      <td>0.789464</td>\n",
       "      <td>-0.803408</td>\n",
       "      <td>-0.806830</td>\n",
       "      <td>-1.564054</td>\n",
       "      <td>0.717324</td>\n",
       "      <td>-0.079272</td>\n",
       "      <td>0.973560</td>\n",
       "      <td>-1.306909</td>\n",
       "      <td>-0.336406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1770</td>\n",
       "      <td>1</td>\n",
       "      <td>Quel domage  RT @CFL_News: No Canada: Bills in...</td>\n",
       "      <td>0</td>\n",
       "      <td>quel domage canada bills series which was colo...</td>\n",
       "      <td>[quel, domage, canada, bills, series, colossal...</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053098</td>\n",
       "      <td>0.338785</td>\n",
       "      <td>-0.614522</td>\n",
       "      <td>-0.438817</td>\n",
       "      <td>-1.694665</td>\n",
       "      <td>0.263905</td>\n",
       "      <td>0.374092</td>\n",
       "      <td>1.131317</td>\n",
       "      <td>-0.876467</td>\n",
       "      <td>-0.546164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2590</td>\n",
       "      <td>0</td>\n",
       "      <td>@PaulGoonerW yeah stressed isnt the word could...</td>\n",
       "      <td>1</td>\n",
       "      <td>yeah stressed isnt the word could easily punch...</td>\n",
       "      <td>[yeah, stressed, word, easily, punch, face, ri...</td>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169351</td>\n",
       "      <td>0.400346</td>\n",
       "      <td>-0.550667</td>\n",
       "      <td>-0.469496</td>\n",
       "      <td>-1.365752</td>\n",
       "      <td>0.274819</td>\n",
       "      <td>0.222147</td>\n",
       "      <td>0.925074</td>\n",
       "      <td>-0.829983</td>\n",
       "      <td>-0.366258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label                                         Tweet text  \\\n",
       "0         2698      0  #OPEC #chief #defends #policy, #says #group to...   \n",
       "1         1185      0  yet you cant provide ANY source that shows it ...   \n",
       "2         1268      1                                   also sick names    \n",
       "3         1770      1  Quel domage  RT @CFL_News: No Canada: Bills in...   \n",
       "4         2590      0  @PaulGoonerW yeah stressed isnt the word could...   \n",
       "\n",
       "   emoji_count                                         clean_text  \\\n",
       "0            0  opec chief defends policy says group try ride ...   \n",
       "1            0         yet you cant provide any source that shows   \n",
       "2            0                                    also sick names   \n",
       "3            0  quel domage canada bills series which was colo...   \n",
       "4            1  yeah stressed isnt the word could easily punch...   \n",
       "\n",
       "                                              lemmas  word_count  char_count  \\\n",
       "0  [opec, chief, defend, policy, group, try, ride...          12          53   \n",
       "1                                  [provide, source]           8          35   \n",
       "2                                             [sick]           3          13   \n",
       "3  [quel, domage, canada, bills, series, colossal...          14          69   \n",
       "4  [yeah, stressed, word, easily, punch, face, ri...          16         101   \n",
       "\n",
       "   all_uppercase_count  all_lowercase_count  ...       290       291  \\\n",
       "0                    0                    3  ... -0.133291  0.373946   \n",
       "1                    1                    8  ... -0.498801  0.739389   \n",
       "2                    0                    3  ... -0.554514  0.789464   \n",
       "3                    2                   11  ... -0.053098  0.338785   \n",
       "4                    0                   17  ... -0.169351  0.400346   \n",
       "\n",
       "        292       293       294       295       296       297       298  \\\n",
       "0 -0.605286 -0.488337 -1.580123  0.299884  0.277527  1.056188 -0.905541   \n",
       "1 -0.782420 -0.765940 -1.578567  0.666944 -0.028898  0.991089 -1.259082   \n",
       "2 -0.803408 -0.806830 -1.564054  0.717324 -0.079272  0.973560 -1.306909   \n",
       "3 -0.614522 -0.438817 -1.694665  0.263905  0.374092  1.131317 -0.876467   \n",
       "4 -0.550667 -0.469496 -1.365752  0.274819  0.222147  0.925074 -0.829983   \n",
       "\n",
       "        299  \n",
       "0 -0.476995  \n",
       "1 -0.359712  \n",
       "2 -0.336406  \n",
       "3 -0.546164  \n",
       "4 -0.366258  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "a8c44fb1",
   "metadata": {
    "id": "a8c44fb1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Features\n",
    "## Broj neg rijeci\n",
    "## Broj poz rijeci\n",
    "## Omjer\n",
    "## Udaljenost izmedu poz i neg rijeci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "1208a12f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1208a12f",
    "outputId": "aa68c64c-f083-4235-df46-4027f5ef3168"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon') # if error run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "af3801bc",
   "metadata": {
    "id": "af3801bc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pos_neg_words(df, limit):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    neg_words = []\n",
    "    neg_word_count = []\n",
    "    pos_words = []\n",
    "    pos_word_count = []\n",
    "    for index, row in df.iterrows():\n",
    "        lemmas = []\n",
    "        if len(row['topic_text']) > 0:\n",
    "            doc = nlp(row['topic_text'])\n",
    "            for token in doc:\n",
    "                lemmas.append(token.lemma_)\n",
    "\n",
    "            current_pos = []\n",
    "            current_neut = []\n",
    "            current_neg = []\n",
    "            for word in lemmas:\n",
    "                if (sid.polarity_scores(word)['compound']) >= limit:\n",
    "                    current_pos.append(word)\n",
    "                elif (sid.polarity_scores(word)['compound']) <= -limit:\n",
    "                    current_neg.append(word)\n",
    "                else:\n",
    "                    current_neut.append(word)\n",
    "\n",
    "            neg_words.append(deepcopy(current_neg))\n",
    "            neg_word_count.append(deepcopy(len(current_neg)))\n",
    "            pos_words.append(deepcopy(current_pos))\n",
    "            pos_word_count.append(deepcopy(len(current_pos)))\n",
    "        else:\n",
    "            neg_words.append([])\n",
    "            neg_word_count.append(0)\n",
    "            pos_words.append([])\n",
    "            pos_word_count.append(0)\n",
    "    return neg_words, neg_word_count, pos_words, pos_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "1447d136",
   "metadata": {
    "id": "1447d136",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg_words_train, neg_word_count_train, pos_words_train, pos_word_count_train = pos_neg_words(df_train, 0.2)\n",
    "neg_words_val, neg_word_count_val, pos_words_val, pos_word_count_val = pos_neg_words(df_validation, 0.2)\n",
    "neg_words_test, neg_word_count_test, pos_words_test, pos_word_count_test = pos_neg_words(df_test, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "5c4c71a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "5c4c71a3",
    "outputId": "1738c41e-41dc-4738-b858-0d45ce47976b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>all_uppercase_count</th>\n",
       "      <th>all_lowercase_count</th>\n",
       "      <th>...</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>neg_word_count</th>\n",
       "      <th>pos_word</th>\n",
       "      <th>pos_word_count</th>\n",
       "      <th>neg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2698</td>\n",
       "      <td>0</td>\n",
       "      <td>#OPEC #chief #defends #policy, #says #group to...</td>\n",
       "      <td>0</td>\n",
       "      <td>opec chief defends policy says group try ride ...</td>\n",
       "      <td>[opec, chief, defend, policy, group, try, ride...</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.580123</td>\n",
       "      <td>0.299884</td>\n",
       "      <td>0.277527</td>\n",
       "      <td>1.056188</td>\n",
       "      <td>-0.905541</td>\n",
       "      <td>-0.476995</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide ANY source that shows it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide any source that shows</td>\n",
       "      <td>[provide, source]</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.578567</td>\n",
       "      <td>0.666944</td>\n",
       "      <td>-0.028898</td>\n",
       "      <td>0.991089</td>\n",
       "      <td>-1.259082</td>\n",
       "      <td>-0.359712</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>0</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>[sick]</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.564054</td>\n",
       "      <td>0.717324</td>\n",
       "      <td>-0.079272</td>\n",
       "      <td>0.973560</td>\n",
       "      <td>-1.306909</td>\n",
       "      <td>-0.336406</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[sick]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1770</td>\n",
       "      <td>1</td>\n",
       "      <td>Quel domage  RT @CFL_News: No Canada: Bills in...</td>\n",
       "      <td>0</td>\n",
       "      <td>quel domage canada bills series which was colo...</td>\n",
       "      <td>[quel, domage, canada, bills, series, colossal...</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.694665</td>\n",
       "      <td>0.263905</td>\n",
       "      <td>0.374092</td>\n",
       "      <td>1.131317</td>\n",
       "      <td>-0.876467</td>\n",
       "      <td>-0.546164</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[flop, cancel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2590</td>\n",
       "      <td>0</td>\n",
       "      <td>@PaulGoonerW yeah stressed isnt the word could...</td>\n",
       "      <td>1</td>\n",
       "      <td>yeah stressed isnt the word could easily punch...</td>\n",
       "      <td>[yeah, stressed, word, easily, punch, face, ri...</td>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.365752</td>\n",
       "      <td>0.274819</td>\n",
       "      <td>0.222147</td>\n",
       "      <td>0.925074</td>\n",
       "      <td>-0.829983</td>\n",
       "      <td>-0.366258</td>\n",
       "      <td>1</td>\n",
       "      <td>[yeah, easily]</td>\n",
       "      <td>2</td>\n",
       "      <td>[stressed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label                                         Tweet text  \\\n",
       "0         2698      0  #OPEC #chief #defends #policy, #says #group to...   \n",
       "1         1185      0  yet you cant provide ANY source that shows it ...   \n",
       "2         1268      1                                   also sick names    \n",
       "3         1770      1  Quel domage  RT @CFL_News: No Canada: Bills in...   \n",
       "4         2590      0  @PaulGoonerW yeah stressed isnt the word could...   \n",
       "\n",
       "   emoji_count                                         clean_text  \\\n",
       "0            0  opec chief defends policy says group try ride ...   \n",
       "1            0         yet you cant provide any source that shows   \n",
       "2            0                                    also sick names   \n",
       "3            0  quel domage canada bills series which was colo...   \n",
       "4            1  yeah stressed isnt the word could easily punch...   \n",
       "\n",
       "                                              lemmas  word_count  char_count  \\\n",
       "0  [opec, chief, defend, policy, group, try, ride...          12          53   \n",
       "1                                  [provide, source]           8          35   \n",
       "2                                             [sick]           3          13   \n",
       "3  [quel, domage, canada, bills, series, colossal...          14          69   \n",
       "4  [yeah, stressed, word, easily, punch, face, ri...          16         101   \n",
       "\n",
       "   all_uppercase_count  all_lowercase_count  ...       294       295  \\\n",
       "0                    0                    3  ... -1.580123  0.299884   \n",
       "1                    1                    8  ... -1.578567  0.666944   \n",
       "2                    0                    3  ... -1.564054  0.717324   \n",
       "3                    2                   11  ... -1.694665  0.263905   \n",
       "4                    0                   17  ... -1.365752  0.274819   \n",
       "\n",
       "        296       297       298       299  neg_word_count        pos_word  \\\n",
       "0  0.277527  1.056188 -0.905541 -0.476995               0              []   \n",
       "1 -0.028898  0.991089 -1.259082 -0.359712               0              []   \n",
       "2 -0.079272  0.973560 -1.306909 -0.336406               1              []   \n",
       "3  0.374092  1.131317 -0.876467 -0.546164               2              []   \n",
       "4  0.222147  0.925074 -0.829983 -0.366258               1  [yeah, easily]   \n",
       "\n",
       "   pos_word_count        neg_word  \n",
       "0               0              []  \n",
       "1               0              []  \n",
       "2               0          [sick]  \n",
       "3               0  [flop, cancel]  \n",
       "4               2      [stressed]  \n",
       "\n",
       "[5 rows x 329 columns]"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['neg_word_count'] = neg_word_count_train\n",
    "df_train['pos_word'] = pos_words_train\n",
    "df_train['pos_word_count'] = pos_word_count_train\n",
    "df_train['neg_word'] = neg_words_train\n",
    "\n",
    "df_validation['neg_word_count'] = neg_word_count_val\n",
    "df_validation['pos_word'] = pos_words_val\n",
    "df_validation['pos_word_count'] = pos_word_count_val\n",
    "df_validation['neg_word'] = neg_words_val\n",
    "\n",
    "df_test['neg_word_count'] = neg_word_count_test\n",
    "df_test['pos_word'] = pos_words_test\n",
    "df_test['pos_word_count'] = pos_word_count_test\n",
    "df_test['neg_word'] = neg_words_test\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "b646a702",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1_topic',\n",
       " '0_topic',\n",
       " '1_topic',\n",
       " '2_topic',\n",
       " '3_topic',\n",
       " '4_topic',\n",
       " '5_topic',\n",
       " '6_topic',\n",
       " '7_topic',\n",
       " '8_topic',\n",
       " '9_topic',\n",
       " '10_topic',\n",
       " '11_topic',\n",
       " '12_topic',\n",
       " '13_topic',\n",
       " '14_topic',\n",
       " '15_topic',\n",
       " '16_topic',\n",
       " '17_topic']"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dummies = []\n",
    "for i in topic_model.get_topic_info()['Topic'].values:\n",
    "    topic_dummies.append(f'{i}_topic')\n",
    "    \n",
    "topic_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "cd7b3407",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>all_uppercase_count</th>\n",
       "      <th>all_lowercase_count</th>\n",
       "      <th>...</th>\n",
       "      <th>8_topic</th>\n",
       "      <th>9_topic</th>\n",
       "      <th>10_topic</th>\n",
       "      <th>11_topic</th>\n",
       "      <th>12_topic</th>\n",
       "      <th>13_topic</th>\n",
       "      <th>14_topic</th>\n",
       "      <th>15_topic</th>\n",
       "      <th>16_topic</th>\n",
       "      <th>17_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2698</td>\n",
       "      <td>0</td>\n",
       "      <td>#OPEC #chief #defends #policy, #says #group to...</td>\n",
       "      <td>0</td>\n",
       "      <td>opec chief defends policy says group try ride ...</td>\n",
       "      <td>[opec, chief, defend, policy, group, try, ride...</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide ANY source that shows it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide any source that shows</td>\n",
       "      <td>[provide, source]</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>0</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>[sick]</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1770</td>\n",
       "      <td>1</td>\n",
       "      <td>Quel domage  RT @CFL_News: No Canada: Bills in...</td>\n",
       "      <td>0</td>\n",
       "      <td>quel domage canada bills series which was colo...</td>\n",
       "      <td>[quel, domage, canada, bills, series, colossal...</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2590</td>\n",
       "      <td>0</td>\n",
       "      <td>@PaulGoonerW yeah stressed isnt the word could...</td>\n",
       "      <td>1</td>\n",
       "      <td>yeah stressed isnt the word could easily punch...</td>\n",
       "      <td>[yeah, stressed, word, easily, punch, face, ri...</td>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 348 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label                                         Tweet text  \\\n",
       "0         2698      0  #OPEC #chief #defends #policy, #says #group to...   \n",
       "1         1185      0  yet you cant provide ANY source that shows it ...   \n",
       "2         1268      1                                   also sick names    \n",
       "3         1770      1  Quel domage  RT @CFL_News: No Canada: Bills in...   \n",
       "4         2590      0  @PaulGoonerW yeah stressed isnt the word could...   \n",
       "\n",
       "   emoji_count                                         clean_text  \\\n",
       "0            0  opec chief defends policy says group try ride ...   \n",
       "1            0         yet you cant provide any source that shows   \n",
       "2            0                                    also sick names   \n",
       "3            0  quel domage canada bills series which was colo...   \n",
       "4            1  yeah stressed isnt the word could easily punch...   \n",
       "\n",
       "                                              lemmas  word_count  char_count  \\\n",
       "0  [opec, chief, defend, policy, group, try, ride...          12          53   \n",
       "1                                  [provide, source]           8          35   \n",
       "2                                             [sick]           3          13   \n",
       "3  [quel, domage, canada, bills, series, colossal...          14          69   \n",
       "4  [yeah, stressed, word, easily, punch, face, ri...          16         101   \n",
       "\n",
       "   all_uppercase_count  all_lowercase_count  ...  8_topic  9_topic  10_topic  \\\n",
       "0                    0                    3  ...        0        0         0   \n",
       "1                    1                    8  ...        0        0         0   \n",
       "2                    0                    3  ...        0        0         0   \n",
       "3                    2                   11  ...        0        0         0   \n",
       "4                    0                   17  ...        0        0         0   \n",
       "\n",
       "   11_topic  12_topic  13_topic  14_topic  15_topic  16_topic  17_topic  \n",
       "0         0         0         0         0         0         0         0  \n",
       "1         0         0         0         0         0         0         0  \n",
       "2         0         0         0         0         0         0         0  \n",
       "3         0         0         0         0         0         0         0  \n",
       "4         0         0         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 348 columns]"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[topic_dummies] = pd.get_dummies(df_train['topic'])\n",
    "df_validation[topic_dummies] = pd.get_dummies(df_validation['topic'])\n",
    "df_test[topic_dummies] = pd.get_dummies(df_test['topic'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "f49072c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f49072c8",
    "outputId": "0f34e424-9c9a-4c8c-ac9d-379d9f001dec",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_ = df_train[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "#                '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "# y_ = df_train['Label']\n",
    "# clf = LogisticRegression(random_state=0, solver='liblinear').fit(X_, y_)\n",
    "\n",
    "# print('Train score')\n",
    "# print(clf.score(X_, y_))\n",
    "\n",
    "# X_val = df_validation[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "#                '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "# y_val = df_validation['Label']\n",
    "\n",
    "# print('Validation score')\n",
    "# print(clf.score(X_val, y_val))\n",
    "\n",
    "# X_val = df_test[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "#                '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "# y_val = df_test['Label']\n",
    "# print('Test score')\n",
    "# print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "a9f4718f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9f4718f",
    "outputId": "f53f55da-c8b7-426a-ef27-c85cc0f0b1b4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text        whoever runs yeovil town twitter account shoul...\n",
       "pos_word                                                     [like]\n",
       "pos_word_count                                                    1\n",
       "neg_word                                                     [fire]\n",
       "neg_word_count                                                    1\n",
       "Name: 25, dtype: object"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pos and neg words within 4 words\n",
    "df_train[['clean_text', 'pos_word', 'pos_word_count', 'neg_word', 'neg_word_count']].iloc[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "fadf4766",
   "metadata": {
    "id": "fadf4766",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pos_neg_within_n(df, n=4):\n",
    "\n",
    "    ret_array = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['pos_word_count'] > 0 and row['neg_word_count'] > 0:\n",
    "            doc = nlp(row['clean_text'])\n",
    "            lemmas = []\n",
    "            for token in doc:\n",
    "                lemmas.append(token.lemma_)\n",
    "\n",
    "            pos_indexes = np.array([])\n",
    "            for word in row['pos_word']:\n",
    "                pos_indexes = np.append(pos_indexes, np.where(np.array(lemmas) == word))\n",
    "            neg_indexes = np.array([])\n",
    "            for word in row['neg_word']:\n",
    "                neg_indexes = np.append(neg_indexes, np.where(np.array(lemmas) == word))\n",
    "\n",
    "            bool_val = 0\n",
    "            for idx in pos_indexes:\n",
    "                if (abs(neg_indexes-idx) < n).any():\n",
    "                    bool_val = 1\n",
    "            ret_array.append(deepcopy(bool_val))\n",
    "        else:\n",
    "            ret_array.append(0)\n",
    "    return ret_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "a9367e01",
   "metadata": {
    "id": "a9367e01",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "within_5_train = pos_neg_within_n(df_train, n=5)\n",
    "within_5_val = pos_neg_within_n(df_validation, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "071f78a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "071f78a5",
    "outputId": "3ba0ff1a-9b49-4143-e83c-c112a67c5c5c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>all_uppercase_count</th>\n",
       "      <th>all_lowercase_count</th>\n",
       "      <th>...</th>\n",
       "      <th>9_topic</th>\n",
       "      <th>10_topic</th>\n",
       "      <th>11_topic</th>\n",
       "      <th>12_topic</th>\n",
       "      <th>13_topic</th>\n",
       "      <th>14_topic</th>\n",
       "      <th>15_topic</th>\n",
       "      <th>16_topic</th>\n",
       "      <th>17_topic</th>\n",
       "      <th>pos_neg_within_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2698</td>\n",
       "      <td>0</td>\n",
       "      <td>#OPEC #chief #defends #policy, #says #group to...</td>\n",
       "      <td>0</td>\n",
       "      <td>opec chief defends policy says group try ride ...</td>\n",
       "      <td>[opec, chief, defend, policy, group, try, ride...</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide ANY source that shows it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yet you cant provide any source that shows</td>\n",
       "      <td>[provide, source]</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>0</td>\n",
       "      <td>also sick names</td>\n",
       "      <td>[sick]</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1770</td>\n",
       "      <td>1</td>\n",
       "      <td>Quel domage  RT @CFL_News: No Canada: Bills in...</td>\n",
       "      <td>0</td>\n",
       "      <td>quel domage canada bills series which was colo...</td>\n",
       "      <td>[quel, domage, canada, bills, series, colossal...</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2590</td>\n",
       "      <td>0</td>\n",
       "      <td>@PaulGoonerW yeah stressed isnt the word could...</td>\n",
       "      <td>1</td>\n",
       "      <td>yeah stressed isnt the word could easily punch...</td>\n",
       "      <td>[yeah, stressed, word, easily, punch, face, ri...</td>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label                                         Tweet text  \\\n",
       "0         2698      0  #OPEC #chief #defends #policy, #says #group to...   \n",
       "1         1185      0  yet you cant provide ANY source that shows it ...   \n",
       "2         1268      1                                   also sick names    \n",
       "3         1770      1  Quel domage  RT @CFL_News: No Canada: Bills in...   \n",
       "4         2590      0  @PaulGoonerW yeah stressed isnt the word could...   \n",
       "\n",
       "   emoji_count                                         clean_text  \\\n",
       "0            0  opec chief defends policy says group try ride ...   \n",
       "1            0         yet you cant provide any source that shows   \n",
       "2            0                                    also sick names   \n",
       "3            0  quel domage canada bills series which was colo...   \n",
       "4            1  yeah stressed isnt the word could easily punch...   \n",
       "\n",
       "                                              lemmas  word_count  char_count  \\\n",
       "0  [opec, chief, defend, policy, group, try, ride...          12          53   \n",
       "1                                  [provide, source]           8          35   \n",
       "2                                             [sick]           3          13   \n",
       "3  [quel, domage, canada, bills, series, colossal...          14          69   \n",
       "4  [yeah, stressed, word, easily, punch, face, ri...          16         101   \n",
       "\n",
       "   all_uppercase_count  all_lowercase_count  ...  9_topic  10_topic  11_topic  \\\n",
       "0                    0                    3  ...        0         0         0   \n",
       "1                    1                    8  ...        0         0         0   \n",
       "2                    0                    3  ...        0         0         0   \n",
       "3                    2                   11  ...        0         0         0   \n",
       "4                    0                   17  ...        0         0         0   \n",
       "\n",
       "   12_topic  13_topic  14_topic  15_topic  16_topic  17_topic  \\\n",
       "0         0         0         0         0         0         0   \n",
       "1         0         0         0         0         0         0   \n",
       "2         0         0         0         0         0         0   \n",
       "3         0         0         0         0         0         0   \n",
       "4         0         0         0         0         0         0   \n",
       "\n",
       "   pos_neg_within_5  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 1  \n",
       "\n",
       "[5 rows x 349 columns]"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['pos_neg_within_5'] = within_5_train\n",
    "df_validation['pos_neg_within_5'] = within_5_val\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "6a917861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a917861",
    "outputId": "38f3877f-6bef-45bf-cb9d-c1ec2f433743",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score\n",
      "0.5183383233532934\n",
      "Validation score\n",
      "0.5152838427947598\n"
     ]
    }
   ],
   "source": [
    "X_ = df_train[[ 'pos_neg_within_5']]\n",
    "y_ = df_train['Label']\n",
    "\n",
    "clf = LogisticRegression().fit(X_, y_)\n",
    "print('Train score')\n",
    "print(clf.score(X_, y_))\n",
    "\n",
    "X_val = df_validation[[ 'pos_neg_within_5']]\n",
    "y_val = df_validation['Label']\n",
    "\n",
    "print('Validation score')\n",
    "print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b6b85",
   "metadata": {},
   "source": [
    "## Baseline classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "8f1446bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = ['word_count', 'char_count', 'all_uppercase_count', 'all_lowercase_count', 'capitalised_count', 'digit_count']\n",
    "\n",
    "y_train = df_train['Label']\n",
    "x_train = df_train[baseline_features]\n",
    "\n",
    "y_validation = df_validation['Label']\n",
    "x_validation = df_validation[baseline_features]\n",
    "\n",
    "y_test = df_test['Label']\n",
    "x_test = df_test[baseline_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "1c358297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>all_uppercase_count</th>\n",
       "      <th>all_lowercase_count</th>\n",
       "      <th>capitalised_count</th>\n",
       "      <th>digit_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  char_count  all_uppercase_count  all_lowercase_count  \\\n",
       "0          12          53                    0                    3   \n",
       "1           8          35                    1                    8   \n",
       "2           3          13                    0                    3   \n",
       "3          14          69                    2                   11   \n",
       "4          16         101                    0                   17   \n",
       "\n",
       "   capitalised_count  digit_count  \n",
       "0                  0            4  \n",
       "1                  1            9  \n",
       "2                  0            0  \n",
       "3                  4            2  \n",
       "4                  0            0  "
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "10bfde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_evaluate(model, x_train, y_train, x_validation, y_validation, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    print('-------------------------- TRAIN --------------------------')\n",
    "    print(classification_report(y_train, y_train_pred, digits=3), 2*'\\n')\n",
    "    \n",
    "    y_validation_pred = model.predict(x_validation)\n",
    "    print('----------------------- VALIDATION ------------------------')\n",
    "    print(classification_report(y_validation, y_validation_pred, digits=3), 2*'\\n')\n",
    "    \n",
    "    y_test_pred = model.predict(x_test)\n",
    "    print('------------------------- TEST ---------------------------')\n",
    "    print(classification_report(y_test, y_test_pred, digits=3), 2*'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680c3de",
   "metadata": {},
   "source": [
    "## Global baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "2a4ea71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.626     0.451     0.524      1337\n",
      "           1      0.570     0.730     0.640      1335\n",
      "\n",
      "    accuracy                          0.590      2672\n",
      "   macro avg      0.598     0.590     0.582      2672\n",
      "weighted avg      0.598     0.590     0.582      2672\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.636     0.437     0.518       579\n",
      "           1      0.564     0.744     0.641       566\n",
      "\n",
      "    accuracy                          0.589      1145\n",
      "   macro avg      0.600     0.590     0.580      1145\n",
      "weighted avg      0.600     0.589     0.579      1145\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.649     0.383     0.481       473\n",
      "           1      0.422     0.685     0.522       311\n",
      "\n",
      "    accuracy                          0.503       784\n",
      "   macro avg      0.535     0.534     0.502       784\n",
      "weighted avg      0.559     0.503     0.498       784\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1eb5de",
   "metadata": {},
   "source": [
    "## Global best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "feab4dec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [813]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselection__k\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpf__degree\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr__solver\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     18\u001b[0m search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline_lr, param_grid\u001b[38;5;241m=\u001b[39mparams, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    381\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 382\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:1158\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1153\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[0;32m   1157\u001b[0m         )\n\u001b[1;32m-> 1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\svm\\_base.py:1205\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1202\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   1204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[1;32m-> 1205\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misspmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m n_iter_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline_lr = Pipeline([\n",
    "    ('selection', SelectKBest()), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pf', PolynomialFeatures()), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'selection__k': list(range(2, 20, 3)),\n",
    "    'pf__degree': [2, 3, 4, 5],\n",
    "    'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'lr__class_weight': ['balanced', None],\n",
    "    'lr__C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 5],\n",
    "    'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline_lr, param_grid=params, cv=5)\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "print(f'Best score: {search.best_score_}')\n",
    "print(f'Best score: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_best = Pipeline([\n",
    "    ('selection', SelectKBest(k=5)), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pf', PolynomialFeatures(degree=3)),\n",
    "    ('lr', LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))\n",
    "])\n",
    "\n",
    "model_fit_evaluate(pipeline_lr_best, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "545612e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5916797927823865\n",
      "Best score: {'lr__C': 0.1, 'lr__penalty': 'l1', 'lr__solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr = Pipeline([\n",
    "    #('selection', SelectKBest()), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    #'selection__k': list(range(2, 20, 3)),\n",
    "    'lr__penalty': ['l1'], \n",
    "    'lr__C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 5],\n",
    "    'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline_lr, param_grid=params, cv=5)\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "print(f'Best score: {search.best_score_}')\n",
    "print(f'Best score: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_best = Pipeline([\n",
    "    ('selection', SelectKBest(k=8)), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pf', PolynomialFeatures(degree=3)),\n",
    "    ('lr', LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))\n",
    "])\n",
    "\n",
    "model_fit_evaluate(pipeline_lr_best, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072cda6",
   "metadata": {},
   "source": [
    "## Baseline classifiers for the first 4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "120eb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validation_and_test_for_topic(topic_num, df_train, df_validation, df_test):\n",
    "    df_train_topic = df_train[df_train[f'{topic_num}_topic'] == 1]\n",
    "    df_validation_topic = df_train[df_train[f'{topic_num}_topic'] == 1]\n",
    "    df_test_topic = df_train[df_train[f'{topic_num}_topic'] == 1]\n",
    "    \n",
    "    y_train = df_train_topic['Label']\n",
    "    x_train = df_train_topic[baseline_features]\n",
    "\n",
    "    y_validation = df_validation_topic['Label']\n",
    "    x_validation = df_validation_topic[baseline_features]\n",
    "\n",
    "    y_test = df_test_topic['Label']\n",
    "    x_test = df_test_topic[baseline_features]\n",
    "\n",
    "    return x_train, y_train, x_validation, y_validation, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "b64885b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.617     0.501     0.553       599\n",
      "           1      0.564     0.675     0.615       573\n",
      "\n",
      "    accuracy                          0.586      1172\n",
      "   macro avg      0.591     0.588     0.584      1172\n",
      "weighted avg      0.591     0.586     0.583      1172\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.617     0.501     0.553       599\n",
      "           1      0.564     0.675     0.615       573\n",
      "\n",
      "    accuracy                          0.586      1172\n",
      "   macro avg      0.591     0.588     0.584      1172\n",
      "weighted avg      0.591     0.586     0.583      1172\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.617     0.501     0.553       599\n",
      "           1      0.564     0.675     0.615       573\n",
      "\n",
      "    accuracy                          0.586      1172\n",
      "   macro avg      0.591     0.588     0.584      1172\n",
      "weighted avg      0.591     0.586     0.583      1172\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_ = \\\n",
    "    get_train_validation_and_test_for_topic(-1, df_train, df_validation, df_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "f51a8bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.632     0.542     0.583       168\n",
      "           1      0.613     0.697     0.652       175\n",
      "\n",
      "    accuracy                          0.621       343\n",
      "   macro avg      0.623     0.619     0.618       343\n",
      "weighted avg      0.622     0.621     0.619       343\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.632     0.542     0.583       168\n",
      "           1      0.613     0.697     0.652       175\n",
      "\n",
      "    accuracy                          0.621       343\n",
      "   macro avg      0.623     0.619     0.618       343\n",
      "weighted avg      0.622     0.621     0.619       343\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.632     0.542     0.583       168\n",
      "           1      0.613     0.697     0.652       175\n",
      "\n",
      "    accuracy                          0.621       343\n",
      "   macro avg      0.623     0.619     0.618       343\n",
      "weighted avg      0.622     0.621     0.619       343\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_ = \\\n",
    "    get_train_validation_and_test_for_topic(0, df_train, df_validation, df_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "4c7e7bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.606     0.679     0.640        84\n",
      "           1      0.571     0.493     0.529        73\n",
      "\n",
      "    accuracy                          0.592       157\n",
      "   macro avg      0.589     0.586     0.585       157\n",
      "weighted avg      0.590     0.592     0.589       157\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.606     0.679     0.640        84\n",
      "           1      0.571     0.493     0.529        73\n",
      "\n",
      "    accuracy                          0.592       157\n",
      "   macro avg      0.589     0.586     0.585       157\n",
      "weighted avg      0.590     0.592     0.589       157\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.606     0.679     0.640        84\n",
      "           1      0.571     0.493     0.529        73\n",
      "\n",
      "    accuracy                          0.592       157\n",
      "   macro avg      0.589     0.586     0.585       157\n",
      "weighted avg      0.590     0.592     0.589       157\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_ = \\\n",
    "    get_train_validation_and_test_for_topic(1, df_train, df_validation, df_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "feb7bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.657     0.878     0.751        74\n",
      "           1      0.591     0.277     0.377        47\n",
      "\n",
      "    accuracy                          0.645       121\n",
      "   macro avg      0.624     0.577     0.564       121\n",
      "weighted avg      0.631     0.645     0.606       121\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.657     0.878     0.751        74\n",
      "           1      0.591     0.277     0.377        47\n",
      "\n",
      "    accuracy                          0.645       121\n",
      "   macro avg      0.624     0.577     0.564       121\n",
      "weighted avg      0.631     0.645     0.606       121\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.657     0.878     0.751        74\n",
      "           1      0.591     0.277     0.377        47\n",
      "\n",
      "    accuracy                          0.645       121\n",
      "   macro avg      0.624     0.577     0.564       121\n",
      "weighted avg      0.631     0.645     0.606       121\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_ = \\\n",
    "    get_train_validation_and_test_for_topic(2, df_train, df_validation, df_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HJvVNgSQGXBE",
   "metadata": {
    "id": "HJvVNgSQGXBE"
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y7QInQCTGXF9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7QInQCTGXF9",
    "outputId": "09832e27-4753-481a-c53a-9ff38ae7507b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94000e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793da6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pipeline_lr_best['lr'].coef_.flatten()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.bar(list(range(len(weights))), weights)\n",
    "plt.ylabel('Value of the weight')\n",
    "plt.xlabel('Index of the weight')\n",
    "plt.title('Feature importance plot for logistic regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w7iw-WmyGXKC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7iw-WmyGXKC",
    "outputId": "89cd15f7-2fa4-4922-ce48-c55af7211d87"
   },
   "outputs": [],
   "source": [
    "pipeline_dtc = Pipeline([\n",
    "    ('selection', SelectKBest()), \n",
    "    ('dtc', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'selection__k': list(range(2, 20, 3)),\n",
    "    'dtc__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'dtc__ccp_alpha': [0.1, .01, .001],\n",
    "    'dtc__max_depth' : list(range(2, 10)),\n",
    "    'dtc__criterion' : ['gini', 'entropy', 'log_loss']\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipeline_dtc, param_grid=params, cv=5)\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "print(f'Best score: {search.best_score_}')\n",
    "print(f'Best score: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dtc_best = Pipeline([\n",
    "    ('selection', SelectKBest(k=5)), \n",
    "    ('dtc', DecisionTreeClassifier(ccp_alpha=0.001, criterion='entropy', max_depth=5, max_features='auto')),\n",
    "])\n",
    "\n",
    "model_fit_evaluate(pipeline_dtc_best, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pipeline_dtc_best['dtc'].feature_importances_.flatten()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.bar(list(range(len(feat_importances))), feat_importances)\n",
    "plt.ylabel('Feature importance value')\n",
    "plt.xlabel('Index of the feature')\n",
    "plt.title('Feature importance plot for decision tree')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
