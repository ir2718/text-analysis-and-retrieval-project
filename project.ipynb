{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "10rFQ6qgEfwG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10rFQ6qgEfwG",
    "outputId": "f1eb8986-5c3b-4ae1-8c9a-767bffd653ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "JxgMNAW8EjHe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxgMNAW8EjHe",
    "outputId": "996ef455-5621-4268-8359-7e2101546f00"
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "emet_2xgEjLL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emet_2xgEjLL",
    "outputId": "88cc1f0e-389f-48c4-d2ba-6eaccc5580b7"
   },
   "outputs": [],
   "source": [
    "# pip install podium-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "1a8d4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "VDPtYtvQElF0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDPtYtvQElF0",
    "outputId": "ee94560a-7cb6-440a-a027-cac624e26a64"
   },
   "outputs": [],
   "source": [
    "# pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "PCRcR-b_ExaN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCRcR-b_ExaN",
    "outputId": "7d142e0b-73c2-4433-dd65-1dbbdf71d0c5"
   },
   "outputs": [],
   "source": [
    "# pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "37c2bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wordsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "463d0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ekphrasis -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "d7b9ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install contextualSpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77de93e4",
   "metadata": {
    "id": "77de93e4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bertopic import BERTopic\n",
    "from podium import Vocab, Field, LabelField\n",
    "from podium.datasets import TabularDataset\n",
    "from podium.vectorizers import GloVe\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from copy import deepcopy\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from time import time\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "\n",
    "import contextualSpellCheck\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import emoji\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "EnG-1KsiOp7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnG-1KsiOp7d",
    "outputId": "9668126a-955e-45f9-8a47-810b3809b270"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "NgNYHv8MPOgK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgNYHv8MPOgK",
    "outputId": "8b773729-8c76-4b84-82d4-238cf443f712"
   },
   "outputs": [],
   "source": [
    "# %cd drive/MyDrive/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfb886",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "548a72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "08931560",
   "metadata": {
    "id": "08931560"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train/SemEval2018-T3-train-taskA_emoji.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')\n",
    "df_test = pd.read_csv(\"goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')\n",
    "df_replace = pd.read_csv(\"test_TaskA/SemEval2018-T3_input_test_taskA.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8cd6bf9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)\n",
    "df_test.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)\n",
    "df_replace.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0a023",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "64b38f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=True)\n",
    "df_tmp = df['Tweet text'].apply(tokenizer.tokenize)\n",
    "df_test_tmp = df['Tweet text'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f5631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Tweet text'] = df['Tweet text'].apply(emoji.demojize)\n",
    "# df_test['Tweet text'] = df_test['Tweet text'].apply(emoji.demojize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f935fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_counter(s):\n",
    "    return len(emoji.emoji_lis(s))\n",
    "\n",
    "df['emoji_count'] = df['Tweet text'].apply(emoji_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "0656d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_emojis(s):\n",
    "#     print(f'S: {s}')\n",
    "#     print()\n",
    "#     d = emoji.emoji_lis(emoji.emojize(s))\n",
    "#     s_new = ''\n",
    "#     if d != []:\n",
    "#         for i, el in enumerate(d):\n",
    "#             print(i, el)\n",
    "#             if i == 0:\n",
    "#                 s_new += s[:el['location']]\n",
    "#             elif i < len(d):\n",
    "#                 up_to = sum([k['location'] + len(emoji.demojize(k['emoji'])) for k in d[:i]])\n",
    "#                 len_text = el['location'] - (d[i-1]['location'] + 1)\n",
    "#                 s_new += s[up_to : up_to + len_text]\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "94a03a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "#seg_eng = Segmenter(corpus=\"english\")\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "seg_tw = Segmenter(corpus=\"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "702478ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_hashtags(s):\n",
    "    '''Removes the hashtag sign and segments the hashtag text.'''\n",
    "    hashtags = []\n",
    "    \n",
    "    l = []\n",
    "    for i, s_i in enumerate(s):\n",
    "        if s_i.startswith('#'):\n",
    "            tmp = tokenizer.tokenize(seg_tw.segment(s_i.replace('#', '')))\n",
    "            l.extend(tmp)\n",
    "        else:\n",
    "            l.append(s_i)\n",
    "    return l\n",
    "            \n",
    "df_tmp = df_tmp.apply(separate_hashtags)\n",
    "df_test_tmp = df_test_tmp.apply(separate_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e5badc5e",
   "metadata": {
    "id": "e5badc5e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_words(s):\n",
    "    '''\n",
    "    Removes tags, emojis, links, smiley faces, | signs, stopwords and changes the case to lower.\n",
    "    '''\n",
    "    ret_list = []\n",
    "\n",
    "    smiley_regex = r'([\\:\\;\\=][()PDO\\/\\]\\[p|]+)+'\n",
    "    \n",
    "    is_tag = lambda w: w.startswith('@')\n",
    "    is_vertical_line = lambda w: w.startswith('|')\n",
    "    is_emoji = lambda w: emoji.is_emoji(w)\n",
    "    # is_emoji = lambda w: w != ':' and w.startswith(':') and w.endswith(':')\n",
    "    # remove_emoji = lambda w: w[:w.index(':')] + w[w.rindex(':')+1:] if ':' in w and w.index(':') != w.rindex(':') else w\n",
    "    is_link = lambda w: w.startswith(\"http\") or w.startswith(\"https\")\n",
    "    is_hashtag = lambda w: w.startswith(\"#\")\n",
    "    is_smiley = lambda w: re.match(smiley_regex, w)\n",
    "\n",
    "    w2 = []\n",
    "    for i, w in enumerate(s):\n",
    "        if is_tag(w) or is_emoji(w) or is_link(w) or is_vertical_line(w):\n",
    "            continue\n",
    "\n",
    "        elif is_hashtag(w):\n",
    "            w_tmp = w.replace('#', '')\n",
    "            if w_tmp != '':\n",
    "                lower_append(w_tmp, w2)\n",
    "\n",
    "        elif is_smiley(w):\n",
    "            w_tmp = re.sub(smiley_regex, '', w)\n",
    "            if w_tmp != '':\n",
    "                lower_append(w_tmp, w2)\n",
    "\n",
    "        else:\n",
    "            w_tmp = w.replace('#', '')\n",
    "            w_tmp = w_tmp.replace('|', '')\n",
    "            w_tmp = w_tmp.replace('_', '')\n",
    "            w_tmp = w_tmp.replace('...', '')\n",
    "            if w_tmp != '':\n",
    "                lower_append(w_tmp, w2)\n",
    "\n",
    "    return ' '.join([i for i in w2 if len(i) > 2])\n",
    "\n",
    "def lower_append(w, l):\n",
    "    l.append(w.lower())\n",
    "\n",
    "df['clean_text'] = df_tmp.apply(preprocess_words)\n",
    "df_test['clean_text'] = df_test_tmp.apply(preprocess_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4606db1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Tweet index  Label                                         Tweet text  \\\n0             1      1  Sweet United Nations video. Just in time for C...   \n1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3             4      0              3 episodes left I'm dying over here\\r   \n4             5      1  I can't breathe! was chosen as the most notabl...   \n5             6      0  You're never too old for Footie Pajamas. http:...   \n6             7      1  Nothing makes me happier then getting on the h...   \n7             8      0  4:30 an opening my first beer now gonna be a l...   \n8             9      0  @Adam_Klug do you think you would support a gu...   \n9            10      0  @samcguigan544 You are not allowed to open tha...   \n10           11      1  Oh, thank GOD - our entire office email system...   \n11           12      0  But instead, I'm scrolling through Facebook, I...   \n12           13      0  @TargetZonePT 😡 no he bloody isn't I was upsta...   \n13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n14           15      1  Just great when you're mobile bill arrives by ...   \n\n    emoji_count                                         clean_text  \n0             0  sweet united nations video just time for chris...  \n1             0  are rumored have talked erv's agent and the an...  \n2             0    hey there nice see you minnesota winter weather  \n3             0                  episodes left i'm dying over here  \n4             0  can't breathe was chosen the most notable quot...  \n5             0            you're never too old for footie pajamas  \n6             0  nothing makes happier then getting the highway...  \n7             0   4:30 opening first beer now gonna long night day  \n8             0  you think you would support guy who knocked ou...  \n9             0  you are not allowed open that until christmas day  \n10            0  thank god our entire office email system down ...  \n11            0  but instead i'm scrolling through facebook ins...  \n12            1          bloody isn't was upstairs getting changed  \n13            0  cold warmth both suffuse one's cheeks with pin...  \n14            0    just great when you're mobile bill arrives text  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>0</td>\n      <td>sweet united nations video just time for chris...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>0</td>\n      <td>are rumored have talked erv's agent and the an...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>0</td>\n      <td>hey there nice see you minnesota winter weather</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>0</td>\n      <td>episodes left i'm dying over here</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>0</td>\n      <td>can't breathe was chosen the most notable quot...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>You're never too old for Footie Pajamas. http:...</td>\n      <td>0</td>\n      <td>you're never too old for footie pajamas</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>Nothing makes me happier then getting on the h...</td>\n      <td>0</td>\n      <td>nothing makes happier then getting the highway...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n      <td>0</td>\n      <td>4:30 opening first beer now gonna long night day</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>@Adam_Klug do you think you would support a gu...</td>\n      <td>0</td>\n      <td>you think you would support guy who knocked ou...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>@samcguigan544 You are not allowed to open tha...</td>\n      <td>0</td>\n      <td>you are not allowed open that until christmas day</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>Oh, thank GOD - our entire office email system...</td>\n      <td>0</td>\n      <td>thank god our entire office email system down ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>But instead, I'm scrolling through Facebook, I...</td>\n      <td>0</td>\n      <td>but instead i'm scrolling through facebook ins...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>@TargetZonePT 😡 no he bloody isn't I was upsta...</td>\n      <td>1</td>\n      <td>bloody isn't was upstairs getting changed</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n      <td>0</td>\n      <td>cold warmth both suffuse one's cheeks with pin...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>Just great when you're mobile bill arrives by ...</td>\n      <td>0</td>\n      <td>just great when you're mobile bill arrives text</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf8c4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def simple_preprocessing(s):\n",
    "#     '''Lowercases, tokenizes, de-accents, removes words shorter than 3 and longer than 14 characters'''\n",
    "#     return [' '.join(simple_preprocess(s_i)) for s_i in s]\n",
    "\n",
    "# df['clean_text'] = df[['clean_text']].apply(simple_preprocessing)\n",
    "# df_test['clean_text'] = df_test[['clean_text']].apply(simple_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "79dead99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Tweet index  Label                                         Tweet text  \\\n0             1      1  Sweet United Nations video. Just in time for C...   \n1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3             4      0              3 episodes left I'm dying over here\\r   \n4             5      1  I can't breathe! was chosen as the most notabl...   \n5             6      0  You're never too old for Footie Pajamas. http:...   \n6             7      1  Nothing makes me happier then getting on the h...   \n7             8      0  4:30 an opening my first beer now gonna be a l...   \n8             9      0  @Adam_Klug do you think you would support a gu...   \n9            10      0  @samcguigan544 You are not allowed to open tha...   \n10           11      1  Oh, thank GOD - our entire office email system...   \n11           12      0  But instead, I'm scrolling through Facebook, I...   \n12           13      0  @TargetZonePT 😡 no he bloody isn't I was upsta...   \n13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n14           15      1  Just great when you're mobile bill arrives by ...   \n\n    emoji_count                                         clean_text  \n0             0  sweet united nations video just time for chris...  \n1             0  are rumored have talked erv's agent and the an...  \n2             0    hey there nice see you minnesota winter weather  \n3             0                  episodes left i'm dying over here  \n4             0  can't breathe was chosen the most notable quot...  \n5             0            you're never too old for footie pajamas  \n6             0  nothing makes happier then getting the highway...  \n7             0   4:30 opening first beer now gonna long night day  \n8             0  you think you would support guy who knocked ou...  \n9             0  you are not allowed open that until christmas day  \n10            0  thank god our entire office email system down ...  \n11            0  but instead i'm scrolling through facebook ins...  \n12            1          bloody isn't was upstairs getting changed  \n13            0  cold warmth both suffuse one's cheeks with pin...  \n14            0    just great when you're mobile bill arrives text  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>0</td>\n      <td>sweet united nations video just time for chris...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>0</td>\n      <td>are rumored have talked erv's agent and the an...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>0</td>\n      <td>hey there nice see you minnesota winter weather</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>0</td>\n      <td>episodes left i'm dying over here</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>0</td>\n      <td>can't breathe was chosen the most notable quot...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>You're never too old for Footie Pajamas. http:...</td>\n      <td>0</td>\n      <td>you're never too old for footie pajamas</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>Nothing makes me happier then getting on the h...</td>\n      <td>0</td>\n      <td>nothing makes happier then getting the highway...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n      <td>0</td>\n      <td>4:30 opening first beer now gonna long night day</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>@Adam_Klug do you think you would support a gu...</td>\n      <td>0</td>\n      <td>you think you would support guy who knocked ou...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>@samcguigan544 You are not allowed to open tha...</td>\n      <td>0</td>\n      <td>you are not allowed open that until christmas day</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>Oh, thank GOD - our entire office email system...</td>\n      <td>0</td>\n      <td>thank god our entire office email system down ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>But instead, I'm scrolling through Facebook, I...</td>\n      <td>0</td>\n      <td>but instead i'm scrolling through facebook ins...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>@TargetZonePT 😡 no he bloody isn't I was upsta...</td>\n      <td>1</td>\n      <td>bloody isn't was upstairs getting changed</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n      <td>0</td>\n      <td>cold warmth both suffuse one's cheeks with pin...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>Just great when you're mobile bill arrives by ...</td>\n      <td>0</td>\n      <td>just great when you're mobile bill arrives text</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e48acc39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Tweet index  Label                                         Tweet text  \\\n0             1      1  Sweet United Nations video. Just in time for C...   \n1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3             4      0              3 episodes left I'm dying over here\\r   \n4             5      1  I can't breathe! was chosen as the most notabl...   \n5             6      0  You're never too old for Footie Pajamas. http:...   \n6             7      1  Nothing makes me happier then getting on the h...   \n7             8      0  4:30 an opening my first beer now gonna be a l...   \n8             9      0  @Adam_Klug do you think you would support a gu...   \n9            10      0  @samcguigan544 You are not allowed to open tha...   \n10           11      1  Oh, thank GOD - our entire office email system...   \n11           12      0  But instead, I'm scrolling through Facebook, I...   \n12           13      0  @TargetZonePT 😡 no he bloody isn't I was upsta...   \n13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n14           15      1  Just great when you're mobile bill arrives by ...   \n\n    emoji_count                                         clean_text  \n0             0  sweet united nations video just time for chris...  \n1             0  are rumored have talked erv's agent and the an...  \n2             0    hey there nice see you minnesota winter weather  \n3             0                  episodes left i'm dying over here  \n4             0  can't breathe was chosen the most notable quot...  \n5             0            you're never too old for footie pajamas  \n6             0  nothing makes happier then getting the highway...  \n7             0   4:30 opening first beer now gonna long night day  \n8             0  you think you would support guy who knocked ou...  \n9             0  you are not allowed open that until christmas day  \n10            0  thank god our entire office email system down ...  \n11            0  but instead i'm scrolling through facebook ins...  \n12            1          bloody isn't was upstairs getting changed  \n13            0  cold warmth both suffuse one's cheeks with pin...  \n14            0    just great when you're mobile bill arrives text  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>0</td>\n      <td>sweet united nations video just time for chris...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>0</td>\n      <td>are rumored have talked erv's agent and the an...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>0</td>\n      <td>hey there nice see you minnesota winter weather</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>0</td>\n      <td>episodes left i'm dying over here</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>0</td>\n      <td>can't breathe was chosen the most notable quot...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>You're never too old for Footie Pajamas. http:...</td>\n      <td>0</td>\n      <td>you're never too old for footie pajamas</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>Nothing makes me happier then getting on the h...</td>\n      <td>0</td>\n      <td>nothing makes happier then getting the highway...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n      <td>0</td>\n      <td>4:30 opening first beer now gonna long night day</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>@Adam_Klug do you think you would support a gu...</td>\n      <td>0</td>\n      <td>you think you would support guy who knocked ou...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>@samcguigan544 You are not allowed to open tha...</td>\n      <td>0</td>\n      <td>you are not allowed open that until christmas day</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>Oh, thank GOD - our entire office email system...</td>\n      <td>0</td>\n      <td>thank god our entire office email system down ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>But instead, I'm scrolling through Facebook, I...</td>\n      <td>0</td>\n      <td>but instead i'm scrolling through facebook ins...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>@TargetZonePT 😡 no he bloody isn't I was upsta...</td>\n      <td>1</td>\n      <td>bloody isn't was upstairs getting changed</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n      <td>0</td>\n      <td>cold warmth both suffuse one's cheeks with pin...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>Just great when you're mobile bill arrives by ...</td>\n      <td>0</td>\n      <td>just great when you're mobile bill arrives text</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_tuple_characters(s):\n",
    "    return [re.sub(r'(.)\\1{2,}', r'\\1', w) for w in s]\n",
    "\n",
    "df['clean_text'] = df[['clean_text']].apply(remove_tuple_characters)\n",
    "df_test['clean_text'] = df_test[['clean_text']].apply(remove_tuple_characters)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bb677a12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lemmatize(s):\n",
    "    '''Lemmatizes the words in the sentences and returns them if theyre not stopwords or punctuation'''\n",
    "    return [[w.lemma_.lower() for w in nlp(s_i) if w.lemma_.lower() not in nlp.Defaults.stop_words and not w.is_punct] for s_i in s]\n",
    "\n",
    "df['lemmas'] = df[['clean_text']].apply(lemmatize)\n",
    "df_test['lemmas'] = df_test[['clean_text']].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2d460e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    Tweet index  Label                                         Tweet text  \\\n0             1      1  Sweet United Nations video. Just in time for C...   \n1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3             4      0              3 episodes left I'm dying over here\\r   \n4             5      1  I can't breathe! was chosen as the most notabl...   \n5             6      0  You're never too old for Footie Pajamas. http:...   \n6             7      1  Nothing makes me happier then getting on the h...   \n7             8      0  4:30 an opening my first beer now gonna be a l...   \n8             9      0  @Adam_Klug do you think you would support a gu...   \n9            10      0  @samcguigan544 You are not allowed to open tha...   \n10           11      1  Oh, thank GOD - our entire office email system...   \n11           12      0  But instead, I'm scrolling through Facebook, I...   \n12           13      0  @TargetZonePT 😡 no he bloody isn't I was upsta...   \n13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n14           15      1  Just great when you're mobile bill arrives by ...   \n\n    emoji_count                                         clean_text  \\\n0             0  sweet united nations video just time for chris...   \n1             0  are rumored have talked erv's agent and the an...   \n2             0    hey there nice see you minnesota winter weather   \n3             0                  episodes left i'm dying over here   \n4             0  can't breathe was chosen the most notable quot...   \n5             0            you're never too old for footie pajamas   \n6             0  nothing makes happier then getting the highway...   \n7             0   4:30 opening first beer now gonna long night day   \n8             0  you think you would support guy who knocked ou...   \n9             0  you are not allowed open that until christmas day   \n10            0  thank god our entire office email system down ...   \n11            0  but instead i'm scrolling through facebook ins...   \n12            1          bloody isn't was upstairs getting changed   \n13            0  cold warmth both suffuse one's cheeks with pin...   \n14            0    just great when you're mobile bill arrives text   \n\n                                               lemmas  \n0   [sweet, united, nations, video, time, christma...  \n1   [rumor, talk, erv, agent, angel, ask, escobar,...  \n2             [hey, nice, minnesota, winter, weather]  \n3                               [episode, leave, die]  \n4   [breathe, choose, notable, quote, year, annual...  \n5                               [old, footie, pajama]  \n6   [happy, highway, break, light, light, like, ch...  \n7         [4:30, open, beer, going, long, night, day]  \n8   [think, support, guy, knock, daughter, rice, d...  \n9                       [allow, open, christmas, day]  \n10  [thank, god, entire, office, email, system, da...  \n11  [instead, scroll, facebook, instagram, twitter...  \n12                [bloody, upstairs, getting, change]  \n13  [cold, warmth, suffuse, cheek, pink, colour, t...  \n14                [great, mobile, bill, arrive, text]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>0</td>\n      <td>sweet united nations video just time for chris...</td>\n      <td>[sweet, united, nations, video, time, christma...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>0</td>\n      <td>are rumored have talked erv's agent and the an...</td>\n      <td>[rumor, talk, erv, agent, angel, ask, escobar,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>0</td>\n      <td>hey there nice see you minnesota winter weather</td>\n      <td>[hey, nice, minnesota, winter, weather]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>0</td>\n      <td>episodes left i'm dying over here</td>\n      <td>[episode, leave, die]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>0</td>\n      <td>can't breathe was chosen the most notable quot...</td>\n      <td>[breathe, choose, notable, quote, year, annual...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>You're never too old for Footie Pajamas. http:...</td>\n      <td>0</td>\n      <td>you're never too old for footie pajamas</td>\n      <td>[old, footie, pajama]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>Nothing makes me happier then getting on the h...</td>\n      <td>0</td>\n      <td>nothing makes happier then getting the highway...</td>\n      <td>[happy, highway, break, light, light, like, ch...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n      <td>0</td>\n      <td>4:30 opening first beer now gonna long night day</td>\n      <td>[4:30, open, beer, going, long, night, day]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>@Adam_Klug do you think you would support a gu...</td>\n      <td>0</td>\n      <td>you think you would support guy who knocked ou...</td>\n      <td>[think, support, guy, knock, daughter, rice, d...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>@samcguigan544 You are not allowed to open tha...</td>\n      <td>0</td>\n      <td>you are not allowed open that until christmas day</td>\n      <td>[allow, open, christmas, day]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>Oh, thank GOD - our entire office email system...</td>\n      <td>0</td>\n      <td>thank god our entire office email system down ...</td>\n      <td>[thank, god, entire, office, email, system, da...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>But instead, I'm scrolling through Facebook, I...</td>\n      <td>0</td>\n      <td>but instead i'm scrolling through facebook ins...</td>\n      <td>[instead, scroll, facebook, instagram, twitter...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>@TargetZonePT 😡 no he bloody isn't I was upsta...</td>\n      <td>1</td>\n      <td>bloody isn't was upstairs getting changed</td>\n      <td>[bloody, upstairs, getting, change]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n      <td>0</td>\n      <td>cold warmth both suffuse one's cheeks with pin...</td>\n      <td>[cold, warmth, suffuse, cheek, pink, colour, t...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>Just great when you're mobile bill arrives by ...</td>\n      <td>0</td>\n      <td>just great when you're mobile bill arrives text</td>\n      <td>[great, mobile, bill, arrive, text]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1419464a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### baseline features ###\n",
    "tokenizer2 = TweetTokenizer()\n",
    "def word_counter(s):\n",
    "    return len([x for x in tokenizer.tokenize(s) if not x.startswith((\"@\", \"#\", \"http\"))])\n",
    "\n",
    "def char_counter(s):\n",
    "    return len(s.replace(' ', ''))\n",
    "\n",
    "def all_uppercase_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.isupper() and not x.startswith((\"@\", \"#\", \"http\"))])\n",
    "\n",
    "def all_lowercase_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.islower() and not x.startswith((\"@\", \"#\", \"http\"))])\n",
    "\n",
    "def capitalised_counter(s):\n",
    "    return sum([i.istitle() for i in tokenizer2.tokenize(s)])\n",
    "\n",
    "def digit_counter(s):\n",
    "    return sum([i.isdigit() for i in s])\n",
    "\n",
    "\n",
    "\n",
    "### other features ###\n",
    "def tag_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.startswith(\"@\")])\n",
    "\n",
    "def hashtag_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.startswith(\"#\")])\n",
    "\n",
    "def link_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if x.startswith(('http:', 'https:'))])\n",
    "\n",
    "def smiley_counter(s):\n",
    "    return len([x for x in tokenizer2.tokenize(s) if re.match(r'([\\:\\;\\=][()PDO\\/\\]\\[p|]+)+', x)])\n",
    "\n",
    "def exclamation_mark_counter(s):\n",
    "    return s.count('!')\n",
    "\n",
    "def question_mark_counter(s):\n",
    "    return s.count('?')\n",
    "\n",
    "def ellipsis_counter(s):\n",
    "    return s.count('...')\n",
    "    \n",
    "\n",
    "\n",
    "### NER ###\n",
    "def ORG_tag_counter(s):\n",
    "    doc = nlp(s)\n",
    "    return len([d.text for d in doc.ents if d.label_ == 'ORG'])\n",
    "\n",
    "def NORP_tag_counter(s):\n",
    "    doc = nlp(s)\n",
    "    return len([d.text for d in doc.ents if d.label_ == 'NORP'])\n",
    "\n",
    "def GPE_tag_counter(s):\n",
    "    doc = nlp(s)\n",
    "    return len([d.text for d in doc.ents if d.label_ == 'GPE'])\n",
    "\n",
    "def PERSON_tag_counter(s):\n",
    "    doc = nlp(s)\n",
    "    return len([d.text for d in doc.ents if d.label_ == 'PERSON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1d8d398f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(some_df):\n",
    "    some_df['word_count'] = df['clean_text'].apply(word_counter)\n",
    "    some_df['char_count'] = df['clean_text'].apply(char_counter)\n",
    "    some_df['all_uppercase_count'] = df['Tweet text'].apply(all_uppercase_counter)\n",
    "    some_df['all_lowercase_count'] = df['Tweet text'].apply(all_lowercase_counter)\n",
    "    some_df['capitalised_count'] = df['Tweet text'].apply(capitalised_counter)\n",
    "    some_df['digit_count'] = df['Tweet text'].apply(digit_counter)\n",
    "    \n",
    "    some_df['tag_count'] = df['Tweet text'].apply(tag_counter)\n",
    "    some_df['hashtag_count'] = df['Tweet text'].apply(hashtag_counter)\n",
    "    some_df['link_count'] = df['Tweet text'].apply(link_counter)\n",
    "    some_df['smiley_count'] = df['Tweet text'].apply(smiley_counter)\n",
    "    \n",
    "    some_df['exclamation_mark_count'] = df['Tweet text'].apply(exclamation_mark_counter)\n",
    "    some_df['question_mark_count'] = df['Tweet text'].apply(question_mark_counter)\n",
    "    some_df['ellipsis_count'] = df['Tweet text'].apply(ellipsis_counter)\n",
    "    \n",
    "    some_df['ORG_tag_count'] = df['Tweet text'].apply(ORG_tag_counter)\n",
    "    some_df['NORP_tag_count'] = df['Tweet text'].apply(NORP_tag_counter)\n",
    "    some_df['GPE_tag_count'] = df['Tweet text'].apply(GPE_tag_counter)\n",
    "    some_df['PERSON_tag_count'] = df['Tweet text'].apply(PERSON_tag_counter)\n",
    "    \n",
    "add_features(df)\n",
    "add_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0b415a2b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Tweet index  Label                                         Tweet text  \\\n0            1      1  Sweet United Nations video. Just in time for C...   \n1            2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2            3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3            4      0              3 episodes left I'm dying over here\\r   \n4            5      1  I can't breathe! was chosen as the most notabl...   \n\n   emoji_count                                         clean_text  \\\n0            0  sweet united nations video just time for chris...   \n1            0  are rumored have talked erv's agent and the an...   \n2            0    hey there nice see you minnesota winter weather   \n3            0                  episodes left i'm dying over here   \n4            0  can't breathe was chosen the most notable quot...   \n\n                                              lemmas  word_count  char_count  \\\n0  [sweet, united, nations, video, time, christma...          10          58   \n1  [rumor, talk, erv, agent, angel, ask, escobar,...          15          78   \n2            [hey, nice, minnesota, winter, weather]           8          40   \n3                              [episode, leave, die]           6          28   \n4  [breathe, choose, notable, quote, year, annual...          16          88   \n\n   all_uppercase_count  all_lowercase_count  ...  hashtag_count  link_count  \\\n0                    0                    4  ...              2           1   \n1                    0                   14  ...              0           0   \n2                    1                    4  ...              0           0   \n3                    0                    5  ...              0           0   \n4                    1                   20  ...              0           0   \n\n   smiley_count  exclamation_mark_count  question_mark_count  ellipsis_count  \\\n0             0                       0                    0               0   \n1             1                       0                    0               2   \n2             0                       1                    0               0   \n3             0                       0                    0               0   \n4             0                       1                    0               0   \n\n   ORG_tag_count  NORP_tag_count  GPE_tag_count  PERSON_tag_count  \n0              1               0              0                 0  \n1              1               0              0                 2  \n2              0               0              1                 0  \n3              0               0              0                 0  \n4              1               0              0                 0  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>all_uppercase_count</th>\n      <th>all_lowercase_count</th>\n      <th>...</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>smiley_count</th>\n      <th>exclamation_mark_count</th>\n      <th>question_mark_count</th>\n      <th>ellipsis_count</th>\n      <th>ORG_tag_count</th>\n      <th>NORP_tag_count</th>\n      <th>GPE_tag_count</th>\n      <th>PERSON_tag_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>0</td>\n      <td>sweet united nations video just time for chris...</td>\n      <td>[sweet, united, nations, video, time, christma...</td>\n      <td>10</td>\n      <td>58</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>0</td>\n      <td>are rumored have talked erv's agent and the an...</td>\n      <td>[rumor, talk, erv, agent, angel, ask, escobar,...</td>\n      <td>15</td>\n      <td>78</td>\n      <td>0</td>\n      <td>14</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>0</td>\n      <td>hey there nice see you minnesota winter weather</td>\n      <td>[hey, nice, minnesota, winter, weather]</td>\n      <td>8</td>\n      <td>40</td>\n      <td>1</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>0</td>\n      <td>episodes left i'm dying over here</td>\n      <td>[episode, leave, die]</td>\n      <td>6</td>\n      <td>28</td>\n      <td>0</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>0</td>\n      <td>can't breathe was chosen the most notable quot...</td>\n      <td>[breathe, choose, notable, quote, year, annual...</td>\n      <td>16</td>\n      <td>88</td>\n      <td>1</td>\n      <td>20</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "af74098d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7)\n",
    "df_validation = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1a5cd369",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n373           374      0  @MrMindMiracle One of those obvious when you s...   \n2217         2221      1  It's a Christmas miracle!|| http://t.co/3X7cxk...   \n232           233      0  @badassbraeden Yeah no I'm still laughing. It'...   \n2703         2707      0  http://t.co/8ICPDcAoNQ #CHRISTMAS #GIFT #BLESS...   \n924           925      1  I picked a great week to start a new show on N...   \n\n      emoji_count                                         clean_text  \\\n373             0  one those obvious when you see but clever idea...   \n2217            0                             it's christmas miracle   \n232             0  yeah i'm still laughing it's not even that fun...   \n2703            0  christmas gift blessing santa follow the inter...   \n924             0  picked great week start new show netflix hell ...   \n\n                                                 lemmas  word_count  \\\n373              [obvious, clever, idea, glad, approve]          12   \n2217                               [christmas, miracle]           3   \n232                         [yeah, laugh, funny, laugh]          13   \n2703  [christmas, gift, blessing, santa, follow, int...          11   \n924   [pick, great, week, start, new, netflix, hell,...           9   \n\n      char_count  all_uppercase_count  all_lowercase_count  ...  \\\n373           54                    0                   14  ...   \n2217          20                    0                    2  ...   \n232           59                    0                   10  ...   \n2703          62                    0                    0  ...   \n924           44                    1                   10  ...   \n\n      hashtag_count  link_count  smiley_count  exclamation_mark_count  \\\n373               0           0             0                       3   \n2217              0           1             0                       1   \n232               0           0             0                       0   \n2703              8           2             0                       0   \n924               1           0             0                       0   \n\n      question_mark_count  ellipsis_count  ORG_tag_count  NORP_tag_count  \\\n373                     0               0              0               0   \n2217                    0               0              0               0   \n232                     0               0              0               0   \n2703                    0               1              0               0   \n924                     0               0              1               0   \n\n      GPE_tag_count  PERSON_tag_count  \n373               0                 0  \n2217              0                 0  \n232               1                 0  \n2703              0                 0  \n924               0                 0  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>all_uppercase_count</th>\n      <th>all_lowercase_count</th>\n      <th>...</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>smiley_count</th>\n      <th>exclamation_mark_count</th>\n      <th>question_mark_count</th>\n      <th>ellipsis_count</th>\n      <th>ORG_tag_count</th>\n      <th>NORP_tag_count</th>\n      <th>GPE_tag_count</th>\n      <th>PERSON_tag_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>373</th>\n      <td>374</td>\n      <td>0</td>\n      <td>@MrMindMiracle One of those obvious when you s...</td>\n      <td>0</td>\n      <td>one those obvious when you see but clever idea...</td>\n      <td>[obvious, clever, idea, glad, approve]</td>\n      <td>12</td>\n      <td>54</td>\n      <td>0</td>\n      <td>14</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2217</th>\n      <td>2221</td>\n      <td>1</td>\n      <td>It's a Christmas miracle!|| http://t.co/3X7cxk...</td>\n      <td>0</td>\n      <td>it's christmas miracle</td>\n      <td>[christmas, miracle]</td>\n      <td>3</td>\n      <td>20</td>\n      <td>0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>232</th>\n      <td>233</td>\n      <td>0</td>\n      <td>@badassbraeden Yeah no I'm still laughing. It'...</td>\n      <td>0</td>\n      <td>yeah i'm still laughing it's not even that fun...</td>\n      <td>[yeah, laugh, funny, laugh]</td>\n      <td>13</td>\n      <td>59</td>\n      <td>0</td>\n      <td>10</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2703</th>\n      <td>2707</td>\n      <td>0</td>\n      <td>http://t.co/8ICPDcAoNQ #CHRISTMAS #GIFT #BLESS...</td>\n      <td>0</td>\n      <td>christmas gift blessing santa follow the inter...</td>\n      <td>[christmas, gift, blessing, santa, follow, int...</td>\n      <td>11</td>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>924</th>\n      <td>925</td>\n      <td>1</td>\n      <td>I picked a great week to start a new show on N...</td>\n      <td>0</td>\n      <td>picked great week start new show netflix hell ...</td>\n      <td>[pick, great, week, start, new, netflix, hell,...</td>\n      <td>9</td>\n      <td>44</td>\n      <td>1</td>\n      <td>10</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "138ca2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_df_with_all_features(df1, df2):\n",
    "    cols_to_add = list(set(df2.columns.tolist()) - set(df1.columns.tolist()))\n",
    "    if 'Tweet index' in cols_to_add:\n",
    "        cols_to_add.remove('Tweet index')\n",
    "    new_df = pd.concat((df1.copy(), df2[cols_to_add]), axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2d560761",
   "metadata": {
    "id": "2d560761"
   },
   "outputs": [],
   "source": [
    "def join_docs(s):\n",
    "    '''Joins the strings inside the inner list of a nested list'''\n",
    "    return ' '.join(s)\n",
    "\n",
    "df_train['topic_text'] = df_train['lemmas'].apply(join_docs)\n",
    "df_validation['topic_text'] = df_validation['lemmas'].apply(join_docs)\n",
    "df_test['topic_text'] = df_test['lemmas'].apply(join_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c6cb21d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2672, 24)"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6ca36c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n373           374      0  @MrMindMiracle One of those obvious when you s...   \n2217         2221      1  It's a Christmas miracle!|| http://t.co/3X7cxk...   \n232           233      0  @badassbraeden Yeah no I'm still laughing. It'...   \n2703         2707      0  http://t.co/8ICPDcAoNQ #CHRISTMAS #GIFT #BLESS...   \n924           925      1  I picked a great week to start a new show on N...   \n\n      emoji_count                                         clean_text  \\\n373             0  one those obvious when you see but clever idea...   \n2217            0                             it's christmas miracle   \n232             0  yeah i'm still laughing it's not even that fun...   \n2703            0  christmas gift blessing santa follow the inter...   \n924             0  picked great week start new show netflix hell ...   \n\n                                                 lemmas  word_count  \\\n373              [obvious, clever, idea, glad, approve]          12   \n2217                               [christmas, miracle]           3   \n232                         [yeah, laugh, funny, laugh]          13   \n2703  [christmas, gift, blessing, santa, follow, int...          11   \n924   [pick, great, week, start, new, netflix, hell,...           9   \n\n      char_count  all_uppercase_count  all_lowercase_count  ...  link_count  \\\n373           54                    0                   14  ...           0   \n2217          20                    0                    2  ...           1   \n232           59                    0                   10  ...           0   \n2703          62                    0                    0  ...           2   \n924           44                    1                   10  ...           0   \n\n      smiley_count  exclamation_mark_count  question_mark_count  \\\n373              0                       3                    0   \n2217             0                       1                    0   \n232              0                       0                    0   \n2703             0                       0                    0   \n924              0                       0                    0   \n\n      ellipsis_count  ORG_tag_count  NORP_tag_count  GPE_tag_count  \\\n373                0              0               0              0   \n2217               0              0               0              0   \n232                0              0               0              1   \n2703               1              0               0              0   \n924                0              1               0              0   \n\n      PERSON_tag_count                                         topic_text  \n373                  0                   obvious clever idea glad approve  \n2217                 0                                  christmas miracle  \n232                  0                             yeah laugh funny laugh  \n2703                 0  christmas gift blessing santa follow interview...  \n924                  0       pick great week start new netflix hell wheel  \n\n[5 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>all_uppercase_count</th>\n      <th>all_lowercase_count</th>\n      <th>...</th>\n      <th>link_count</th>\n      <th>smiley_count</th>\n      <th>exclamation_mark_count</th>\n      <th>question_mark_count</th>\n      <th>ellipsis_count</th>\n      <th>ORG_tag_count</th>\n      <th>NORP_tag_count</th>\n      <th>GPE_tag_count</th>\n      <th>PERSON_tag_count</th>\n      <th>topic_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>373</th>\n      <td>374</td>\n      <td>0</td>\n      <td>@MrMindMiracle One of those obvious when you s...</td>\n      <td>0</td>\n      <td>one those obvious when you see but clever idea...</td>\n      <td>[obvious, clever, idea, glad, approve]</td>\n      <td>12</td>\n      <td>54</td>\n      <td>0</td>\n      <td>14</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>obvious clever idea glad approve</td>\n    </tr>\n    <tr>\n      <th>2217</th>\n      <td>2221</td>\n      <td>1</td>\n      <td>It's a Christmas miracle!|| http://t.co/3X7cxk...</td>\n      <td>0</td>\n      <td>it's christmas miracle</td>\n      <td>[christmas, miracle]</td>\n      <td>3</td>\n      <td>20</td>\n      <td>0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>christmas miracle</td>\n    </tr>\n    <tr>\n      <th>232</th>\n      <td>233</td>\n      <td>0</td>\n      <td>@badassbraeden Yeah no I'm still laughing. It'...</td>\n      <td>0</td>\n      <td>yeah i'm still laughing it's not even that fun...</td>\n      <td>[yeah, laugh, funny, laugh]</td>\n      <td>13</td>\n      <td>59</td>\n      <td>0</td>\n      <td>10</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>yeah laugh funny laugh</td>\n    </tr>\n    <tr>\n      <th>2703</th>\n      <td>2707</td>\n      <td>0</td>\n      <td>http://t.co/8ICPDcAoNQ #CHRISTMAS #GIFT #BLESS...</td>\n      <td>0</td>\n      <td>christmas gift blessing santa follow the inter...</td>\n      <td>[christmas, gift, blessing, santa, follow, int...</td>\n      <td>11</td>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>christmas gift blessing santa follow interview...</td>\n    </tr>\n    <tr>\n      <th>924</th>\n      <td>925</td>\n      <td>1</td>\n      <td>I picked a great week to start a new show on N...</td>\n      <td>0</td>\n      <td>picked great week start new show netflix hell ...</td>\n      <td>[pick, great, week, start, new, netflix, hell,...</td>\n      <td>9</td>\n      <td>44</td>\n      <td>1</td>\n      <td>10</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>pick great week start new netflix hell wheel</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98404b14",
   "metadata": {
    "id": "98404b14"
   },
   "source": [
    "## Topic modeling baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127a752",
   "metadata": {
    "id": "7127a752"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    min_df=20,\n",
    "    max_df=0.5,\n",
    ")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    min_df=20,\n",
    "    max_df=0.5,\n",
    ")\n",
    "\n",
    "tweet_text_count_train = count_vectorizer.fit_transform(df_train['topic_text'])\n",
    "tweet_text_count_validation = count_vectorizer.transform(df_validation['topic_text'])\n",
    "tweet_text_count_test = count_vectorizer.transform(df_test['topic_text'])\n",
    "\n",
    "tweet_text_tfidf_train = tfidf_vectorizer.fit_transform(df_train['topic_text'])\n",
    "tweet_text_tfidf_validation = tfidf_vectorizer.transform(df_validation['topic_text'])\n",
    "tweet_text_tfidf_test = tfidf_vectorizer.transform(df_test['topic_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08150dae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "08150dae",
    "outputId": "03815e50-e2ab-48a5-a465-97b6c0e7571e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "range_ = list(range(2, 60))\n",
    "for i in range_:\n",
    "    model = KMeans(i)\n",
    "    model.fit(tweet_text_count_train)\n",
    "    inertia.append(model.inertia_)\n",
    "    \n",
    "plt.plot(range_, inertia)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d0b6e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(17)\n",
    "model.fit(tweet_text_count_train)\n",
    "\n",
    "kmeans_count_labels_train = model.predict(tweet_text_count_train)\n",
    "kmeans_count_labels_validation = model.predict(tweet_text_count_validation)\n",
    "kmeans_count_labels_test = model.predict(tweet_text_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f11fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "fe5f11fc",
    "outputId": "8000ac3b-1375-4905-8bca-179acf2ac778",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for i in range_:\n",
    "    model = KMeans(i)\n",
    "    model.fit(tweet_text_tfidf_train)\n",
    "    inertia.append(model.inertia_)\n",
    "plt.plot(range_, inertia)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72eb5b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(22)\n",
    "model.fit(tweet_text_tfidf_train)\n",
    "\n",
    "kmeans_tfidf_labels_train = model.predict(tweet_text_tfidf_train)\n",
    "kmeans_tfidf_labels_validation = model.predict(tweet_text_tfidf_validation)\n",
    "kmeans_tfidf_labels_test = model.predict(tweet_text_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0vdf4uM_F2cg",
   "metadata": {
    "id": "0vdf4uM_F2cg"
   },
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0fdbed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_validation.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "NRdFJDRVF2m2",
   "metadata": {
    "id": "NRdFJDRVF2m2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.638654708862305\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "topic_model = BERTopic(top_n_words=8, min_topic_size=20, calculate_probabilities=True)\n",
    "topics, probs = topic_model.fit_transform(df_train['topic_text'])\n",
    "end = time()\n",
    "print(end-start)\n",
    "pred_train = topic_model.transform(df_train['topic_text'])\n",
    "pred_validation = topic_model.transform(df_validation['topic_text'])\n",
    "pred_test = topic_model.transform(df_test['topic_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "W9IK-LziICsL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "W9IK-LziICsL",
    "outputId": "76f344ac-14f1-47a9-fc10-eef060d23b5a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    Topic  Count                            Name\n0      -1   1158           -1_day_work_love_like\n1       0    210          0_funny_think_lol_know\n2       1    203       1_black_police_obama_race\n3       2    154         2_team_game_playoff_win\n4       3    145           3_look_girl_sock_wear\n5       4    111     4_christmas_gift_xmas_merry\n6       5    109       5_sleep_morning_wake_hour\n7       6     94    6_study_class_school_teacher\n8       7     87  7_twitter_tweet_follow_retweet\n9       8     67      8_music_photo_listen_drink\n10      9     62        9_pay_service_phone_xbox\n11     10     53           10_turkey_eat_cat_bee\n12     11     50     11_winter_rain_weather_cold\n13     12     42        12_day_today_great_start\n14     13     40       13_love_hurt_feeling_pain\n15     14     35           14_year_new_2015_boot\n16     15     28   15_friday_monday_week_weekend\n17     16     24     16_bus_orion_launch_airport",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Count</th>\n      <th>Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>1158</td>\n      <td>-1_day_work_love_like</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>210</td>\n      <td>0_funny_think_lol_know</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>203</td>\n      <td>1_black_police_obama_race</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>154</td>\n      <td>2_team_game_playoff_win</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>145</td>\n      <td>3_look_girl_sock_wear</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>111</td>\n      <td>4_christmas_gift_xmas_merry</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>109</td>\n      <td>5_sleep_morning_wake_hour</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>94</td>\n      <td>6_study_class_school_teacher</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>87</td>\n      <td>7_twitter_tweet_follow_retweet</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>67</td>\n      <td>8_music_photo_listen_drink</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>9</td>\n      <td>62</td>\n      <td>9_pay_service_phone_xbox</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10</td>\n      <td>53</td>\n      <td>10_turkey_eat_cat_bee</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>11</td>\n      <td>50</td>\n      <td>11_winter_rain_weather_cold</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>12</td>\n      <td>42</td>\n      <td>12_day_today_great_start</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>13</td>\n      <td>40</td>\n      <td>13_love_hurt_feeling_pain</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>14</td>\n      <td>35</td>\n      <td>14_year_new_2015_boot</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>15</td>\n      <td>28</td>\n      <td>15_friday_monday_week_weekend</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>16</td>\n      <td>24</td>\n      <td>16_bus_orion_launch_airport</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n        16], dtype=int64),\n array([746, 233, 243, 146, 275,  99, 102, 111,  84,  72, 111,  77,  70,\n         77,  50,  38,  58,  80], dtype=int64))"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_threshold = 0.07\n",
    "new_topics = [np.argmax(prob) if max(prob) >= probability_threshold else -1 for prob in pred_train[1]]\n",
    "np.unique(new_topics, return_counts=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n        16], dtype=int64),\n array([746, 233, 243, 146, 275,  99, 102, 111,  84,  72, 111,  77,  70,\n         77,  50,  38,  58,  80], dtype=int64))"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_threshold = 0.07\n",
    "new_topics_train = [np.argmax(prob) if max(prob) >= probability_threshold else -1 for prob in pred_train[1]]\n",
    "new_topics_validation = [np.argmax(prob) if max(prob) >= probability_threshold else -1 for prob in pred_validation[1]]\n",
    "new_topics_test = [np.argmax(prob) if max(prob) >= probability_threshold else -1 for prob in pred_test[1]]\n",
    "np.unique(new_topics_train, return_counts=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca352aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_bertopic = df_train.copy()\n",
    "# df_validation_bertopic = df_validation.copy()\n",
    "# df_test_bertopic = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7e68e524",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train['topic'] = new_topics_train\n",
    "df_validation['topic'] = new_topics_validation\n",
    "df_test['topic'] = new_topics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_bertopic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad485a",
   "metadata": {
    "id": "6cad485a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cc4e8fed",
   "metadata": {
    "id": "cc4e8fed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_embedding_train = df_train[['topic_text', 'Label']]\n",
    "tweet_embedding_validation = df_validation[['topic_text', 'Label']]\n",
    "tweet_embedding_test = df_test[['topic_text', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "97c7341c",
   "metadata": {
    "id": "97c7341c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_embedding_train.reset_index(drop=True, inplace=True)\n",
    "tweet_embedding_validation.reset_index(drop=True, inplace=True)\n",
    "tweet_embedding_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c409d8cc",
   "metadata": {
    "id": "c409d8cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10_000\n",
    "vocab = Vocab(max_size=max_vocab_size, min_freq=20)\n",
    "\n",
    "TWEET = Field('text', numericalizer=vocab)\n",
    "LABEL = LabelField('Label')\n",
    "\n",
    "fields = [TWEET, LABEL]\n",
    "\n",
    "train = TabularDataset.from_pandas(df_train[['topic_text', 'Label']], fields)\n",
    "validation = TabularDataset.from_pandas(df_validation[['topic_text', 'Label']], fields)\n",
    "test = TabularDataset.from_pandas(df_test[['topic_text', 'Label']], fields)\n",
    "train.finalize_fields()\n",
    "\n",
    "glove = GloVe()\n",
    "embeddings = glove.load_vocab(vocab)\n",
    "\n",
    "train_batch = train.batch(add_padding=True)\n",
    "validation_batch = validation.batch(add_padding=True)\n",
    "test_batch = test.batch(add_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e707fbc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e707fbc3",
    "outputId": "68f44c29-513e-40cf-a62d-f02ee2cb168c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1],\n       [10,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1],\n       [45,  0, 67,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1],\n       [10,  0,  0,  0, 58,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1],\n       [ 0,  7, 35, 26, 18,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1]])"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch['text'].astype(int)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "df0d0eed",
   "metadata": {
    "id": "df0d0eed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_train = embeddings[train_batch['text'].astype(int)]\n",
    "tweet_validation = embeddings[validation_batch['text'].astype(int)]\n",
    "tweet_test = embeddings[test_batch['text'].astype(int)]\n",
    "\n",
    "# Mean\n",
    "tweet_train_mean = tweet_train.mean(axis=1)\n",
    "tweet_validation_mean = tweet_validation.mean(axis=1)\n",
    "tweet_test_mean = tweet_test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cf850683",
   "metadata": {
    "id": "cf850683",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_train_mean_df = pd.DataFrame(tweet_train_mean)\n",
    "df_train = pd.merge(df_train, embeddings_train_mean_df, left_index=True, right_index=True)\n",
    "\n",
    "embeddings_validation_mean_df = pd.DataFrame(tweet_validation_mean)\n",
    "df_validation = pd.merge(df_validation, embeddings_validation_mean_df, left_index=True, right_index=True)\n",
    "\n",
    "embeddings_test_mean_df = pd.DataFrame(tweet_test_mean)\n",
    "df_test = pd.merge(df_test, embeddings_test_mean_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "470341bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "470341bb",
    "outputId": "e580e876-4810-4056-a94c-62109a8a1e33",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Tweet index  Label                                         Tweet text  \\\n0         2484      1  Babysitting is what I like to do on a Saturday...   \n1         2494      0          @BigAlMLM Please give me a Follow Back.\\r   \n2         1297      1  Obama wants closer relations with a Marxist na...   \n3         3755      1  Reading the Dundee United chairmanship stateme...   \n4         3119      1  There's nothing like almost rear-ending someon...   \n\n   emoji_count                                         clean_text  \\\n0            1        babysitting what like saturday unamusedface   \n1            0                            please give follow back   \n2            0  obama wants closer relations with marxist nati...   \n3            0  reading the dundee united chairmanship stateme...   \n4            0  there's nothing like almost rear-ending someon...   \n\n                                              lemmas  word_count  char_count  \\\n0          [babysitte, like, saturday, unamusedface]           5          39   \n1                                           [follow]           4          20   \n2  [obama, want, close, relation, marxist, nation...          13          70   \n3  [read, dundee, united, chairmanship, statement...          18          96   \n4  [like, rear, end, slam, brake, reason, alert, ...          19          98   \n\n   all_uppercase_count  all_lowercase_count  ...       290       291  \\\n0                    1                    8  ... -0.374424 -3.264361   \n1                    0                    3  ... -0.436824 -3.424057   \n2                    3                    8  ... -0.263332 -2.963807   \n3                    0                   19  ... -0.251506 -2.910116   \n4                    0                   22  ... -0.290822 -3.009742   \n\n        292       293       294       295       296       297       298  \\\n0  0.275496  0.532334 -0.278861 -0.158356  1.689196 -1.279314 -0.930186   \n1  0.319748  0.563718 -0.227369 -0.133755  1.794007 -1.279203 -1.040222   \n2  0.208874  0.466685 -0.358867 -0.203663  1.484307 -1.259718 -0.724626   \n3  0.199453  0.470484 -0.359425 -0.199702  1.467706 -1.243554 -0.719899   \n4  0.214149  0.487589 -0.339617 -0.174653  1.521694 -1.268621 -0.764868   \n\n        299  \n0  2.086180  \n1  2.162534  \n2  1.928348  \n3  1.895914  \n4  1.950810  \n\n[5 rows x 325 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>all_uppercase_count</th>\n      <th>all_lowercase_count</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2484</td>\n      <td>1</td>\n      <td>Babysitting is what I like to do on a Saturday...</td>\n      <td>1</td>\n      <td>babysitting what like saturday unamusedface</td>\n      <td>[babysitte, like, saturday, unamusedface]</td>\n      <td>5</td>\n      <td>39</td>\n      <td>1</td>\n      <td>8</td>\n      <td>...</td>\n      <td>-0.374424</td>\n      <td>-3.264361</td>\n      <td>0.275496</td>\n      <td>0.532334</td>\n      <td>-0.278861</td>\n      <td>-0.158356</td>\n      <td>1.689196</td>\n      <td>-1.279314</td>\n      <td>-0.930186</td>\n      <td>2.086180</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2494</td>\n      <td>0</td>\n      <td>@BigAlMLM Please give me a Follow Back.\\r</td>\n      <td>0</td>\n      <td>please give follow back</td>\n      <td>[follow]</td>\n      <td>4</td>\n      <td>20</td>\n      <td>0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>-0.436824</td>\n      <td>-3.424057</td>\n      <td>0.319748</td>\n      <td>0.563718</td>\n      <td>-0.227369</td>\n      <td>-0.133755</td>\n      <td>1.794007</td>\n      <td>-1.279203</td>\n      <td>-1.040222</td>\n      <td>2.162534</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1297</td>\n      <td>1</td>\n      <td>Obama wants closer relations with a Marxist na...</td>\n      <td>0</td>\n      <td>obama wants closer relations with marxist nati...</td>\n      <td>[obama, want, close, relation, marxist, nation...</td>\n      <td>13</td>\n      <td>70</td>\n      <td>3</td>\n      <td>8</td>\n      <td>...</td>\n      <td>-0.263332</td>\n      <td>-2.963807</td>\n      <td>0.208874</td>\n      <td>0.466685</td>\n      <td>-0.358867</td>\n      <td>-0.203663</td>\n      <td>1.484307</td>\n      <td>-1.259718</td>\n      <td>-0.724626</td>\n      <td>1.928348</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3755</td>\n      <td>1</td>\n      <td>Reading the Dundee United chairmanship stateme...</td>\n      <td>0</td>\n      <td>reading the dundee united chairmanship stateme...</td>\n      <td>[read, dundee, united, chairmanship, statement...</td>\n      <td>18</td>\n      <td>96</td>\n      <td>0</td>\n      <td>19</td>\n      <td>...</td>\n      <td>-0.251506</td>\n      <td>-2.910116</td>\n      <td>0.199453</td>\n      <td>0.470484</td>\n      <td>-0.359425</td>\n      <td>-0.199702</td>\n      <td>1.467706</td>\n      <td>-1.243554</td>\n      <td>-0.719899</td>\n      <td>1.895914</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3119</td>\n      <td>1</td>\n      <td>There's nothing like almost rear-ending someon...</td>\n      <td>0</td>\n      <td>there's nothing like almost rear-ending someon...</td>\n      <td>[like, rear, end, slam, brake, reason, alert, ...</td>\n      <td>19</td>\n      <td>98</td>\n      <td>0</td>\n      <td>22</td>\n      <td>...</td>\n      <td>-0.290822</td>\n      <td>-3.009742</td>\n      <td>0.214149</td>\n      <td>0.487589</td>\n      <td>-0.339617</td>\n      <td>-0.174653</td>\n      <td>1.521694</td>\n      <td>-1.268621</td>\n      <td>-0.764868</td>\n      <td>1.950810</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 325 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "a8c44fb1",
   "metadata": {
    "id": "a8c44fb1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Features\n",
    "## Broj neg rijeci\n",
    "## Broj poz rijeci\n",
    "## Omjer\n",
    "## Udaljenost izmedu poz i neg rijeci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1208a12f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1208a12f",
    "outputId": "aa68c64c-f083-4235-df46-4027f5ef3168"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Marino\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon') # if error run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af3801bc",
   "metadata": {
    "id": "af3801bc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pos_neg_words(df, limit):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    neg_words = []\n",
    "    neg_word_count = []\n",
    "    pos_words = []\n",
    "    pos_word_count = []\n",
    "    for index, row in df.iterrows():\n",
    "        lemmas = []\n",
    "        if len(row['topic_text']) > 0:\n",
    "            doc = nlp(row['topic_text'])\n",
    "            for token in doc:\n",
    "                lemmas.append(token.lemma_)\n",
    "\n",
    "            current_pos = []\n",
    "            current_neut = []\n",
    "            current_neg = []\n",
    "            for word in lemmas:\n",
    "                if (sid.polarity_scores(word)['compound']) >= limit:\n",
    "                    current_pos.append(word)\n",
    "                elif (sid.polarity_scores(word)['compound']) <= -limit:\n",
    "                    current_neg.append(word)\n",
    "                else:\n",
    "                    current_neut.append(word)\n",
    "\n",
    "            neg_words.append(deepcopy(current_neg))\n",
    "            neg_word_count.append(deepcopy(len(current_neg)))\n",
    "            pos_words.append(deepcopy(current_pos))\n",
    "            pos_word_count.append(deepcopy(len(current_pos)))\n",
    "        else:\n",
    "            neg_words.append([])\n",
    "            neg_word_count.append(0)\n",
    "            pos_words.append([])\n",
    "            pos_word_count.append(0)\n",
    "    return neg_words, neg_word_count, pos_words, pos_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1447d136",
   "metadata": {
    "id": "1447d136",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg_words_train, neg_word_count_train, pos_words_train, pos_word_count_train = pos_neg_words(df_train, 0.2)\n",
    "neg_words_val, neg_word_count_val, pos_words_val, pos_word_count_val = pos_neg_words(df_validation, 0.2)\n",
    "neg_words_test, neg_word_count_test, pos_words_test, pos_word_count_test = pos_neg_words(df_test, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c4c71a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "5c4c71a3",
    "outputId": "1738c41e-41dc-4738-b858-0d45ce47976b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Tweet index  Label                                         Tweet text  \\\n0         2484      1  Babysitting is what I like to do on a Saturday...   \n1         2494      0          @BigAlMLM Please give me a Follow Back.\\r   \n2         1297      1  Obama wants closer relations with a Marxist na...   \n3         3755      1  Reading the Dundee United chairmanship stateme...   \n4         3119      1  There's nothing like almost rear-ending someon...   \n\n   emoji_count                                         clean_text  \\\n0            1        babysitting what like saturday unamusedface   \n1            0                            please give follow back   \n2            0  obama wants closer relations with marxist nati...   \n3            0  reading the dundee united chairmanship stateme...   \n4            0  there's nothing like almost rear-ending someon...   \n\n                                              lemmas  word_count  char_count  \\\n0          [babysitte, like, saturday, unamusedface]           5          39   \n1                                           [follow]           4          20   \n2  [obama, want, close, relation, marxist, nation...          13          70   \n3  [read, dundee, united, chairmanship, statement...          18          96   \n4  [like, rear, end, slam, brake, reason, alert, ...          19          98   \n\n   all_uppercase_count  all_lowercase_count  ...       294       295  \\\n0                    1                    8  ... -0.278861 -0.158356   \n1                    0                    3  ... -0.227369 -0.133755   \n2                    3                    8  ... -0.358867 -0.203663   \n3                    0                   19  ... -0.359425 -0.199702   \n4                    0                   22  ... -0.339617 -0.174653   \n\n        296       297       298       299  neg_word_count  \\\n0  1.689196 -1.279314 -0.930186  2.086180               0   \n1  1.794007 -1.279203 -1.040222  2.162534               0   \n2  1.484307 -1.259718 -0.724626  1.928348               2   \n3  1.467706 -1.243554 -0.719899  1.895914               0   \n4  1.521694 -1.268621 -0.764868  1.950810               1   \n\n               pos_word  pos_word_count        neg_word  \n0                [like]               1              []  \n1                    []               0              []  \n2                    []               0  [shock, shock]  \n3        [united, care]               2              []  \n4  [like, alert, ready]               3          [slam]  \n\n[5 rows x 329 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>all_uppercase_count</th>\n      <th>all_lowercase_count</th>\n      <th>...</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n      <th>neg_word_count</th>\n      <th>pos_word</th>\n      <th>pos_word_count</th>\n      <th>neg_word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2484</td>\n      <td>1</td>\n      <td>Babysitting is what I like to do on a Saturday...</td>\n      <td>1</td>\n      <td>babysitting what like saturday unamusedface</td>\n      <td>[babysitte, like, saturday, unamusedface]</td>\n      <td>5</td>\n      <td>39</td>\n      <td>1</td>\n      <td>8</td>\n      <td>...</td>\n      <td>-0.278861</td>\n      <td>-0.158356</td>\n      <td>1.689196</td>\n      <td>-1.279314</td>\n      <td>-0.930186</td>\n      <td>2.086180</td>\n      <td>0</td>\n      <td>[like]</td>\n      <td>1</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2494</td>\n      <td>0</td>\n      <td>@BigAlMLM Please give me a Follow Back.\\r</td>\n      <td>0</td>\n      <td>please give follow back</td>\n      <td>[follow]</td>\n      <td>4</td>\n      <td>20</td>\n      <td>0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>-0.227369</td>\n      <td>-0.133755</td>\n      <td>1.794007</td>\n      <td>-1.279203</td>\n      <td>-1.040222</td>\n      <td>2.162534</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1297</td>\n      <td>1</td>\n      <td>Obama wants closer relations with a Marxist na...</td>\n      <td>0</td>\n      <td>obama wants closer relations with marxist nati...</td>\n      <td>[obama, want, close, relation, marxist, nation...</td>\n      <td>13</td>\n      <td>70</td>\n      <td>3</td>\n      <td>8</td>\n      <td>...</td>\n      <td>-0.358867</td>\n      <td>-0.203663</td>\n      <td>1.484307</td>\n      <td>-1.259718</td>\n      <td>-0.724626</td>\n      <td>1.928348</td>\n      <td>2</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[shock, shock]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3755</td>\n      <td>1</td>\n      <td>Reading the Dundee United chairmanship stateme...</td>\n      <td>0</td>\n      <td>reading the dundee united chairmanship stateme...</td>\n      <td>[read, dundee, united, chairmanship, statement...</td>\n      <td>18</td>\n      <td>96</td>\n      <td>0</td>\n      <td>19</td>\n      <td>...</td>\n      <td>-0.359425</td>\n      <td>-0.199702</td>\n      <td>1.467706</td>\n      <td>-1.243554</td>\n      <td>-0.719899</td>\n      <td>1.895914</td>\n      <td>0</td>\n      <td>[united, care]</td>\n      <td>2</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3119</td>\n      <td>1</td>\n      <td>There's nothing like almost rear-ending someon...</td>\n      <td>0</td>\n      <td>there's nothing like almost rear-ending someon...</td>\n      <td>[like, rear, end, slam, brake, reason, alert, ...</td>\n      <td>19</td>\n      <td>98</td>\n      <td>0</td>\n      <td>22</td>\n      <td>...</td>\n      <td>-0.339617</td>\n      <td>-0.174653</td>\n      <td>1.521694</td>\n      <td>-1.268621</td>\n      <td>-0.764868</td>\n      <td>1.950810</td>\n      <td>1</td>\n      <td>[like, alert, ready]</td>\n      <td>3</td>\n      <td>[slam]</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 329 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['neg_word_count'] = neg_word_count_train\n",
    "df_train['pos_word'] = pos_words_train\n",
    "df_train['pos_word_count'] = pos_word_count_train\n",
    "df_train['neg_word'] = neg_words_train\n",
    "\n",
    "df_validation['neg_word_count'] = neg_word_count_val\n",
    "df_validation['pos_word'] = pos_words_val\n",
    "df_validation['pos_word_count'] = pos_word_count_val\n",
    "df_validation['neg_word'] = neg_words_val\n",
    "\n",
    "df_test['neg_word_count'] = neg_word_count_test\n",
    "df_test['pos_word'] = pos_words_test\n",
    "df_test['pos_word_count'] = pos_word_count_test\n",
    "df_test['neg_word'] = neg_words_test\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b646a702",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['-1_topic',\n '0_topic',\n '1_topic',\n '2_topic',\n '3_topic',\n '4_topic',\n '5_topic',\n '6_topic',\n '7_topic',\n '8_topic',\n '9_topic',\n '10_topic',\n '11_topic',\n '12_topic',\n '13_topic',\n '14_topic',\n '15_topic',\n '16_topic',\n '17_topic',\n '18_topic',\n '19_topic',\n '20_topic',\n '21_topic']"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dummies = []\n",
    "for i in topic_model.get_topic_info()['Topic'].values:\n",
    "    topic_dummies.append(f'{i}_topic')\n",
    "    \n",
    "topic_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd7b3407",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Tweet index  Label                                         Tweet text  \\\n0         2484      1  Babysitting is what I like to do on a Saturday...   \n1         2494      0          @BigAlMLM Please give me a Follow Back.\\r   \n2         1297      1  Obama wants closer relations with a Marxist na...   \n3         3755      1  Reading the Dundee United chairmanship stateme...   \n4         3119      1  There's nothing like almost rear-ending someon...   \n\n   emoji_count                                         clean_text  \\\n0            1        babysitting what like saturday unamusedface   \n1            0                            please give follow back   \n2            0  obama wants closer relations with marxist nati...   \n3            0  reading the dundee united chairmanship stateme...   \n4            0  there's nothing like almost rear-ending someon...   \n\n                                              lemmas  word_count  char_count  \\\n0          [babysitte, like, saturday, unamusedface]           5          39   \n1                                           [follow]           4          20   \n2  [obama, want, close, relation, marxist, nation...          13          70   \n3  [read, dundee, united, chairmanship, statement...          18          96   \n4  [like, rear, end, slam, brake, reason, alert, ...          19          98   \n\n   all_uppercase_count  all_lowercase_count  ...  12_topic  13_topic  \\\n0                    1                    8  ...         0         0   \n1                    0                    3  ...         0         0   \n2                    3                    8  ...         0         0   \n3                    0                   19  ...         0         0   \n4                    0                   22  ...         0         1   \n\n   14_topic  15_topic  16_topic  17_topic  18_topic  19_topic  20_topic  \\\n0         0         0         0         0         0         0         0   \n1         0         0         0         0         0         0         0   \n2         0         0         0         0         0         0         0   \n3         0         0         0         0         0         0         0   \n4         0         0         0         0         0         0         0   \n\n   21_topic  \n0         0  \n1         0  \n2         0  \n3         0  \n4         0  \n\n[5 rows x 352 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>all_uppercase_count</th>\n      <th>all_lowercase_count</th>\n      <th>...</th>\n      <th>12_topic</th>\n      <th>13_topic</th>\n      <th>14_topic</th>\n      <th>15_topic</th>\n      <th>16_topic</th>\n      <th>17_topic</th>\n      <th>18_topic</th>\n      <th>19_topic</th>\n      <th>20_topic</th>\n      <th>21_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2484</td>\n      <td>1</td>\n      <td>Babysitting is what I like to do on a Saturday...</td>\n      <td>1</td>\n      <td>babysitting what like saturday unamusedface</td>\n      <td>[babysitte, like, saturday, unamusedface]</td>\n      <td>5</td>\n      <td>39</td>\n      <td>1</td>\n      <td>8</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2494</td>\n      <td>0</td>\n      <td>@BigAlMLM Please give me a Follow Back.\\r</td>\n      <td>0</td>\n      <td>please give follow back</td>\n      <td>[follow]</td>\n      <td>4</td>\n      <td>20</td>\n      <td>0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1297</td>\n      <td>1</td>\n      <td>Obama wants closer relations with a Marxist na...</td>\n      <td>0</td>\n      <td>obama wants closer relations with marxist nati...</td>\n      <td>[obama, want, close, relation, marxist, nation...</td>\n      <td>13</td>\n      <td>70</td>\n      <td>3</td>\n      <td>8</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3755</td>\n      <td>1</td>\n      <td>Reading the Dundee United chairmanship stateme...</td>\n      <td>0</td>\n      <td>reading the dundee united chairmanship stateme...</td>\n      <td>[read, dundee, united, chairmanship, statement...</td>\n      <td>18</td>\n      <td>96</td>\n      <td>0</td>\n      <td>19</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3119</td>\n      <td>1</td>\n      <td>There's nothing like almost rear-ending someon...</td>\n      <td>0</td>\n      <td>there's nothing like almost rear-ending someon...</td>\n      <td>[like, rear, end, slam, brake, reason, alert, ...</td>\n      <td>19</td>\n      <td>98</td>\n      <td>0</td>\n      <td>22</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 352 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[topic_dummies] = pd.get_dummies(df_train['topic'])\n",
    "df_validation[topic_dummies] = pd.get_dummies(df_validation['topic'])\n",
    "df_test[topic_dummies] = pd.get_dummies(df_test['topic'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "f49072c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f49072c8",
    "outputId": "0f34e424-9c9a-4c8c-ac9d-379d9f001dec",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_ = df_train[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "#                '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "# y_ = df_train['Label']\n",
    "# clf = LogisticRegression(random_state=0, solver='liblinear').fit(X_, y_)\n",
    "\n",
    "# print('Train score')\n",
    "# print(clf.score(X_, y_))\n",
    "\n",
    "# X_val = df_validation[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "#                '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "# y_val = df_validation['Label']\n",
    "\n",
    "# print('Validation score')\n",
    "# print(clf.score(X_val, y_val))\n",
    "\n",
    "# X_val = df_test[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "#                '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "# y_val = df_test['Label']\n",
    "# print('Test score')\n",
    "# print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "a9f4718f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9f4718f",
    "outputId": "f53f55da-c8b7-426a-ef27-c85cc0f0b1b4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text        whoever runs yeovil town twitter account shoul...\n",
       "pos_word                                                     [like]\n",
       "pos_word_count                                                    1\n",
       "neg_word                                                     [fire]\n",
       "neg_word_count                                                    1\n",
       "Name: 25, dtype: object"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pos and neg words within 4 words\n",
    "df_train[['clean_text', 'pos_word', 'pos_word_count', 'neg_word', 'neg_word_count']].iloc[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fadf4766",
   "metadata": {
    "id": "fadf4766",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pos_neg_within_n(df, n=4):\n",
    "\n",
    "    ret_array = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['pos_word_count'] > 0 and row['neg_word_count'] > 0:\n",
    "            doc = nlp(row['clean_text'])\n",
    "            lemmas = []\n",
    "            for token in doc:\n",
    "                lemmas.append(token.lemma_)\n",
    "\n",
    "            pos_indexes = np.array([])\n",
    "            for word in row['pos_word']:\n",
    "                pos_indexes = np.append(pos_indexes, np.where(np.array(lemmas) == word))\n",
    "            neg_indexes = np.array([])\n",
    "            for word in row['neg_word']:\n",
    "                neg_indexes = np.append(neg_indexes, np.where(np.array(lemmas) == word))\n",
    "\n",
    "            bool_val = 0\n",
    "            for idx in pos_indexes:\n",
    "                if (abs(neg_indexes-idx) < n).any():\n",
    "                    bool_val = 1\n",
    "            ret_array.append(deepcopy(bool_val))\n",
    "        else:\n",
    "            ret_array.append(0)\n",
    "    return ret_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9367e01",
   "metadata": {
    "id": "a9367e01",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "within_5_train = pos_neg_within_n(df_train, n=5)\n",
    "within_5_val = pos_neg_within_n(df_validation, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "071f78a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "071f78a5",
    "outputId": "3ba0ff1a-9b49-4143-e83c-c112a67c5c5c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Tweet index  Label                                         Tweet text  \\\n0         2484      1  Babysitting is what I like to do on a Saturday...   \n1         2494      0          @BigAlMLM Please give me a Follow Back.\\r   \n2         1297      1  Obama wants closer relations with a Marxist na...   \n3         3755      1  Reading the Dundee United chairmanship stateme...   \n4         3119      1  There's nothing like almost rear-ending someon...   \n\n   emoji_count                                         clean_text  \\\n0            1        babysitting what like saturday unamusedface   \n1            0                            please give follow back   \n2            0  obama wants closer relations with marxist nati...   \n3            0  reading the dundee united chairmanship stateme...   \n4            0  there's nothing like almost rear-ending someon...   \n\n                                              lemmas  word_count  char_count  \\\n0          [babysitte, like, saturday, unamusedface]           5          39   \n1                                           [follow]           4          20   \n2  [obama, want, close, relation, marxist, nation...          13          70   \n3  [read, dundee, united, chairmanship, statement...          18          96   \n4  [like, rear, end, slam, brake, reason, alert, ...          19          98   \n\n   all_uppercase_count  all_lowercase_count  ...  13_topic  14_topic  \\\n0                    1                    8  ...         0         0   \n1                    0                    3  ...         0         0   \n2                    3                    8  ...         0         0   \n3                    0                   19  ...         0         0   \n4                    0                   22  ...         1         0   \n\n   15_topic  16_topic  17_topic  18_topic  19_topic  20_topic  21_topic  \\\n0         0         0         0         0         0         0         0   \n1         0         0         0         0         0         0         0   \n2         0         0         0         0         0         0         0   \n3         0         0         0         0         0         0         0   \n4         0         0         0         0         0         0         0   \n\n   pos_neg_within_5  \n0                 0  \n1                 0  \n2                 0  \n3                 0  \n4                 0  \n\n[5 rows x 353 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>emoji_count</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>all_uppercase_count</th>\n      <th>all_lowercase_count</th>\n      <th>...</th>\n      <th>13_topic</th>\n      <th>14_topic</th>\n      <th>15_topic</th>\n      <th>16_topic</th>\n      <th>17_topic</th>\n      <th>18_topic</th>\n      <th>19_topic</th>\n      <th>20_topic</th>\n      <th>21_topic</th>\n      <th>pos_neg_within_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2484</td>\n      <td>1</td>\n      <td>Babysitting is what I like to do on a Saturday...</td>\n      <td>1</td>\n      <td>babysitting what like saturday unamusedface</td>\n      <td>[babysitte, like, saturday, unamusedface]</td>\n      <td>5</td>\n      <td>39</td>\n      <td>1</td>\n      <td>8</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2494</td>\n      <td>0</td>\n      <td>@BigAlMLM Please give me a Follow Back.\\r</td>\n      <td>0</td>\n      <td>please give follow back</td>\n      <td>[follow]</td>\n      <td>4</td>\n      <td>20</td>\n      <td>0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1297</td>\n      <td>1</td>\n      <td>Obama wants closer relations with a Marxist na...</td>\n      <td>0</td>\n      <td>obama wants closer relations with marxist nati...</td>\n      <td>[obama, want, close, relation, marxist, nation...</td>\n      <td>13</td>\n      <td>70</td>\n      <td>3</td>\n      <td>8</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3755</td>\n      <td>1</td>\n      <td>Reading the Dundee United chairmanship stateme...</td>\n      <td>0</td>\n      <td>reading the dundee united chairmanship stateme...</td>\n      <td>[read, dundee, united, chairmanship, statement...</td>\n      <td>18</td>\n      <td>96</td>\n      <td>0</td>\n      <td>19</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3119</td>\n      <td>1</td>\n      <td>There's nothing like almost rear-ending someon...</td>\n      <td>0</td>\n      <td>there's nothing like almost rear-ending someon...</td>\n      <td>[like, rear, end, slam, brake, reason, alert, ...</td>\n      <td>19</td>\n      <td>98</td>\n      <td>0</td>\n      <td>22</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 353 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['pos_neg_within_5'] = within_5_train\n",
    "df_validation['pos_neg_within_5'] = within_5_val\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "6a917861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a917861",
    "outputId": "38f3877f-6bef-45bf-cb9d-c1ec2f433743",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score\n",
      "0.5183383233532934\n",
      "Validation score\n",
      "0.5152838427947598\n"
     ]
    }
   ],
   "source": [
    "X_ = df_train[[ 'pos_neg_within_5']]\n",
    "y_ = df_train['Label']\n",
    "\n",
    "clf = LogisticRegression().fit(X_, y_)\n",
    "print('Train score')\n",
    "print(clf.score(X_, y_))\n",
    "\n",
    "X_val = df_validation[[ 'pos_neg_within_5']]\n",
    "y_val = df_validation['Label']\n",
    "\n",
    "print('Validation score')\n",
    "print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b6b85",
   "metadata": {},
   "source": [
    "## Baseline classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f1446bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = ['word_count', 'char_count', 'all_uppercase_count', 'all_lowercase_count', 'capitalised_count', 'digit_count']\n",
    "\n",
    "y_train = df_train['Label']\n",
    "x_train = df_train[baseline_features]\n",
    "\n",
    "y_validation = df_validation['Label']\n",
    "x_validation = df_validation[baseline_features]\n",
    "\n",
    "y_test = df_test['Label']\n",
    "x_test = df_test[baseline_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c358297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   word_count  char_count  all_uppercase_count  all_lowercase_count  \\\n0           5          39                    1                    8   \n1           4          20                    0                    3   \n2          13          70                    3                    8   \n3          18          96                    0                   19   \n4          19          98                    0                   22   \n\n   capitalised_count  digit_count  \n0                  3            0  \n1                  3            0  \n2                  3            4  \n3                  3            0  \n4                  0            0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>all_uppercase_count</th>\n      <th>all_lowercase_count</th>\n      <th>capitalised_count</th>\n      <th>digit_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>39</td>\n      <td>1</td>\n      <td>8</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>20</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>70</td>\n      <td>3</td>\n      <td>8</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18</td>\n      <td>96</td>\n      <td>0</td>\n      <td>19</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19</td>\n      <td>98</td>\n      <td>0</td>\n      <td>22</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10bfde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_evaluate(model, x_train, y_train, x_validation, y_validation, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    print('-------------------------- TRAIN --------------------------')\n",
    "    print(classification_report(y_train, y_train_pred, digits=3), 2*'\\n')\n",
    "    \n",
    "    y_validation_pred = model.predict(x_validation)\n",
    "    print('----------------------- VALIDATION ------------------------')\n",
    "    print(classification_report(y_validation, y_validation_pred, digits=3), 2*'\\n')\n",
    "    \n",
    "    y_test_pred = model.predict(x_test)\n",
    "    print('------------------------- TEST ---------------------------')\n",
    "    print(classification_report(y_test, y_test_pred, digits=3), 2*'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680c3de",
   "metadata": {},
   "source": [
    "## Global baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a4ea71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.626     0.445     0.521      1336\n",
      "           1      0.570     0.734     0.642      1336\n",
      "\n",
      "    accuracy                          0.590      2672\n",
      "   macro avg      0.598     0.590     0.581      2672\n",
      "weighted avg      0.598     0.590     0.581      2672\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.635     0.453     0.529       580\n",
      "           1      0.566     0.733     0.639       565\n",
      "\n",
      "    accuracy                          0.591      1145\n",
      "   macro avg      0.601     0.593     0.584      1145\n",
      "weighted avg      0.601     0.591     0.583      1145\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.635     0.378     0.474       473\n",
      "           1      0.414     0.669     0.512       311\n",
      "\n",
      "    accuracy                          0.494       784\n",
      "   macro avg      0.525     0.524     0.493       784\n",
      "weighted avg      0.547     0.494     0.489       784\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1eb5de",
   "metadata": {},
   "source": [
    "## Global best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "feab4dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5973068710840421\n",
      "Best score: {'lr__C': 0.1, 'lr__class_weight': 'balanced', 'lr__penalty': 'l1', 'lr__solver': 'liblinear', 'pf__degree': 2, 'selection__k': 5}\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr = Pipeline([\n",
    "    ('selection', SelectKBest()), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pf', PolynomialFeatures()), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'selection__k': list(range(2, 20, 3)),\n",
    "    'pf__degree': [2, 3, 4, 5],\n",
    "    'lr__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'lr__class_weight': ['balanced', None],\n",
    "    'lr__C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 5],\n",
    "    'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline_lr, param_grid=params, cv=5)\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "print(f'Best score: {search.best_score_}')\n",
    "print(f'Best score: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b55cbe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.646     0.451     0.531      1336\n",
      "           1      0.578     0.752     0.654      1336\n",
      "\n",
      "    accuracy                          0.602      2672\n",
      "   macro avg      0.612     0.602     0.593      2672\n",
      "weighted avg      0.612     0.602     0.593      2672\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.600     0.421     0.494       580\n",
      "           1      0.545     0.712     0.617       565\n",
      "\n",
      "    accuracy                          0.564      1145\n",
      "   macro avg      0.572     0.566     0.556      1145\n",
      "weighted avg      0.572     0.564     0.555      1145\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.652     0.372     0.474       473\n",
      "           1      0.422     0.698     0.526       311\n",
      "\n",
      "    accuracy                          0.501       784\n",
      "   macro avg      0.537     0.535     0.500       784\n",
      "weighted avg      0.561     0.501     0.495       784\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr_best = Pipeline([\n",
    "    ('selection', SelectKBest(k=5)), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pf', PolynomialFeatures(degree=3)),\n",
    "    ('lr', LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))\n",
    "])\n",
    "\n",
    "model_fit_evaluate(pipeline_lr_best, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "545612e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5916797927823865\n",
      "Best score: {'lr__C': 0.1, 'lr__penalty': 'l1', 'lr__solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr = Pipeline([\n",
    "    #('selection', SelectKBest()), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    #'selection__k': list(range(2, 20, 3)),\n",
    "    'lr__penalty': ['l1'], \n",
    "    'lr__C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 2, 5],\n",
    "    'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline_lr, param_grid=params, cv=5)\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "print(f'Best score: {search.best_score_}')\n",
    "print(f'Best score: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_best = Pipeline([\n",
    "    ('selection', SelectKBest(k=8)), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pf', PolynomialFeatures(degree=3)),\n",
    "    ('lr', LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))\n",
    "])\n",
    "\n",
    "model_fit_evaluate(pipeline_lr_best, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072cda6",
   "metadata": {},
   "source": [
    "## Baseline classifiers for the first 4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "120eb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validation_and_test_for_topic(topic_num, df_train, df_validation, df_test):\n",
    "    df_train_topic = df_train[df_train[f'{topic_num}_topic'] == 1]\n",
    "    df_validation_topic = df_train[df_train[f'{topic_num}_topic'] == 1]\n",
    "    df_test_topic = df_train[df_train[f'{topic_num}_topic'] == 1]\n",
    "    \n",
    "    y_train = df_train_topic['Label']\n",
    "    x_train = df_train_topic[baseline_features]\n",
    "\n",
    "    y_validation = df_validation_topic['Label']\n",
    "    x_validation = df_validation_topic[baseline_features]\n",
    "\n",
    "    y_test = df_test_topic['Label']\n",
    "    x_test = df_test_topic[baseline_features]\n",
    "\n",
    "    return x_train, y_train, x_validation, y_validation, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "b64885b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.617     0.501     0.553       599\n",
      "           1      0.564     0.675     0.615       573\n",
      "\n",
      "    accuracy                          0.586      1172\n",
      "   macro avg      0.591     0.588     0.584      1172\n",
      "weighted avg      0.591     0.586     0.583      1172\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.617     0.501     0.553       599\n",
      "           1      0.564     0.675     0.615       573\n",
      "\n",
      "    accuracy                          0.586      1172\n",
      "   macro avg      0.591     0.588     0.584      1172\n",
      "weighted avg      0.591     0.586     0.583      1172\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.617     0.501     0.553       599\n",
      "           1      0.564     0.675     0.615       573\n",
      "\n",
      "    accuracy                          0.586      1172\n",
      "   macro avg      0.591     0.588     0.584      1172\n",
      "weighted avg      0.591     0.586     0.583      1172\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_ = \\\n",
    "    get_train_validation_and_test_for_topic(-1, df_train, df_validation, df_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "f51a8bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.632     0.542     0.583       168\n",
      "           1      0.613     0.697     0.652       175\n",
      "\n",
      "    accuracy                          0.621       343\n",
      "   macro avg      0.623     0.619     0.618       343\n",
      "weighted avg      0.622     0.621     0.619       343\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.632     0.542     0.583       168\n",
      "           1      0.613     0.697     0.652       175\n",
      "\n",
      "    accuracy                          0.621       343\n",
      "   macro avg      0.623     0.619     0.618       343\n",
      "weighted avg      0.622     0.621     0.619       343\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.632     0.542     0.583       168\n",
      "           1      0.613     0.697     0.652       175\n",
      "\n",
      "    accuracy                          0.621       343\n",
      "   macro avg      0.623     0.619     0.618       343\n",
      "weighted avg      0.622     0.621     0.619       343\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_ = \\\n",
    "    get_train_validation_and_test_for_topic(0, df_train, df_validation, df_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "4c7e7bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.606     0.679     0.640        84\n",
      "           1      0.571     0.493     0.529        73\n",
      "\n",
      "    accuracy                          0.592       157\n",
      "   macro avg      0.589     0.586     0.585       157\n",
      "weighted avg      0.590     0.592     0.589       157\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.606     0.679     0.640        84\n",
      "           1      0.571     0.493     0.529        73\n",
      "\n",
      "    accuracy                          0.592       157\n",
      "   macro avg      0.589     0.586     0.585       157\n",
      "weighted avg      0.590     0.592     0.589       157\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.606     0.679     0.640        84\n",
      "           1      0.571     0.493     0.529        73\n",
      "\n",
      "    accuracy                          0.592       157\n",
      "   macro avg      0.589     0.586     0.585       157\n",
      "weighted avg      0.590     0.592     0.589       157\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_ = \\\n",
    "    get_train_validation_and_test_for_topic(1, df_train, df_validation, df_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "feb7bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- TRAIN --------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.657     0.878     0.751        74\n",
      "           1      0.591     0.277     0.377        47\n",
      "\n",
      "    accuracy                          0.645       121\n",
      "   macro avg      0.624     0.577     0.564       121\n",
      "weighted avg      0.631     0.645     0.606       121\n",
      " \n",
      "\n",
      "\n",
      "----------------------- VALIDATION ------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.657     0.878     0.751        74\n",
      "           1      0.591     0.277     0.377        47\n",
      "\n",
      "    accuracy                          0.645       121\n",
      "   macro avg      0.624     0.577     0.564       121\n",
      "weighted avg      0.631     0.645     0.606       121\n",
      " \n",
      "\n",
      "\n",
      "------------------------- TEST ---------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.657     0.878     0.751        74\n",
      "           1      0.591     0.277     0.377        47\n",
      "\n",
      "    accuracy                          0.645       121\n",
      "   macro avg      0.624     0.577     0.564       121\n",
      "weighted avg      0.631     0.645     0.606       121\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_ = \\\n",
    "    get_train_validation_and_test_for_topic(2, df_train, df_validation, df_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model_fit_evaluate(model, x_train_, y_train_, x_validation_, y_validation_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HJvVNgSQGXBE",
   "metadata": {
    "id": "HJvVNgSQGXBE"
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y7QInQCTGXF9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7QInQCTGXF9",
    "outputId": "09832e27-4753-481a-c53a-9ff38ae7507b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94000e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793da6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pipeline_lr_best['lr'].coef_.flatten()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.bar(list(range(len(weights))), weights)\n",
    "plt.ylabel('Value of the weight')\n",
    "plt.xlabel('Index of the weight')\n",
    "plt.title('Feature importance plot for logistic regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w7iw-WmyGXKC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7iw-WmyGXKC",
    "outputId": "89cd15f7-2fa4-4922-ce48-c55af7211d87"
   },
   "outputs": [],
   "source": [
    "pipeline_dtc = Pipeline([\n",
    "    ('selection', SelectKBest()), \n",
    "    ('dtc', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'selection__k': list(range(2, 20, 3)),\n",
    "    'dtc__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'dtc__ccp_alpha': [0.1, .01, .001],\n",
    "    'dtc__max_depth' : list(range(2, 10)),\n",
    "    'dtc__criterion' : ['gini', 'entropy', 'log_loss']\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipeline_dtc, param_grid=params, cv=5)\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "print(f'Best score: {search.best_score_}')\n",
    "print(f'Best score: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dtc_best = Pipeline([\n",
    "    ('selection', SelectKBest(k=5)), \n",
    "    ('dtc', DecisionTreeClassifier(ccp_alpha=0.001, criterion='entropy', max_depth=5, max_features='auto')),\n",
    "])\n",
    "\n",
    "model_fit_evaluate(pipeline_dtc_best, x_train, y_train, x_validation, y_validation, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pipeline_dtc_best['dtc'].feature_importances_.flatten()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.bar(list(range(len(feat_importances))), feat_importances)\n",
    "plt.ylabel('Feature importance value')\n",
    "plt.xlabel('Index of the feature')\n",
    "plt.title('Feature importance plot for decision tree')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}