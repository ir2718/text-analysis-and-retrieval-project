{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "10rFQ6qgEfwG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10rFQ6qgEfwG",
    "outputId": "f1eb8986-5c3b-4ae1-8c9a-767bffd653ec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (8.0.16)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "JxgMNAW8EjHe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxgMNAW8EjHe",
    "outputId": "996ef455-5621-4268-8359-7e2101546f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\n",
      "     ---------------------------------------- 33.5/33.5 MB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from en-core-web-md==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (58.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.16)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "emet_2xgEjLL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emet_2xgEjLL",
    "outputId": "88cc1f0e-389f-48c4-d2ba-6eaccc5580b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: podium-nlp in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (2.27.1)\n",
      "Requirement already satisfied: nltk<3.6,>=3.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (3.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (1.22.3)\n",
      "Requirement already satisfied: dill in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (0.3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (1.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (4.64.0)\n",
      "Requirement already satisfied: paramiko in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (2.10.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from podium-nlp) (1.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk<3.6,>=3.0->podium-nlp) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk<3.6,>=3.0->podium-nlp) (1.1.0)\n",
      "Requirement already satisfied: regex in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk<3.6,>=3.0->podium-nlp) (2022.4.24)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->podium-nlp) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->podium-nlp) (2.8.2)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from paramiko->podium-nlp) (1.5.0)\n",
      "Requirement already satisfied: six in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from paramiko->podium-nlp) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from paramiko->podium-nlp) (37.0.1)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from paramiko->podium-nlp) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->podium-nlp) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->podium-nlp) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->podium-nlp) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->podium-nlp) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->podium-nlp) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->podium-nlp) (0.4.4)\n",
      "Requirement already satisfied: cffi>=1.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bcrypt>=3.1.3->paramiko->podium-nlp) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->podium-nlp) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install podium-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8d4e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.8.28)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan) (1.0.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan) (0.29.28)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "VDPtYtvQElF0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDPtYtvQElF0",
    "outputId": "ee94560a-7cb6-440a-a027-cac624e26a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (4.64.0)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (5.8.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (1.4.2)\n",
      "Requirement already satisfied: pyyaml<6.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (5.4.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (0.5.3)\n",
      "Requirement already satisfied: hdbscan>=0.8.28 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (0.8.28)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (1.22.4)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.19.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.11.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.12.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.6.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.4)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.5.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (58.1.0)\n",
      "Collecting numpy>=1.20.0\n",
      "  Using cached numpy-1.21.6-cp39-cp39-win_amd64.whl (14.0 MB)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.4.24)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: click in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "Successfully installed numpy-1.21.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tmtoolkit 0.11.2 requires numpy>=1.22.0, but you have numpy 1.21.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "PCRcR-b_ExaN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCRcR-b_ExaN",
    "outputId": "7d142e0b-73c2-4433-dd65-1dbbdf71d0c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77de93e4",
   "metadata": {
    "id": "77de93e4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import emoji\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "EnG-1KsiOp7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnG-1KsiOp7d",
    "outputId": "9668126a-955e-45f9-8a47-810b3809b270"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "NgNYHv8MPOgK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgNYHv8MPOgK",
    "outputId": "8b773729-8c76-4b84-82d4-238cf443f712"
   },
   "outputs": [],
   "source": [
    "# %cd drive/MyDrive/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08931560",
   "metadata": {
    "id": "08931560"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train/SemEval2018-T3-train-taskA.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')\n",
    "df_test = pd.read_csv(\"goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')\n",
    "df_replace = pd.read_csv(\"test_TaskA/SemEval2018-T3_input_test_taskA.txt\", sep='\\t', lineterminator='\\n', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)\n",
    "df_test.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)\n",
    "df_replace.rename({'Tweet text\\r': 'Tweet text'}, inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0       Sweet United Nations video. Just in time for C...\n1       @mrdahl87 We are rumored to have talked to Erv...\n2       Hey there! Nice to see you Minnesota/ND Winter...\n3                   3 episodes left I'm dying over here\\r\n4       I can't breathe! was chosen as the most notabl...\n                              ...                        \n3812    @banditelli regarding what the PSU president d...\n3813    @banditelli But still bothers me that I see no...\n3814    well now that i've listened to all of into the...\n3815    Hummingbirds #Are  #Experts #at #Hovering #Aft...\n3816    Only thing missing now is a session at the gym...\nName: Tweet text, Length: 3817, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet text'].apply(emoji.demojize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90da0270",
   "metadata": {
    "id": "90da0270",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_test['Tweet text'] = df_test['Tweet text'].apply(emoji.demojize)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting clean text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5badc5e",
   "metadata": {
    "id": "e5badc5e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_words(s):\n",
    "    '''\n",
    "    Removes tags, emojis, links, smiley faces, hashtag signs, \n",
    "    stopwords and changes the case to lower.\n",
    "    '''\n",
    "    ret_list = []\n",
    "\n",
    "    smiley_regex = r'([\\:\\;\\=][\\(\\)PDO\\/\\\\\\]\\[]+)+'\n",
    "    \n",
    "    is_tag = lambda w: w.startswith('@')\n",
    "    is_vertical_line = lambda w: w.startswith('|')\n",
    "    is_emoji = lambda w: w != ':' and w.startswith(':') and w.endswith(':')\n",
    "    remove_emoji = lambda w: w[:w.index(':')] + w[w.rindex(':')+1:] if ':' in w and w.index(':') != w.rindex(':') else w\n",
    "    is_link = lambda w: w.startswith(\"http\") or w.startswith(\"https\")\n",
    "    is_hashtag = lambda w: w.startswith(\"#\")\n",
    "    is_smiley = lambda w: re.match(smiley_regex, w)\n",
    "    \n",
    "    for i, s_i in enumerate(s):\n",
    "        w_arr = s_i.split()\n",
    "        w2 = []\n",
    "        for w in w_arr:\n",
    "            if is_tag(w) or is_emoji(w) or is_link(w) or is_vertical_line(w):\n",
    "                continue\n",
    "            \n",
    "            elif is_hashtag(w):\n",
    "                w_tmp = w.replace('#', '')\n",
    "                if w_tmp != '':\n",
    "                    lower_append(w_tmp, w2)\n",
    "            \n",
    "            elif is_smiley(w):\n",
    "                w_tmp = re.sub(smiley_regex, '', w)\n",
    "                if w_tmp != '':\n",
    "                    lower_append(w_tmp, w2)\n",
    "            \n",
    "            else:\n",
    "                if w != '':\n",
    "                    w.replace('#', '')\n",
    "                    w.replace('|', '')\n",
    "                    w.replace('_', '')\n",
    "                    lower_append(w, w2)\n",
    "\n",
    "        ret_list.append(' '.join(w2))\n",
    "    return ret_list\n",
    "\n",
    "def lower_append(w, l):\n",
    "    l.append(w.lower())\n",
    "\n",
    "df['clean_text'] = df[['Tweet text']].apply(preprocess_words)\n",
    "df_test['clean_text'] = df_test[['Tweet text']].apply(preprocess_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Tweet index  Label                                         Tweet text  \\\n0             1      1  Sweet United Nations video. Just in time for C...   \n1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3             4      0              3 episodes left I'm dying over here\\r   \n4             5      1  I can't breathe! was chosen as the most notabl...   \n5             6      0  You're never too old for Footie Pajamas. http:...   \n6             7      1  Nothing makes me happier then getting on the h...   \n7             8      0  4:30 an opening my first beer now gonna be a l...   \n8             9      0  @Adam_Klug do you think you would support a gu...   \n9            10      0  @samcguigan544 You are not allowed to open tha...   \n10           11      1  Oh, thank GOD - our entire office email system...   \n11           12      0  But instead, I'm scrolling through Facebook, I...   \n12           13      0  @TargetZonePT :pouting_face: no he bloody isn'...   \n13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n14           15      1  Just great when you're mobile bill arrives by ...   \n\n                                           clean_text  \n0   sweet united nations video. just in time for c...  \n1   we are rumored to have talked to erv's agent.....  \n2   hey there! nice to see you minnesota/nd winter...  \n3                 3 episodes left i'm dying over here  \n4   i can't breathe! was chosen as the most notabl...  \n5            you're never too old for footie pajamas.  \n6   nothing makes me happier then getting on the h...  \n7   4:30 an opening my first beer now gonna be a l...  \n8   do you think you would support a guy who knock...  \n9   you are not allowed to open that until christm...  \n10  oh, thank god - our entire office email system...  \n11  but instead, i'm scrolling through facebook, i...  \n12  no he bloody isn't i was upstairs getting chan...  \n13  cold or warmth both suffuse one's cheeks with ...  \n14  just great when you're mobile bill arrives by ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>sweet united nations video. just in time for c...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>we are rumored to have talked to erv's agent.....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>hey there! nice to see you minnesota/nd winter...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>3 episodes left i'm dying over here</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>i can't breathe! was chosen as the most notabl...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>You're never too old for Footie Pajamas. http:...</td>\n      <td>you're never too old for footie pajamas.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>Nothing makes me happier then getting on the h...</td>\n      <td>nothing makes me happier then getting on the h...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>@Adam_Klug do you think you would support a gu...</td>\n      <td>do you think you would support a guy who knock...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>@samcguigan544 You are not allowed to open tha...</td>\n      <td>you are not allowed to open that until christm...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>Oh, thank GOD - our entire office email system...</td>\n      <td>oh, thank god - our entire office email system...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>But instead, I'm scrolling through Facebook, I...</td>\n      <td>but instead, i'm scrolling through facebook, i...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>@TargetZonePT :pouting_face: no he bloody isn'...</td>\n      <td>no he bloody isn't i was upstairs getting chan...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n      <td>cold or warmth both suffuse one's cheeks with ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>Just great when you're mobile bill arrives by ...</td>\n      <td>just great when you're mobile bill arrives by ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def simple_preprocessing(s):\n",
    "    '''Lowercases, tokenizes, de-accents, removes words shorter than 3 and longer than 14 characters'''\n",
    "    return [' '.join(simple_preprocess(s_i)) for s_i in s]\n",
    "\n",
    "df['clean_text'] = df[['clean_text']].apply(simple_preprocessing)\n",
    "df_test['clean_text'] = df_test[['clean_text']].apply(simple_preprocessing)\n",
    "# tweet_text_train = tweet_text_train.apply(simple_preprocessing)\n",
    "# tweet_text_test = tweet_text_test.apply(simple_preprocessing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Tweet index  Label                                         Tweet text  \\\n0             1      1  Sweet United Nations video. Just in time for C...   \n1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3             4      0              3 episodes left I'm dying over here\\r   \n4             5      1  I can't breathe! was chosen as the most notabl...   \n5             6      0  You're never too old for Footie Pajamas. http:...   \n6             7      1  Nothing makes me happier then getting on the h...   \n7             8      0  4:30 an opening my first beer now gonna be a l...   \n8             9      0  @Adam_Klug do you think you would support a gu...   \n9            10      0  @samcguigan544 You are not allowed to open tha...   \n10           11      1  Oh, thank GOD - our entire office email system...   \n11           12      0  But instead, I'm scrolling through Facebook, I...   \n12           13      0  @TargetZonePT :pouting_face: no he bloody isn'...   \n13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n14           15      1  Just great when you're mobile bill arrives by ...   \n\n                                           clean_text  \n0   sweet united nations video just in time for ch...  \n1   we are rumored to have talked to erv agent and...  \n2   hey there nice to see you minnesota nd winter ...  \n3                       episodes left dying over here  \n4   can breathe was chosen as the most notable quo...  \n5             you re never too old for footie pajamas  \n6   nothing makes me happier then getting on the h...  \n7   an opening my first beer now gonna be long nig...  \n8   do you think you would support guy who knocked...  \n9   you are not allowed to open that until christm...  \n10  oh thank god our entire office email system is...  \n11  but instead scrolling through facebook instagr...  \n12      no he bloody isn was upstairs getting changed  \n13  cold or warmth both suffuse one cheeks with pi...  \n14  just great when you re mobile bill arrives by ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>sweet united nations video just in time for ch...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>we are rumored to have talked to erv agent and...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>hey there nice to see you minnesota nd winter ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>episodes left dying over here</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>can breathe was chosen as the most notable quo...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>You're never too old for Footie Pajamas. http:...</td>\n      <td>you re never too old for footie pajamas</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>Nothing makes me happier then getting on the h...</td>\n      <td>nothing makes me happier then getting on the h...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n      <td>an opening my first beer now gonna be long nig...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>@Adam_Klug do you think you would support a gu...</td>\n      <td>do you think you would support guy who knocked...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>@samcguigan544 You are not allowed to open tha...</td>\n      <td>you are not allowed to open that until christm...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>Oh, thank GOD - our entire office email system...</td>\n      <td>oh thank god our entire office email system is...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>But instead, I'm scrolling through Facebook, I...</td>\n      <td>but instead scrolling through facebook instagr...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>@TargetZonePT :pouting_face: no he bloody isn'...</td>\n      <td>no he bloody isn was upstairs getting changed</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n      <td>cold or warmth both suffuse one cheeks with pi...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>Just great when you're mobile bill arrives by ...</td>\n      <td>just great when you re mobile bill arrives by ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "    Tweet index  Label                                         Tweet text  \\\n0             1      1  Sweet United Nations video. Just in time for C...   \n1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3             4      0              3 episodes left I'm dying over here\\r   \n4             5      1  I can't breathe! was chosen as the most notabl...   \n5             6      0  You're never too old for Footie Pajamas. http:...   \n6             7      1  Nothing makes me happier then getting on the h...   \n7             8      0  4:30 an opening my first beer now gonna be a l...   \n8             9      0  @Adam_Klug do you think you would support a gu...   \n9            10      0  @samcguigan544 You are not allowed to open tha...   \n10           11      1  Oh, thank GOD - our entire office email system...   \n11           12      0  But instead, I'm scrolling through Facebook, I...   \n12           13      0  @TargetZonePT :pouting_face: no he bloody isn'...   \n13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n14           15      1  Just great when you're mobile bill arrives by ...   \n\n                                           clean_text  \n0   sweet united nations video just in time for ch...  \n1   we are rumored to have talked to erv agent and...  \n2   hey there nice to see you minnesota nd winter ...  \n3                       episodes left dying over here  \n4   can breathe was chosen as the most notable quo...  \n5             you re never too old for footie pajamas  \n6   nothing makes me happier then getting on the h...  \n7   an opening my first beer now gonna be long nig...  \n8   do you think you would support guy who knocked...  \n9   you are not allowed to open that until christm...  \n10  oh thank god our entire office email system is...  \n11  but instead scrolling through facebook instagr...  \n12      no he bloody isn was upstairs getting changed  \n13  cold or warmth both suffuse one cheeks with pi...  \n14  just great when you re mobile bill arrives by ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>sweet united nations video just in time for ch...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>we are rumored to have talked to erv agent and...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>hey there nice to see you minnesota nd winter ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>episodes left dying over here</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>can breathe was chosen as the most notable quo...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>You're never too old for Footie Pajamas. http:...</td>\n      <td>you re never too old for footie pajamas</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>Nothing makes me happier then getting on the h...</td>\n      <td>nothing makes me happier then getting on the h...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n      <td>an opening my first beer now gonna be long nig...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>@Adam_Klug do you think you would support a gu...</td>\n      <td>do you think you would support guy who knocked...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>@samcguigan544 You are not allowed to open tha...</td>\n      <td>you are not allowed to open that until christm...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>Oh, thank GOD - our entire office email system...</td>\n      <td>oh thank god our entire office email system is...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>But instead, I'm scrolling through Facebook, I...</td>\n      <td>but instead scrolling through facebook instagr...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>@TargetZonePT :pouting_face: no he bloody isn'...</td>\n      <td>no he bloody isn was upstairs getting changed</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n      <td>cold or warmth both suffuse one cheeks with pi...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>Just great when you're mobile bill arrives by ...</td>\n      <td>just great when you re mobile bill arrives by ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_tuple_characters(s):\n",
    "    return [re.sub(r'(.)\\1{2,}', r'\\1', w) for w in s]\n",
    "\n",
    "df['clean_text'] = df[['clean_text']].apply(remove_tuple_characters)\n",
    "df_test['clean_text'] = df_test[['clean_text']].apply(remove_tuple_characters)\n",
    "# tweet_text_train = tweet_text_train.apply(remove_tuple_characters)\n",
    "# tweet_text_test = tweet_text_test.apply(remove_tuple_characters)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def lemmatize(s):\n",
    "    '''Lemmatizes the words in the sentences and returns them if theyre not stopwords or punctuation'''\n",
    "    return [[w.lemma_.lower() for w in nlp(s_i) if w.lemma_.lower() not in nlp.Defaults.stop_words and not w.is_punct] for s_i in s]\n",
    "\n",
    "df['lemmas'] = df[['clean_text']].apply(lemmatize)\n",
    "df_test['lemmas'] = df_test[['clean_text']].apply(lemmatize)\n",
    "# tweet_text_train = tweet_text_train.apply(lemmatize)\n",
    "# tweet_text_test = tweet_text_test.apply(lemmatize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "    Tweet index  Label                                         Tweet text  \\\n0             1      1  Sweet United Nations video. Just in time for C...   \n1             2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2             3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3             4      0              3 episodes left I'm dying over here\\r   \n4             5      1  I can't breathe! was chosen as the most notabl...   \n5             6      0  You're never too old for Footie Pajamas. http:...   \n6             7      1  Nothing makes me happier then getting on the h...   \n7             8      0  4:30 an opening my first beer now gonna be a l...   \n8             9      0  @Adam_Klug do you think you would support a gu...   \n9            10      0  @samcguigan544 You are not allowed to open tha...   \n10           11      1  Oh, thank GOD - our entire office email system...   \n11           12      0  But instead, I'm scrolling through Facebook, I...   \n12           13      0  @TargetZonePT :pouting_face: no he bloody isn'...   \n13           14      0  Cold or warmth both suffuse one's cheeks with ...   \n14           15      1  Just great when you're mobile bill arrives by ...   \n\n                                           clean_text  \\\n0   sweet united nations video just in time for ch...   \n1   we are rumored to have talked to erv agent and...   \n2   hey there nice to see you minnesota nd winter ...   \n3                       episodes left dying over here   \n4   can breathe was chosen as the most notable quo...   \n5             you re never too old for footie pajamas   \n6   nothing makes me happier then getting on the h...   \n7   an opening my first beer now gonna be long nig...   \n8   do you think you would support guy who knocked...   \n9   you are not allowed to open that until christm...   \n10  oh thank god our entire office email system is...   \n11  but instead scrolling through facebook instagr...   \n12      no he bloody isn was upstairs getting changed   \n13  cold or warmth both suffuse one cheeks with pi...   \n14  just great when you re mobile bill arrives by ...   \n\n                                               lemmas  \n0   [sweet, united, nations, video, time, christma...  \n1   [rumor, talk, erv, agent, angel, ask, ed, esco...  \n2         [hey, nice, minnesota, nd, winter, weather]  \n3                               [episode, leave, die]  \n4   [breathe, choose, notable, quote, year, annual...  \n5                               [old, footie, pajama]  \n6   [happy, highway, break, light, light, like, ch...  \n7            [opening, beer, going, long, night, day]  \n8   [think, support, guy, knock, daughter, rice, d...  \n9                       [allow, open, christmas, day]  \n10  [oh, thank, god, entire, office, email, system...  \n11  [instead, scroll, facebook, instagram, twitter...  \n12           [bloody, isn, upstairs, getting, change]  \n13  [cold, warmth, suffuse, cheek, pink, colour, t...  \n14                [great, mobile, bill, arrive, text]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>sweet united nations video just in time for ch...</td>\n      <td>[sweet, united, nations, video, time, christma...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>we are rumored to have talked to erv agent and...</td>\n      <td>[rumor, talk, erv, agent, angel, ask, ed, esco...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>hey there nice to see you minnesota nd winter ...</td>\n      <td>[hey, nice, minnesota, nd, winter, weather]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>episodes left dying over here</td>\n      <td>[episode, leave, die]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>can breathe was chosen as the most notable quo...</td>\n      <td>[breathe, choose, notable, quote, year, annual...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>You're never too old for Footie Pajamas. http:...</td>\n      <td>you re never too old for footie pajamas</td>\n      <td>[old, footie, pajama]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>Nothing makes me happier then getting on the h...</td>\n      <td>nothing makes me happier then getting on the h...</td>\n      <td>[happy, highway, break, light, light, like, ch...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>4:30 an opening my first beer now gonna be a l...</td>\n      <td>an opening my first beer now gonna be long nig...</td>\n      <td>[opening, beer, going, long, night, day]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0</td>\n      <td>@Adam_Klug do you think you would support a gu...</td>\n      <td>do you think you would support guy who knocked...</td>\n      <td>[think, support, guy, knock, daughter, rice, d...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>0</td>\n      <td>@samcguigan544 You are not allowed to open tha...</td>\n      <td>you are not allowed to open that until christm...</td>\n      <td>[allow, open, christmas, day]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>Oh, thank GOD - our entire office email system...</td>\n      <td>oh thank god our entire office email system is...</td>\n      <td>[oh, thank, god, entire, office, email, system...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>0</td>\n      <td>But instead, I'm scrolling through Facebook, I...</td>\n      <td>but instead scrolling through facebook instagr...</td>\n      <td>[instead, scroll, facebook, instagram, twitter...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>@TargetZonePT :pouting_face: no he bloody isn'...</td>\n      <td>no he bloody isn was upstairs getting changed</td>\n      <td>[bloody, isn, upstairs, getting, change]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>Cold or warmth both suffuse one's cheeks with ...</td>\n      <td>cold or warmth both suffuse one cheeks with pi...</td>\n      <td>[cold, warmth, suffuse, cheek, pink, colour, t...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>Just great when you're mobile bill arrives by ...</td>\n      <td>just great when you re mobile bill arrives by ...</td>\n      <td>[great, mobile, bill, arrive, text]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def word_counter(s):\n",
    "    splitted = s.split()\n",
    "    newlist = [x for x in splitted if not x.startswith((\"@\", \"#\", \"http\"))]\n",
    "    return len(newlist)\n",
    "\n",
    "def char_counter(s):\n",
    "    s_new = copy.copy(s)\n",
    "    s_new = s_new.replace(' ', '')\n",
    "    return len(s_new)\n",
    "\n",
    "def tag_counter(s):\n",
    "    splitted = s.split()\n",
    "    newlist = [x for x in splitted if x.startswith(\"@\")]\n",
    "    return len(newlist)\n",
    "\n",
    "def hashtag_counter(s):\n",
    "    splitted = s.split()\n",
    "    newlist = [x for x in splitted if x.startswith(\"#\")]\n",
    "    return len(newlist)\n",
    "\n",
    "def has_emoji(s):\n",
    "    ## potreban upgrade, ne pronalazi sve emoji-e\n",
    "    splitted = s.split()\n",
    "    newlist = [x for x in splitted if x != \":\" and x.startswith(\":\") and x.endswith(\":\")]\n",
    "    return len(newlist)\n",
    "\n",
    "# def clean_text(s):\n",
    "#     splitted = s.split()\n",
    "#     newlist = [x for x in splitted if not x.startswith((\":\", \"@\", \"#\"))]\n",
    "#     return ' '.join(newlist)\n",
    "\n",
    "### dodani featuresi ###\n",
    "def link_counter(s):\n",
    "    splitted = s.split()\n",
    "    newlist = [x for x in splitted if x.startswith(('http:', 'https:'))]\n",
    "    return len(newlist)\n",
    "\n",
    "def smiley_counter(s):\n",
    "    splitted = s.split()\n",
    "    newlist = [x for x in splitted if re.match(r'([\\:\\;\\=][\\(\\)PDO\\/\\\\\\]\\[]+)+', x)]\n",
    "    return len(newlist)\n",
    "\n",
    "# def exclamation_mark_counter(s):\n",
    "#     return s.count('!')\n",
    "\n",
    "def mark_counter(s):\n",
    "    return (s.count('!') + s.count('?') + s.count('...'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "df['word_count'] = df['clean_text'].apply(word_counter)\n",
    "df['char_count'] = df['clean_text'].apply(char_counter)\n",
    "df['tag_count'] = df['Tweet text'].apply(tag_counter)\n",
    "df['hashtag_count'] = df['Tweet text'].apply(hashtag_counter)\n",
    "df['link_count'] = df['Tweet text'].apply(link_counter)\n",
    "df['smiley_count'] = df['Tweet text'].apply(smiley_counter)\n",
    "df['mark_count'] = df['Tweet text'].apply(mark_counter)\n",
    "df['has_emoji'] = df['Tweet text'].apply(has_emoji)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df_test['word_count'] = df_test['clean_text'].apply(word_counter)\n",
    "df_test['char_count'] = df_test['clean_text'].apply(char_counter)\n",
    "df_test['tag_count'] = df_test['Tweet text'].apply(tag_counter)\n",
    "df_test['hashtag_count'] = df_test['Tweet text'].apply(hashtag_counter)\n",
    "df_test['link_count'] = df_test['Tweet text'].apply(link_counter)\n",
    "df_test['smiley_count'] = df_test['Tweet text'].apply(smiley_counter)\n",
    "df_test['mark_count'] = df_test['Tweet text'].apply(mark_counter)\n",
    "df_test['has_emoji'] = df_test['Tweet text'].apply(has_emoji)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n0               1      1  Sweet United Nations video. Just in time for C...   \n1               2      1  @mrdahl87 We are rumored to have talked to Erv...   \n2               3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n3               4      0              3 episodes left I'm dying over here\\r   \n4               5      1  I can't breathe! was chosen as the most notabl...   \n...           ...    ...                                                ...   \n3812         3830      0  @banditelli regarding what the PSU president d...   \n3813         3831      0  @banditelli But still bothers me that I see no...   \n3814         3832      0  well now that i've listened to all of into the...   \n3815         3833      0  Hummingbirds #Are  #Experts #at #Hovering #Aft...   \n3816         3834      0  Only thing missing now is a session at the gym...   \n\n                                             clean_text  \\\n0     sweet united nations video just in time for ch...   \n1     we are rumored to have talked to erv agent and...   \n2     hey there nice to see you minnesota nd winter ...   \n3                         episodes left dying over here   \n4     can breathe was chosen as the most notable quo...   \n...                                                 ...   \n3812              regarding what the psu president does   \n3813  but still bothers me that see now follow up re...   \n3814  well now that ve listened to all of into the w...   \n3815  hummingbirds are experts at hovering after all...   \n3816  only thing missing now is session at the gym w...   \n\n                                                 lemmas  word_count  \\\n0     [sweet, united, nations, video, time, christma...          11   \n1     [rumor, talk, erv, agent, angel, ask, ed, esco...          19   \n2           [hey, nice, minnesota, nd, winter, weather]          10   \n3                                 [episode, leave, die]           5   \n4     [breathe, choose, notable, quote, year, annual...          21   \n...                                                 ...         ...   \n3812                           [regard, psu, president]           6   \n3813                           [bother, follow, report]          10   \n3814   [ve, listen, wood, listen, fob, nosurprisethere]          18   \n3815  [hummingbird, expert, hover, background, motio...          15   \n3816  [thing, miss, session, gym, want, body, cv, po...          19   \n\n      char_count  tag_count  hashtag_count  link_count  smiley_count  \\\n0             62          0              2           1             0   \n1             82          1              0           0             1   \n2             44          0              0           0             0   \n3             25          0              0           0             0   \n4             96          0              0           0             0   \n...          ...        ...            ...         ...           ...   \n3812          32          1              0           0             0   \n3813          41          1              0           0             0   \n3814          77          0              1           0             0   \n3815          81          0              8           1             0   \n3816          76          0              1           0             0   \n\n      mark_count  has_emoji  \n0              0          0  \n1              2          0  \n2              1          0  \n3              0          0  \n4              1          0  \n...          ...        ...  \n3812           0          0  \n3813           0          0  \n3814           0          0  \n3815           2          0  \n3816           2          0  \n\n[3817 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>tag_count</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>smiley_count</th>\n      <th>mark_count</th>\n      <th>has_emoji</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Sweet United Nations video. Just in time for C...</td>\n      <td>sweet united nations video just in time for ch...</td>\n      <td>[sweet, united, nations, video, time, christma...</td>\n      <td>11</td>\n      <td>62</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n      <td>we are rumored to have talked to erv agent and...</td>\n      <td>[rumor, talk, erv, agent, angel, ask, ed, esco...</td>\n      <td>19</td>\n      <td>82</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n      <td>hey there nice to see you minnesota nd winter ...</td>\n      <td>[hey, nice, minnesota, nd, winter, weather]</td>\n      <td>10</td>\n      <td>44</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>3 episodes left I'm dying over here\\r</td>\n      <td>episodes left dying over here</td>\n      <td>[episode, leave, die]</td>\n      <td>5</td>\n      <td>25</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>I can't breathe! was chosen as the most notabl...</td>\n      <td>can breathe was chosen as the most notable quo...</td>\n      <td>[breathe, choose, notable, quote, year, annual...</td>\n      <td>21</td>\n      <td>96</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3812</th>\n      <td>3830</td>\n      <td>0</td>\n      <td>@banditelli regarding what the PSU president d...</td>\n      <td>regarding what the psu president does</td>\n      <td>[regard, psu, president]</td>\n      <td>6</td>\n      <td>32</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3813</th>\n      <td>3831</td>\n      <td>0</td>\n      <td>@banditelli But still bothers me that I see no...</td>\n      <td>but still bothers me that see now follow up re...</td>\n      <td>[bother, follow, report]</td>\n      <td>10</td>\n      <td>41</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3814</th>\n      <td>3832</td>\n      <td>0</td>\n      <td>well now that i've listened to all of into the...</td>\n      <td>well now that ve listened to all of into the w...</td>\n      <td>[ve, listen, wood, listen, fob, nosurprisethere]</td>\n      <td>18</td>\n      <td>77</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3815</th>\n      <td>3833</td>\n      <td>0</td>\n      <td>Hummingbirds #Are  #Experts #at #Hovering #Aft...</td>\n      <td>hummingbirds are experts at hovering after all...</td>\n      <td>[hummingbird, expert, hover, background, motio...</td>\n      <td>15</td>\n      <td>81</td>\n      <td>0</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3816</th>\n      <td>3834</td>\n      <td>0</td>\n      <td>Only thing missing now is a session at the gym...</td>\n      <td>only thing missing now is session at the gym w...</td>\n      <td>[thing, miss, session, gym, want, body, cv, po...</td>\n      <td>19</td>\n      <td>76</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3817 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.7)\n",
    "df_validation = df.drop(df_train.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n1225         1226      1  I'm a Victoria Secret model. It's such a secre...   \n418           419      1  Left my lunch at home. Swansea canteen outdoin...   \n2057         2061      0  @tweetkumud we are here to fight endlessly and...   \n426           427      1  love, love, love how the left side of my face ...   \n2011         2015      1  @MattHDGamer FUT14 ofc, the corners <3___<3 th...   \n...           ...    ...                                                ...   \n1492         1493      1  Welcome to twitter @orafa2 myself n @SlimDandy...   \n2282         2286      0     95% of my clothes are black. #noshame :bomb:\\r   \n2368         2372      0              This pressure is unreal. Keep it up\\r   \n939           940      0  Wings? I don't have wings! |\"Of course not... ...   \n677           678      0  #finalsweekgotmelike Ugh. Make it stop. #final...   \n\n                                             clean_text  \\\n1225  victoria secret model it such secret not even ...   \n418   left my lunch at home swansea canteen outdoing...   \n2057  we are here to fight endlessly and change the ...   \n426   love love love how the left side of my face is...   \n2011           fut ofc the corners they where fantastic   \n...                                                 ...   \n1492  welcome to twitter myself at ur service as ur ...   \n2282                    of my clothes are black noshame   \n2368                 this pressure is unreal keep it up   \n939          wings don have wings course not you re boy   \n677              ugh make it stop finals finalsweek ewu   \n\n                                                 lemmas  word_count  \\\n1225  [victoria, secret, model, secret, victoria, know]          10   \n418   [leave, lunch, home, swansea, canteen, outdo, ...          18   \n2057  [fight, endlessly, change, system, atleast, ir...          20   \n426             [love, love, love, left, face, swollen]          12   \n2011                      [fut, ofc, corner, fantastic]           7   \n...                                                 ...         ...   \n1492  [welcome, twitter, ur, service, ur, twitter, c...          11   \n2282                           [clothe, black, noshame]           6   \n2368                                 [pressure, unreal]           7   \n939                      [wing, don, wing, course, boy]           9   \n677                 [ugh, stop, final, finalsweek, ewu]           7   \n\n      char_count  tag_count  hashtag_count  link_count  smiley_count  \\\n1225          51          0              0           1             0   \n418           91          0              0           1             0   \n2057          95          1              1           0             0   \n426           43          0              0           0             0   \n2011          34          1              0           0             0   \n...          ...        ...            ...         ...           ...   \n1492          51          2              0           0             0   \n2282          26          0              1           0             0   \n2368          28          0              0           0             0   \n939           34          0              0           0             0   \n677           32          0              4           1             0   \n\n      mark_count  has_emoji  \n1225           0          0  \n418            0          0  \n2057           0          0  \n426            0          1  \n2011           0          0  \n...          ...        ...  \n1492           0          0  \n2282           0          1  \n2368           0          0  \n939            4          0  \n677            0          0  \n\n[2672 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>tag_count</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>smiley_count</th>\n      <th>mark_count</th>\n      <th>has_emoji</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1225</th>\n      <td>1226</td>\n      <td>1</td>\n      <td>I'm a Victoria Secret model. It's such a secre...</td>\n      <td>victoria secret model it such secret not even ...</td>\n      <td>[victoria, secret, model, secret, victoria, know]</td>\n      <td>10</td>\n      <td>51</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>419</td>\n      <td>1</td>\n      <td>Left my lunch at home. Swansea canteen outdoin...</td>\n      <td>left my lunch at home swansea canteen outdoing...</td>\n      <td>[leave, lunch, home, swansea, canteen, outdo, ...</td>\n      <td>18</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>2061</td>\n      <td>0</td>\n      <td>@tweetkumud we are here to fight endlessly and...</td>\n      <td>we are here to fight endlessly and change the ...</td>\n      <td>[fight, endlessly, change, system, atleast, ir...</td>\n      <td>20</td>\n      <td>95</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>426</th>\n      <td>427</td>\n      <td>1</td>\n      <td>love, love, love how the left side of my face ...</td>\n      <td>love love love how the left side of my face is...</td>\n      <td>[love, love, love, left, face, swollen]</td>\n      <td>12</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2011</th>\n      <td>2015</td>\n      <td>1</td>\n      <td>@MattHDGamer FUT14 ofc, the corners &lt;3___&lt;3 th...</td>\n      <td>fut ofc the corners they where fantastic</td>\n      <td>[fut, ofc, corner, fantastic]</td>\n      <td>7</td>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1492</th>\n      <td>1493</td>\n      <td>1</td>\n      <td>Welcome to twitter @orafa2 myself n @SlimDandy...</td>\n      <td>welcome to twitter myself at ur service as ur ...</td>\n      <td>[welcome, twitter, ur, service, ur, twitter, c...</td>\n      <td>11</td>\n      <td>51</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2282</th>\n      <td>2286</td>\n      <td>0</td>\n      <td>95% of my clothes are black. #noshame :bomb:\\r</td>\n      <td>of my clothes are black noshame</td>\n      <td>[clothe, black, noshame]</td>\n      <td>6</td>\n      <td>26</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2368</th>\n      <td>2372</td>\n      <td>0</td>\n      <td>This pressure is unreal. Keep it up\\r</td>\n      <td>this pressure is unreal keep it up</td>\n      <td>[pressure, unreal]</td>\n      <td>7</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>939</th>\n      <td>940</td>\n      <td>0</td>\n      <td>Wings? I don't have wings! |\"Of course not... ...</td>\n      <td>wings don have wings course not you re boy</td>\n      <td>[wing, don, wing, course, boy]</td>\n      <td>9</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>677</th>\n      <td>678</td>\n      <td>0</td>\n      <td>#finalsweekgotmelike Ugh. Make it stop. #final...</td>\n      <td>ugh make it stop finals finalsweek ewu</td>\n      <td>[ugh, stop, final, finalsweek, ewu]</td>\n      <td>7</td>\n      <td>32</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2672 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "138ca2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_df_with_all_features(df1, df2):\n",
    "    cols_to_add = list(set(df2.columns.tolist()) - set(df1.columns.tolist()))\n",
    "    if 'Tweet index' in cols_to_add:\n",
    "        cols_to_add.remove('Tweet index')\n",
    "    new_df = pd.concat((df1.copy(), df2[cols_to_add]), axis=1)\n",
    "    return new_df\n",
    "\n",
    "# tweet_text_train = new_df_with_all_features(tweet_text_train, df_train)\n",
    "# tweet_text_validation = new_df_with_all_features(tweet_validation_train, df_validation)\n",
    "# tweet_text_test = new_df_with_all_features(tweet_validation_train, df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8dddd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>length</th>\n",
       "      <th>link_count</th>\n",
       "      <th>exclamation_mark_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>smiley_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>[hybridcloud, need, hybrid, monitor, monitor, ...</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>needs hybrid monitoring... monitor Azure, plus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>[bet, outside, today, warm, shirt, short, weat...</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I bet you're gonna be outside today, its soooo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>[ugly, christmas, sweater, woman, sz]</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://t.co/juRDWnOQ8z so Ugly Christmas Sweat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[sick, look, tweet, oh, yeah, sex, lame]</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why am I sick? *looks at other tweets* oh yeah...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>[mama, korean, award, hold, hong, kong, headli...</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MAMA - a Korean awards show that will be held ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>[bibotour, champwithin, founder, dancer, ricky...</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>&amp; founder on w/dancers Ricky &amp; Gracey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>[world, level]</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The whole world can never get me on my level</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>[cup, tea, total, ass, switch, coffee, victory...</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You were my cup of tea, but you were a total a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[mom, picture, grunge, haha, love, momgoal]</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>showed mom this picture of me and &amp; says \"this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>[finnish, sniper, work, fascist, kyiv, nice, guy]</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO, there 'only' are some Finnish working with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>[hour, car, ride, sleep, car, yayaayayayayay, ...</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10 HOUR CAR RIDE AND I CANT SLEEP IN CARS YAYA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>[love, nhs]</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I just love the NHS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>[notcie, eu, foreign, affairs, committee, mep,...</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Foreign Affairs Committee MEPs to discuss Leba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>[event, technology, session, internet, problem...</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Event technology session is having Internet pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>[bombard, country, immigration]</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>They bombarded out country</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Tweet text  length  link_count  \\\n",
       "1731  [hybridcloud, need, hybrid, monitor, monitor, ...     107           1   \n",
       "2615  [bet, outside, today, warm, shirt, short, weat...     128           0   \n",
       "1341              [ugly, christmas, sweater, woman, sz]      85           2   \n",
       "128            [sick, look, tweet, oh, yeah, sex, lame]      60           0   \n",
       "3466  [mama, korean, award, hold, hong, kong, headli...     103           0   \n",
       "1746  [bibotour, champwithin, founder, dancer, ricky...     107           0   \n",
       "936                                      [world, level]      45           0   \n",
       "3150  [cup, tea, total, ass, switch, coffee, victory...     171           0   \n",
       "121         [mom, picture, grunge, haha, love, momgoal]     140           1   \n",
       "3521  [finnish, sniper, work, fascist, kyiv, nice, guy]     101           0   \n",
       "532   [hour, car, ride, sleep, car, yayaayayayayay, ...      89           0   \n",
       "2579                                        [love, nhs]      20           0   \n",
       "2892  [notcie, eu, foreign, affairs, committee, mep,...     105           1   \n",
       "1387  [event, technology, session, internet, problem...      64           0   \n",
       "614                     [bombard, country, immigration]      52           0   \n",
       "\n",
       "      exclamation_mark_count  word_count  has_emoji  tag_count  smiley_count  \\\n",
       "1731                       0          12          0          0             0   \n",
       "2615                       0          15          1          1             0   \n",
       "1341                       0           9          0          0             0   \n",
       "128                        0          11          0          0             0   \n",
       "3466                       0          21          0          0             0   \n",
       "1746                       0           7          0          2             0   \n",
       "936                        0          10          0          0             0   \n",
       "3150                       0          18          1          1             0   \n",
       "121                        3          20          0          1             0   \n",
       "3521                       2          15          0          1             0   \n",
       "532                        1          14          0          1             0   \n",
       "2579                       0           5          0          0             0   \n",
       "2892                       0          11          0          0             0   \n",
       "1387                       0           7          0          0             0   \n",
       "614                        0           4          0          1             0   \n",
       "\n",
       "      hashtag_count                                         clean_text  Label  \n",
       "1731              1  needs hybrid monitoring... monitor Azure, plus...      0  \n",
       "2615              1  I bet you're gonna be outside today, its soooo...      1  \n",
       "1341              0  http://t.co/juRDWnOQ8z so Ugly Christmas Sweat...      0  \n",
       "128               1  Why am I sick? *looks at other tweets* oh yeah...      1  \n",
       "3466              0  MAMA - a Korean awards show that will be held ...      1  \n",
       "1746              4              & founder on w/dancers Ricky & Gracey      0  \n",
       "936               0       The whole world can never get me on my level      1  \n",
       "3150              0  You were my cup of tea, but you were a total a...      0  \n",
       "121               1  showed mom this picture of me and & says \"this...      0  \n",
       "3521              1  NO, there 'only' are some Finnish working with...      1  \n",
       "532               0  10 HOUR CAR RIDE AND I CANT SLEEP IN CARS YAYA...      1  \n",
       "2579              0                                I just love the NHS      1  \n",
       "2892              2  Foreign Affairs Committee MEPs to discuss Leba...      0  \n",
       "1387              1  Event technology session is having Internet pr...      1  \n",
       "614               1                         They bombarded out country      1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet_text_train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d560761",
   "metadata": {
    "id": "2d560761"
   },
   "outputs": [],
   "source": [
    "def join_docs(s):\n",
    "    '''Joins the strings inside the inner list of a nested list'''\n",
    "    return ' '.join(s)\n",
    "\n",
    "df_train['topic_text'] = df_train['lemmas'].apply(join_docs)\n",
    "df_validation['topic_text'] = df_validation['lemmas'].apply(join_docs)\n",
    "df_test['topic_text'] = df_test['lemmas'].apply(join_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "726b260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_text_train = tweet_text_train[tweet_text_train['Tweet text'] != '']\n",
    "# tweet_text_validation = tweet_text_validation[tweet_text_validation['Tweet text'] != '']\n",
    "# tweet_text_test = tweet_text_test[tweet_text_test['Tweet text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30c31df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>length</th>\n",
       "      <th>link_count</th>\n",
       "      <th>exclamation_mark_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>smiley_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>hybridcloud need hybrid monitor monitor azure ...</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>needs hybrid monitoring... monitor Azure, plus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>bet outside today warm shirt short weather funny</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I bet you're gonna be outside today, its soooo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>ugly christmas sweater woman sz</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>http://t.co/juRDWnOQ8z so Ugly Christmas Sweat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>sick look tweet oh yeah sex lame</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why am I sick? *looks at other tweets* oh yeah...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>mama korean award hold hong kong headline amer...</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MAMA - a Korean awards show that will be held ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>bibotour champwithin founder dancer ricky grac...</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>&amp; founder on w/dancers Ricky &amp; Gracey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>world level</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The whole world can never get me on my level</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>cup tea total ass switch coffee victory_hand</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You were my cup of tea, but you were a total a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>mom picture grunge haha love momgoal</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>showed mom this picture of me and &amp; says \"this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>finnish sniper work fascist kyiv nice guy</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO, there 'only' are some Finnish working with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>hour car ride sleep car yayaayayayayay fun bff</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10 HOUR CAR RIDE AND I CANT SLEEP IN CARS YAYA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>love nhs</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I just love the NHS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>notcie eu foreign affairs committee mep discus...</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Foreign Affairs Committee MEPs to discuss Leba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>event technology session internet problem hsc</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Event technology session is having Internet pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>bombard country immigration</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>They bombarded out country</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Tweet text  length  link_count  \\\n",
       "1731  hybridcloud need hybrid monitor monitor azure ...     107           1   \n",
       "2615   bet outside today warm shirt short weather funny     128           0   \n",
       "1341                    ugly christmas sweater woman sz      85           2   \n",
       "128                    sick look tweet oh yeah sex lame      60           0   \n",
       "3466  mama korean award hold hong kong headline amer...     103           0   \n",
       "1746  bibotour champwithin founder dancer ricky grac...     107           0   \n",
       "936                                         world level      45           0   \n",
       "3150       cup tea total ass switch coffee victory_hand     171           0   \n",
       "121                mom picture grunge haha love momgoal     140           1   \n",
       "3521          finnish sniper work fascist kyiv nice guy     101           0   \n",
       "532      hour car ride sleep car yayaayayayayay fun bff      89           0   \n",
       "2579                                           love nhs      20           0   \n",
       "2892  notcie eu foreign affairs committee mep discus...     105           1   \n",
       "1387      event technology session internet problem hsc      64           0   \n",
       "614                         bombard country immigration      52           0   \n",
       "\n",
       "      exclamation_mark_count  word_count  has_emoji  tag_count  smiley_count  \\\n",
       "1731                       0          12          0          0             0   \n",
       "2615                       0          15          1          1             0   \n",
       "1341                       0           9          0          0             0   \n",
       "128                        0          11          0          0             0   \n",
       "3466                       0          21          0          0             0   \n",
       "1746                       0           7          0          2             0   \n",
       "936                        0          10          0          0             0   \n",
       "3150                       0          18          1          1             0   \n",
       "121                        3          20          0          1             0   \n",
       "3521                       2          15          0          1             0   \n",
       "532                        1          14          0          1             0   \n",
       "2579                       0           5          0          0             0   \n",
       "2892                       0          11          0          0             0   \n",
       "1387                       0           7          0          0             0   \n",
       "614                        0           4          0          1             0   \n",
       "\n",
       "      hashtag_count                                         clean_text  Label  \n",
       "1731              1  needs hybrid monitoring... monitor Azure, plus...      0  \n",
       "2615              1  I bet you're gonna be outside today, its soooo...      1  \n",
       "1341              0  http://t.co/juRDWnOQ8z so Ugly Christmas Sweat...      0  \n",
       "128               1  Why am I sick? *looks at other tweets* oh yeah...      1  \n",
       "3466              0  MAMA - a Korean awards show that will be held ...      1  \n",
       "1746              4              & founder on w/dancers Ricky & Gracey      0  \n",
       "936               0       The whole world can never get me on my level      1  \n",
       "3150              0  You were my cup of tea, but you were a total a...      0  \n",
       "121               1  showed mom this picture of me and & says \"this...      0  \n",
       "3521              1  NO, there 'only' are some Finnish working with...      1  \n",
       "532               0  10 HOUR CAR RIDE AND I CANT SLEEP IN CARS YAYA...      1  \n",
       "2579              0                                I just love the NHS      1  \n",
       "2892              2  Foreign Affairs Committee MEPs to discuss Leba...      0  \n",
       "1387              1  Event technology session is having Internet pr...      1  \n",
       "614               1                         They bombarded out country      1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet_text_train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6cb21d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2655, 11)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98404b14",
   "metadata": {
    "id": "98404b14"
   },
   "source": [
    "## Topic modeling baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7127a752",
   "metadata": {
    "id": "7127a752"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    min_df=20,\n",
    "    max_df=0.5,\n",
    ")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    min_df=20,\n",
    "    max_df=0.5,\n",
    ")\n",
    "\n",
    "tweet_text_count_train = count_vectorizer.fit_transform(df_train['clean_text'])\n",
    "tweet_text_count_validation = count_vectorizer.transform(df_validation['clean_text'])\n",
    "tweet_text_count_test = count_vectorizer.transform(df_test['clean_text'])\n",
    "\n",
    "tweet_text_tfidf_train = tfidf_vectorizer.fit_transform(df_train['clean_text'])\n",
    "tweet_text_tfidf_validation = tfidf_vectorizer.transform(df_validation['clean_text'])\n",
    "tweet_text_tfidf_test = tfidf_vectorizer.transform(df_test['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08150dae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "08150dae",
    "outputId": "03815e50-e2ab-48a5-a465-97b6c0e7571e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:2\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_fast._relocate_empty_clusters_sparse'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 2, in where\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "inertia = []\n",
    "range_ = list(range(2, 60))\n",
    "for i in range_:\n",
    "    model = KMeans(i)\n",
    "    model.fit(tweet_text_count_train)\n",
    "    inertia.append(model.inertia_)\n",
    "    \n",
    "plt.plot(range_, inertia)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d0b6e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(26)\n",
    "model.fit(tweet_text_count_train)\n",
    "\n",
    "kmeans_count_labels_train = model.predict(tweet_text_count_train)\n",
    "kmeans_count_labels_validation = model.predict(tweet_text_count_validation)\n",
    "kmeans_count_labels_test = model.predict(tweet_text_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f11fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "fe5f11fc",
    "outputId": "8000ac3b-1375-4905-8bca-179acf2ac778",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for i in range_:\n",
    "    model = KMeans(i)\n",
    "    model.fit(tweet_text_tfidf_train)\n",
    "    inertia.append(model.inertia_)\n",
    "plt.plot(range_, inertia)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72eb5b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(17)\n",
    "model.fit(tweet_text_count_train)\n",
    "\n",
    "kmeans_tfidf_labels_train = model.predict(tweet_text_count_train)\n",
    "kmeans_tfidf_labels_validation = model.predict(tweet_text_count_validation)\n",
    "kmeans_tfidf_labels_test = model.predict(tweet_text_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "9ed966da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.10.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting plotly>=4.7.0\n",
      "  Using cached plotly-5.8.0-py2.py3-none-any.whl (15.2 MB)\n",
      "Collecting pyyaml<6.0\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-win_amd64.whl (213 kB)\n",
      "Collecting hdbscan>=0.8.28\n",
      "  Using cached hdbscan-0.8.28.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap_learn-0.5.3-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (4.64.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (1.22.3)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence_transformers-2.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.19.2)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.6.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.12.0-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: nltk in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.4)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.7-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (58.1.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Collecting numpy>=1.20.0\n",
      "  Using cached numpy-1.21.6-cp39-cp39-win_amd64.whl (14.0 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.4.24)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.12.1)\n",
      "Requirement already satisfied: click in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml): started\n",
      "  Building wheel for hdbscan (pyproject.toml): finished with status 'error'\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for hdbscan (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [40 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  creating build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
      "  C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-build-env-t_era065\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-install-dgxdvea9\\hdbscan_0b8aa2e9b8a041a299e2cb86b789e644\\hdbscan\\_hdbscan_tree.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
      "  C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-build-env-t_era065\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-install-dgxdvea9\\hdbscan_0b8aa2e9b8a041a299e2cb86b789e644\\hdbscan\\_hdbscan_linkage.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
      "  C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-build-env-t_era065\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-install-dgxdvea9\\hdbscan_0b8aa2e9b8a041a299e2cb86b789e644\\hdbscan\\_hdbscan_boruvka.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
      "  C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-build-env-t_era065\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-install-dgxdvea9\\hdbscan_0b8aa2e9b8a041a299e2cb86b789e644\\hdbscan\\_hdbscan_reachability.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
      "  C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-build-env-t_era065\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-install-dgxdvea9\\hdbscan_0b8aa2e9b8a041a299e2cb86b789e644\\hdbscan\\_prediction_utils.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
      "  C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-build-env-t_era065\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Ivan\\AppData\\Local\\Temp\\pip-install-dgxdvea9\\hdbscan_0b8aa2e9b8a041a299e2cb86b789e644\\hdbscan\\dist_metrics.pxd\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "#!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fdbed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_validation.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0vdf4uM_F2cg",
   "metadata": {
    "id": "0vdf4uM_F2cg"
   },
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "NRdFJDRVF2m2",
   "metadata": {
    "id": "NRdFJDRVF2m2"
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(min_topic_size=50)\n",
    "topics, probs = topic_model.fit_transform(df_train['topic_text'])\n",
    "\n",
    "pred_train = topic_model.transform(df_train['topic_text'])\n",
    "pred_validation = topic_model.transform(df_validation['topic_text'])\n",
    "pred_test = topic_model.transform(df_test['topic_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "pWXDBfusPI5v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWXDBfusPI5v",
    "outputId": "9cf44cfb-a7d0-41d1-b1d3-898c75de400e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('christmas', 0.12289697955789392),\n ('look', 0.03731473284405396),\n ('gift', 0.03379362592458907),\n ('girl', 0.030105766651632084),\n ('song', 0.029487034269259093),\n ('wear', 0.028594606551575374),\n ('day', 0.026087743525930657),\n ('hair', 0.02563227804746289),\n ('selfie', 0.025490263223589306),\n ('music', 0.025293063208118933)]"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "W9IK-LziICsL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "W9IK-LziICsL",
    "outputId": "76f344ac-14f1-47a9-fc10-eef060d23b5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Topic  Count                             Name\n0     -1   1573           -1_love_like_year_time\n1      0    274       0_christmas_look_gift_girl\n2      1    177       1_police_white_racism_rape\n3      2    164             2_game_play_win_good\n4      3    116            3_day_week_today_work\n5      4    114        4_sleep_wake_morning_hour\n6      5     93           5_love_funny_dad_child\n7      6     89       6_school_paper_final_study\n8      7     72  7_tweet_twitter_follow_facebook",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Count</th>\n      <th>Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>1573</td>\n      <td>-1_love_like_year_time</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>274</td>\n      <td>0_christmas_look_gift_girl</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>177</td>\n      <td>1_police_white_racism_rape</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>164</td>\n      <td>2_game_play_win_good</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>116</td>\n      <td>3_day_week_today_work</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>114</td>\n      <td>4_sleep_wake_morning_hour</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>93</td>\n      <td>5_love_funny_dad_child</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>89</td>\n      <td>6_school_paper_final_study</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>72</td>\n      <td>7_tweet_twitter_follow_facebook</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "df_train['topic'] = pred_train[0]\n",
    "df_validation['topic'] = pred_validation[0]\n",
    "df_test['topic'] = pred_test[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8e76a786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n0            1226      1  I'm a Victoria Secret model. It's such a secre...   \n1             419      1  Left my lunch at home. Swansea canteen outdoin...   \n2            2061      0  @tweetkumud we are here to fight endlessly and...   \n3             427      1  love, love, love how the left side of my face ...   \n4            2015      1  @MattHDGamer FUT14 ofc, the corners <3___<3 th...   \n...           ...    ...                                                ...   \n2667         1493      1  Welcome to twitter @orafa2 myself n @SlimDandy...   \n2668         2286      0     95% of my clothes are black. #noshame :bomb:\\r   \n2669         2372      0              This pressure is unreal. Keep it up\\r   \n2670          940      0  Wings? I don't have wings! |\"Of course not... ...   \n2671          678      0  #finalsweekgotmelike Ugh. Make it stop. #final...   \n\n                                             clean_text  \\\n0     victoria secret model it such secret not even ...   \n1     left my lunch at home swansea canteen outdoing...   \n2     we are here to fight endlessly and change the ...   \n3     love love love how the left side of my face is...   \n4              fut ofc the corners they where fantastic   \n...                                                 ...   \n2667  welcome to twitter myself at ur service as ur ...   \n2668                    of my clothes are black noshame   \n2669                 this pressure is unreal keep it up   \n2670         wings don have wings course not you re boy   \n2671             ugh make it stop finals finalsweek ewu   \n\n                                                 lemmas  word_count  \\\n0     [victoria, secret, model, secret, victoria, know]          10   \n1     [leave, lunch, home, swansea, canteen, outdo, ...          18   \n2     [fight, endlessly, change, system, atleast, ir...          20   \n3               [love, love, love, left, face, swollen]          12   \n4                         [fut, ofc, corner, fantastic]           7   \n...                                                 ...         ...   \n2667  [welcome, twitter, ur, service, ur, twitter, c...          11   \n2668                           [clothe, black, noshame]           6   \n2669                                 [pressure, unreal]           7   \n2670                     [wing, don, wing, course, boy]           9   \n2671                [ugh, stop, final, finalsweek, ewu]           7   \n\n      char_count  tag_count  hashtag_count  link_count  ...  17_topic  \\\n0             51          0              0           1  ...         0   \n1             91          0              0           1  ...         1   \n2             95          1              1           0  ...         0   \n3             43          0              0           0  ...         0   \n4             34          1              0           0  ...         0   \n...          ...        ...            ...         ...  ...       ...   \n2667          51          2              0           0  ...         0   \n2668          26          0              1           0  ...         0   \n2669          28          0              0           0  ...         0   \n2670          34          0              0           0  ...         0   \n2671          32          0              4           1  ...         0   \n\n      18_topic  19_topic 20_topic  21_topic  22_topic  23_topic  24_topic  \\\n0            0         0        0         0         0         0         0   \n1            0         0        0         0         0         0         0   \n2            0         0        0         0         0         0         0   \n3            0         0        0         0         0         0         0   \n4            0         0        0         0         0         0         0   \n...        ...       ...      ...       ...       ...       ...       ...   \n2667         0         0        0         0         0         0         0   \n2668         0         0        0         0         0         0         0   \n2669         0         0        0         0         0         0         0   \n2670         0         0        0         0         0         0         0   \n2671         0         0        0         0         0         0         0   \n\n      25_topic  26_topic  \n0            0         0  \n1            0         0  \n2            0         0  \n3            0         0  \n4            0         0  \n...        ...       ...  \n2667         0         0  \n2668         0         0  \n2669         0         0  \n2670         0         0  \n2671         0         0  \n\n[2672 rows x 345 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>tag_count</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>...</th>\n      <th>17_topic</th>\n      <th>18_topic</th>\n      <th>19_topic</th>\n      <th>20_topic</th>\n      <th>21_topic</th>\n      <th>22_topic</th>\n      <th>23_topic</th>\n      <th>24_topic</th>\n      <th>25_topic</th>\n      <th>26_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1226</td>\n      <td>1</td>\n      <td>I'm a Victoria Secret model. It's such a secre...</td>\n      <td>victoria secret model it such secret not even ...</td>\n      <td>[victoria, secret, model, secret, victoria, know]</td>\n      <td>10</td>\n      <td>51</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>419</td>\n      <td>1</td>\n      <td>Left my lunch at home. Swansea canteen outdoin...</td>\n      <td>left my lunch at home swansea canteen outdoing...</td>\n      <td>[leave, lunch, home, swansea, canteen, outdo, ...</td>\n      <td>18</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2061</td>\n      <td>0</td>\n      <td>@tweetkumud we are here to fight endlessly and...</td>\n      <td>we are here to fight endlessly and change the ...</td>\n      <td>[fight, endlessly, change, system, atleast, ir...</td>\n      <td>20</td>\n      <td>95</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>427</td>\n      <td>1</td>\n      <td>love, love, love how the left side of my face ...</td>\n      <td>love love love how the left side of my face is...</td>\n      <td>[love, love, love, left, face, swollen]</td>\n      <td>12</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015</td>\n      <td>1</td>\n      <td>@MattHDGamer FUT14 ofc, the corners &lt;3___&lt;3 th...</td>\n      <td>fut ofc the corners they where fantastic</td>\n      <td>[fut, ofc, corner, fantastic]</td>\n      <td>7</td>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>1493</td>\n      <td>1</td>\n      <td>Welcome to twitter @orafa2 myself n @SlimDandy...</td>\n      <td>welcome to twitter myself at ur service as ur ...</td>\n      <td>[welcome, twitter, ur, service, ur, twitter, c...</td>\n      <td>11</td>\n      <td>51</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>2286</td>\n      <td>0</td>\n      <td>95% of my clothes are black. #noshame :bomb:\\r</td>\n      <td>of my clothes are black noshame</td>\n      <td>[clothe, black, noshame]</td>\n      <td>6</td>\n      <td>26</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>2372</td>\n      <td>0</td>\n      <td>This pressure is unreal. Keep it up\\r</td>\n      <td>this pressure is unreal keep it up</td>\n      <td>[pressure, unreal]</td>\n      <td>7</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>940</td>\n      <td>0</td>\n      <td>Wings? I don't have wings! |\"Of course not... ...</td>\n      <td>wings don have wings course not you re boy</td>\n      <td>[wing, don, wing, course, boy]</td>\n      <td>9</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2671</th>\n      <td>678</td>\n      <td>0</td>\n      <td>#finalsweekgotmelike Ugh. Make it stop. #final...</td>\n      <td>ugh make it stop finals finalsweek ewu</td>\n      <td>[ugh, stop, final, finalsweek, ewu]</td>\n      <td>7</td>\n      <td>32</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2672 rows × 345 columns</p>\n</div>"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad485a",
   "metadata": {
    "id": "6cad485a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0d38321",
   "metadata": {
    "id": "a0d38321",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from podium import Vocab, Field, LabelField\n",
    "from podium.datasets import TabularDataset\n",
    "from podium.vectorizers import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc4e8fed",
   "metadata": {
    "id": "cc4e8fed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_embedding_train = df_train[['topic_text', 'Label']]\n",
    "tweet_embedding_validation = df_validation[['topic_text', 'Label']]\n",
    "tweet_embedding_test = df_test[['topic_text', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97c7341c",
   "metadata": {
    "id": "97c7341c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_embedding_train.reset_index(drop=True, inplace=True)\n",
    "tweet_embedding_validation.reset_index(drop=True, inplace=True)\n",
    "tweet_embedding_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c409d8cc",
   "metadata": {
    "id": "c409d8cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10_000\n",
    "vocab = Vocab(max_size=max_vocab_size, min_freq=20)\n",
    "\n",
    "TWEET = Field('text', numericalizer=vocab)\n",
    "LABEL = LabelField('Label')\n",
    "\n",
    "fields = [TWEET, LABEL]\n",
    "\n",
    "train = TabularDataset.from_pandas(df_train[['topic_text', 'Label']], fields)\n",
    "validation = TabularDataset.from_pandas(df_validation[['topic_text', 'Label']], fields)\n",
    "test = TabularDataset.from_pandas(df_test[['topic_text', 'Label']], fields)\n",
    "train.finalize_fields()\n",
    "\n",
    "glove = GloVe()\n",
    "embeddings = glove.load_vocab(vocab)\n",
    "\n",
    "train_batch = train.batch(add_padding=True)\n",
    "validation_batch = validation.batch(add_padding=True)\n",
    "test_batch = test.batch(add_padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e707fbc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e707fbc3",
    "outputId": "68f44c29-513e-40cf-a62d-f02ee2cb168c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0,  0,  0,  0,  0, 11,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1],\n       [67,  0, 89,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1],\n       [ 0,  0, 79,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1],\n       [ 2,  2,  2,  0, 80,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1],\n       [ 0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch['text'].astype(int)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df0d0eed",
   "metadata": {
    "id": "df0d0eed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_train = embeddings[train_batch['text'].astype(int)]\n",
    "tweet_validation = embeddings[validation_batch['text'].astype(int)]\n",
    "tweet_test = embeddings[test_batch['text'].astype(int)]\n",
    "\n",
    "# Mean\n",
    "tweet_train_mean = tweet_train.mean(axis=1)\n",
    "tweet_validation_mean = tweet_validation.mean(axis=1)\n",
    "tweet_test_mean = tweet_test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf850683",
   "metadata": {
    "id": "cf850683",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_train_mean_df = pd.DataFrame(tweet_train_mean)\n",
    "df_train = pd.merge(df_train, embeddings_train_mean_df, left_index=True, right_index=True)\n",
    "\n",
    "embeddings_validation_mean_df = pd.DataFrame(tweet_validation_mean)\n",
    "df_validation = pd.merge(df_validation, embeddings_validation_mean_df, left_index=True, right_index=True)\n",
    "\n",
    "embeddings_test_mean_df = pd.DataFrame(tweet_test_mean)\n",
    "df_test = pd.merge(df_test, embeddings_test_mean_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "470341bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "470341bb",
    "outputId": "e580e876-4810-4056-a94c-62109a8a1e33",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n0            1226      1  I'm a Victoria Secret model. It's such a secre...   \n1             419      1  Left my lunch at home. Swansea canteen outdoin...   \n2            2061      0  @tweetkumud we are here to fight endlessly and...   \n3             427      1  love, love, love how the left side of my face ...   \n4            2015      1  @MattHDGamer FUT14 ofc, the corners <3___<3 th...   \n...           ...    ...                                                ...   \n2667         1493      1  Welcome to twitter @orafa2 myself n @SlimDandy...   \n2668         2286      0     95% of my clothes are black. #noshame :bomb:\\r   \n2669         2372      0              This pressure is unreal. Keep it up\\r   \n2670          940      0  Wings? I don't have wings! |\"Of course not... ...   \n2671          678      0  #finalsweekgotmelike Ugh. Make it stop. #final...   \n\n                                             clean_text  \\\n0     victoria secret model it such secret not even ...   \n1     left my lunch at home swansea canteen outdoing...   \n2     we are here to fight endlessly and change the ...   \n3     love love love how the left side of my face is...   \n4              fut ofc the corners they where fantastic   \n...                                                 ...   \n2667  welcome to twitter myself at ur service as ur ...   \n2668                    of my clothes are black noshame   \n2669                 this pressure is unreal keep it up   \n2670         wings don have wings course not you re boy   \n2671             ugh make it stop finals finalsweek ewu   \n\n                                                 lemmas  word_count  \\\n0     [victoria, secret, model, secret, victoria, know]          10   \n1     [leave, lunch, home, swansea, canteen, outdo, ...          18   \n2     [fight, endlessly, change, system, atleast, ir...          20   \n3               [love, love, love, left, face, swollen]          12   \n4                         [fut, ofc, corner, fantastic]           7   \n...                                                 ...         ...   \n2667  [welcome, twitter, ur, service, ur, twitter, c...          11   \n2668                           [clothe, black, noshame]           6   \n2669                                 [pressure, unreal]           7   \n2670                     [wing, don, wing, course, boy]           9   \n2671                [ugh, stop, final, finalsweek, ewu]           7   \n\n      char_count  tag_count  hashtag_count  link_count  ...       290  \\\n0             51          0              0           1  ... -0.734994   \n1             91          0              0           1  ... -0.743841   \n2             95          1              1           0  ... -0.748988   \n3             43          0              0           0  ... -0.716877   \n4             34          1              0           0  ... -0.745567   \n...          ...        ...            ...         ...  ...       ...   \n2667          51          2              0           0  ... -0.703266   \n2668          26          0              1           0  ... -0.741880   \n2669          28          0              0           0  ... -0.738192   \n2670          34          0              0           0  ... -0.739118   \n2671          32          0              4           1  ... -0.728200   \n\n           291       292       293       294       295       296       297  \\\n0    -0.656253 -1.247315 -0.715902 -0.472768  0.245063 -1.055144 -0.341575   \n1    -0.647377 -1.135379 -0.688410 -0.585451  0.292726 -0.978633 -0.313849   \n2    -0.654107 -1.158139 -0.682845 -0.588523  0.297747 -1.003470 -0.309303   \n3    -0.623747 -1.275515 -0.717432 -0.334869  0.194307 -1.042801 -0.342240   \n4    -0.661025 -1.299867 -0.733037 -0.440542  0.246756 -1.088922 -0.354721   \n...        ...       ...       ...       ...       ...       ...       ...   \n2667 -0.638952 -1.253360 -0.704631 -0.435985  0.229188 -1.043825 -0.335427   \n2668 -0.660962 -1.329347 -0.741173 -0.402545  0.233630 -1.107008 -0.364313   \n2669 -0.660898 -1.358828 -0.749309 -0.364549  0.220503 -1.125094 -0.373904   \n2670 -0.657139 -1.282313 -0.720612 -0.445044  0.245276 -1.075079 -0.345795   \n2671 -0.646012 -1.287697 -0.713609 -0.380788  0.231520 -1.068907 -0.361190   \n\n           298       299  \n0    -1.258775 -0.501747  \n1    -1.094880 -0.424406  \n2    -1.111962 -0.444621  \n3    -1.329015 -0.548489  \n4    -1.329997 -0.539491  \n...        ...       ...  \n2667 -1.280024 -0.526104  \n2668 -1.379796 -0.560553  \n2669 -1.429596 -0.581615  \n2670 -1.310360 -0.521672  \n2671 -1.338058 -0.542515  \n\n[2672 rows x 315 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>tag_count</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1226</td>\n      <td>1</td>\n      <td>I'm a Victoria Secret model. It's such a secre...</td>\n      <td>victoria secret model it such secret not even ...</td>\n      <td>[victoria, secret, model, secret, victoria, know]</td>\n      <td>10</td>\n      <td>51</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.734994</td>\n      <td>-0.656253</td>\n      <td>-1.247315</td>\n      <td>-0.715902</td>\n      <td>-0.472768</td>\n      <td>0.245063</td>\n      <td>-1.055144</td>\n      <td>-0.341575</td>\n      <td>-1.258775</td>\n      <td>-0.501747</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>419</td>\n      <td>1</td>\n      <td>Left my lunch at home. Swansea canteen outdoin...</td>\n      <td>left my lunch at home swansea canteen outdoing...</td>\n      <td>[leave, lunch, home, swansea, canteen, outdo, ...</td>\n      <td>18</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.743841</td>\n      <td>-0.647377</td>\n      <td>-1.135379</td>\n      <td>-0.688410</td>\n      <td>-0.585451</td>\n      <td>0.292726</td>\n      <td>-0.978633</td>\n      <td>-0.313849</td>\n      <td>-1.094880</td>\n      <td>-0.424406</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2061</td>\n      <td>0</td>\n      <td>@tweetkumud we are here to fight endlessly and...</td>\n      <td>we are here to fight endlessly and change the ...</td>\n      <td>[fight, endlessly, change, system, atleast, ir...</td>\n      <td>20</td>\n      <td>95</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.748988</td>\n      <td>-0.654107</td>\n      <td>-1.158139</td>\n      <td>-0.682845</td>\n      <td>-0.588523</td>\n      <td>0.297747</td>\n      <td>-1.003470</td>\n      <td>-0.309303</td>\n      <td>-1.111962</td>\n      <td>-0.444621</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>427</td>\n      <td>1</td>\n      <td>love, love, love how the left side of my face ...</td>\n      <td>love love love how the left side of my face is...</td>\n      <td>[love, love, love, left, face, swollen]</td>\n      <td>12</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.716877</td>\n      <td>-0.623747</td>\n      <td>-1.275515</td>\n      <td>-0.717432</td>\n      <td>-0.334869</td>\n      <td>0.194307</td>\n      <td>-1.042801</td>\n      <td>-0.342240</td>\n      <td>-1.329015</td>\n      <td>-0.548489</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015</td>\n      <td>1</td>\n      <td>@MattHDGamer FUT14 ofc, the corners &lt;3___&lt;3 th...</td>\n      <td>fut ofc the corners they where fantastic</td>\n      <td>[fut, ofc, corner, fantastic]</td>\n      <td>7</td>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.745567</td>\n      <td>-0.661025</td>\n      <td>-1.299867</td>\n      <td>-0.733037</td>\n      <td>-0.440542</td>\n      <td>0.246756</td>\n      <td>-1.088922</td>\n      <td>-0.354721</td>\n      <td>-1.329997</td>\n      <td>-0.539491</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>1493</td>\n      <td>1</td>\n      <td>Welcome to twitter @orafa2 myself n @SlimDandy...</td>\n      <td>welcome to twitter myself at ur service as ur ...</td>\n      <td>[welcome, twitter, ur, service, ur, twitter, c...</td>\n      <td>11</td>\n      <td>51</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.703266</td>\n      <td>-0.638952</td>\n      <td>-1.253360</td>\n      <td>-0.704631</td>\n      <td>-0.435985</td>\n      <td>0.229188</td>\n      <td>-1.043825</td>\n      <td>-0.335427</td>\n      <td>-1.280024</td>\n      <td>-0.526104</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>2286</td>\n      <td>0</td>\n      <td>95% of my clothes are black. #noshame :bomb:\\r</td>\n      <td>of my clothes are black noshame</td>\n      <td>[clothe, black, noshame]</td>\n      <td>6</td>\n      <td>26</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.741880</td>\n      <td>-0.660962</td>\n      <td>-1.329347</td>\n      <td>-0.741173</td>\n      <td>-0.402545</td>\n      <td>0.233630</td>\n      <td>-1.107008</td>\n      <td>-0.364313</td>\n      <td>-1.379796</td>\n      <td>-0.560553</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>2372</td>\n      <td>0</td>\n      <td>This pressure is unreal. Keep it up\\r</td>\n      <td>this pressure is unreal keep it up</td>\n      <td>[pressure, unreal]</td>\n      <td>7</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.738192</td>\n      <td>-0.660898</td>\n      <td>-1.358828</td>\n      <td>-0.749309</td>\n      <td>-0.364549</td>\n      <td>0.220503</td>\n      <td>-1.125094</td>\n      <td>-0.373904</td>\n      <td>-1.429596</td>\n      <td>-0.581615</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>940</td>\n      <td>0</td>\n      <td>Wings? I don't have wings! |\"Of course not... ...</td>\n      <td>wings don have wings course not you re boy</td>\n      <td>[wing, don, wing, course, boy]</td>\n      <td>9</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.739118</td>\n      <td>-0.657139</td>\n      <td>-1.282313</td>\n      <td>-0.720612</td>\n      <td>-0.445044</td>\n      <td>0.245276</td>\n      <td>-1.075079</td>\n      <td>-0.345795</td>\n      <td>-1.310360</td>\n      <td>-0.521672</td>\n    </tr>\n    <tr>\n      <th>2671</th>\n      <td>678</td>\n      <td>0</td>\n      <td>#finalsweekgotmelike Ugh. Make it stop. #final...</td>\n      <td>ugh make it stop finals finalsweek ewu</td>\n      <td>[ugh, stop, final, finalsweek, ewu]</td>\n      <td>7</td>\n      <td>32</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.728200</td>\n      <td>-0.646012</td>\n      <td>-1.287697</td>\n      <td>-0.713609</td>\n      <td>-0.380788</td>\n      <td>0.231520</td>\n      <td>-1.068907</td>\n      <td>-0.361190</td>\n      <td>-1.338058</td>\n      <td>-0.542515</td>\n    </tr>\n  </tbody>\n</table>\n<p>2672 rows × 315 columns</p>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c44fb1",
   "metadata": {
    "id": "a8c44fb1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Features\n",
    "## Broj neg rijeci\n",
    "## Broj poz rijeci\n",
    "## Omjer\n",
    "## Udaljenost izmedu poz i neg rijeci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb9631",
   "metadata": {
    "id": "d9cb9631",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Potrebno dodatno preprocesat da samo rjeci ostanu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1208a12f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1208a12f",
    "outputId": "aa68c64c-f083-4235-df46-4027f5ef3168"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Marino\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon') # if error run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af3801bc",
   "metadata": {
    "id": "af3801bc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import copy\n",
    "\n",
    "def pos_neg_words(df, limit):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    neg_words = []\n",
    "    neg_word_count = []\n",
    "    pos_words = []\n",
    "    pos_word_count = []\n",
    "    for index, row in df.iterrows():\n",
    "        lemmas = []\n",
    "        if len(row['topic_text']) > 0:\n",
    "            doc = nlp(row['topic_text'])\n",
    "            for token in doc:\n",
    "                lemmas.append(token.lemma_)\n",
    "\n",
    "            current_pos = []\n",
    "            current_neut = []\n",
    "            current_neg = []\n",
    "            for word in lemmas:\n",
    "                if (sid.polarity_scores(word)['compound']) >= limit:\n",
    "                    current_pos.append(word)\n",
    "                elif (sid.polarity_scores(word)['compound']) <= -limit:\n",
    "                    current_neg.append(word)\n",
    "                else:\n",
    "                    current_neut.append(word)\n",
    "\n",
    "            neg_words.append(copy.copy(current_neg))\n",
    "            neg_word_count.append(copy.copy(len(current_neg)))\n",
    "            pos_words.append(copy.copy(current_pos))\n",
    "            pos_word_count.append(copy.copy(len(current_pos)))\n",
    "        else:\n",
    "            neg_words.append([])\n",
    "            neg_word_count.append(0)\n",
    "            pos_words.append([])\n",
    "            pos_word_count.append(0)\n",
    "    return neg_words, neg_word_count, pos_words, pos_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca3cbc8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ca3cbc8a",
    "outputId": "94d87f4c-9877-4f67-db5f-b2efebb09927",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n0            1226      1  I'm a Victoria Secret model. It's such a secre...   \n1             419      1  Left my lunch at home. Swansea canteen outdoin...   \n2            2061      0  @tweetkumud we are here to fight endlessly and...   \n3             427      1  love, love, love how the left side of my face ...   \n4            2015      1  @MattHDGamer FUT14 ofc, the corners <3___<3 th...   \n...           ...    ...                                                ...   \n2667         1493      1  Welcome to twitter @orafa2 myself n @SlimDandy...   \n2668         2286      0     95% of my clothes are black. #noshame :bomb:\\r   \n2669         2372      0              This pressure is unreal. Keep it up\\r   \n2670          940      0  Wings? I don't have wings! |\"Of course not... ...   \n2671          678      0  #finalsweekgotmelike Ugh. Make it stop. #final...   \n\n                                             clean_text  \\\n0     victoria secret model it such secret not even ...   \n1     left my lunch at home swansea canteen outdoing...   \n2     we are here to fight endlessly and change the ...   \n3     love love love how the left side of my face is...   \n4              fut ofc the corners they where fantastic   \n...                                                 ...   \n2667  welcome to twitter myself at ur service as ur ...   \n2668                    of my clothes are black noshame   \n2669                 this pressure is unreal keep it up   \n2670         wings don have wings course not you re boy   \n2671             ugh make it stop finals finalsweek ewu   \n\n                                                 lemmas  word_count  \\\n0     [victoria, secret, model, secret, victoria, know]          10   \n1     [leave, lunch, home, swansea, canteen, outdo, ...          18   \n2     [fight, endlessly, change, system, atleast, ir...          20   \n3               [love, love, love, left, face, swollen]          12   \n4                         [fut, ofc, corner, fantastic]           7   \n...                                                 ...         ...   \n2667  [welcome, twitter, ur, service, ur, twitter, c...          11   \n2668                           [clothe, black, noshame]           6   \n2669                                 [pressure, unreal]           7   \n2670                     [wing, don, wing, course, boy]           9   \n2671                [ugh, stop, final, finalsweek, ewu]           7   \n\n      char_count  tag_count  hashtag_count  link_count  ...       290  \\\n0             51          0              0           1  ... -0.734994   \n1             91          0              0           1  ... -0.743841   \n2             95          1              1           0  ... -0.748988   \n3             43          0              0           0  ... -0.716877   \n4             34          1              0           0  ... -0.745567   \n...          ...        ...            ...         ...  ...       ...   \n2667          51          2              0           0  ... -0.703266   \n2668          26          0              1           0  ... -0.741880   \n2669          28          0              0           0  ... -0.738192   \n2670          34          0              0           0  ... -0.739118   \n2671          32          0              4           1  ... -0.728200   \n\n           291       292       293       294       295       296       297  \\\n0    -0.656253 -1.247315 -0.715902 -0.472768  0.245063 -1.055144 -0.341575   \n1    -0.647377 -1.135379 -0.688410 -0.585451  0.292726 -0.978633 -0.313849   \n2    -0.654107 -1.158139 -0.682845 -0.588523  0.297747 -1.003470 -0.309303   \n3    -0.623747 -1.275515 -0.717432 -0.334869  0.194307 -1.042801 -0.342240   \n4    -0.661025 -1.299867 -0.733037 -0.440542  0.246756 -1.088922 -0.354721   \n...        ...       ...       ...       ...       ...       ...       ...   \n2667 -0.638952 -1.253360 -0.704631 -0.435985  0.229188 -1.043825 -0.335427   \n2668 -0.660962 -1.329347 -0.741173 -0.402545  0.233630 -1.107008 -0.364313   \n2669 -0.660898 -1.358828 -0.749309 -0.364549  0.220503 -1.125094 -0.373904   \n2670 -0.657139 -1.282313 -0.720612 -0.445044  0.245276 -1.075079 -0.345795   \n2671 -0.646012 -1.287697 -0.713609 -0.380788  0.231520 -1.068907 -0.361190   \n\n           298       299  \n0    -1.258775 -0.501747  \n1    -1.094880 -0.424406  \n2    -1.111962 -0.444621  \n3    -1.329015 -0.548489  \n4    -1.329997 -0.539491  \n...        ...       ...  \n2667 -1.280024 -0.526104  \n2668 -1.379796 -0.560553  \n2669 -1.429596 -0.581615  \n2670 -1.310360 -0.521672  \n2671 -1.338058 -0.542515  \n\n[2672 rows x 315 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>tag_count</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1226</td>\n      <td>1</td>\n      <td>I'm a Victoria Secret model. It's such a secre...</td>\n      <td>victoria secret model it such secret not even ...</td>\n      <td>[victoria, secret, model, secret, victoria, know]</td>\n      <td>10</td>\n      <td>51</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.734994</td>\n      <td>-0.656253</td>\n      <td>-1.247315</td>\n      <td>-0.715902</td>\n      <td>-0.472768</td>\n      <td>0.245063</td>\n      <td>-1.055144</td>\n      <td>-0.341575</td>\n      <td>-1.258775</td>\n      <td>-0.501747</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>419</td>\n      <td>1</td>\n      <td>Left my lunch at home. Swansea canteen outdoin...</td>\n      <td>left my lunch at home swansea canteen outdoing...</td>\n      <td>[leave, lunch, home, swansea, canteen, outdo, ...</td>\n      <td>18</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.743841</td>\n      <td>-0.647377</td>\n      <td>-1.135379</td>\n      <td>-0.688410</td>\n      <td>-0.585451</td>\n      <td>0.292726</td>\n      <td>-0.978633</td>\n      <td>-0.313849</td>\n      <td>-1.094880</td>\n      <td>-0.424406</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2061</td>\n      <td>0</td>\n      <td>@tweetkumud we are here to fight endlessly and...</td>\n      <td>we are here to fight endlessly and change the ...</td>\n      <td>[fight, endlessly, change, system, atleast, ir...</td>\n      <td>20</td>\n      <td>95</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.748988</td>\n      <td>-0.654107</td>\n      <td>-1.158139</td>\n      <td>-0.682845</td>\n      <td>-0.588523</td>\n      <td>0.297747</td>\n      <td>-1.003470</td>\n      <td>-0.309303</td>\n      <td>-1.111962</td>\n      <td>-0.444621</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>427</td>\n      <td>1</td>\n      <td>love, love, love how the left side of my face ...</td>\n      <td>love love love how the left side of my face is...</td>\n      <td>[love, love, love, left, face, swollen]</td>\n      <td>12</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.716877</td>\n      <td>-0.623747</td>\n      <td>-1.275515</td>\n      <td>-0.717432</td>\n      <td>-0.334869</td>\n      <td>0.194307</td>\n      <td>-1.042801</td>\n      <td>-0.342240</td>\n      <td>-1.329015</td>\n      <td>-0.548489</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015</td>\n      <td>1</td>\n      <td>@MattHDGamer FUT14 ofc, the corners &lt;3___&lt;3 th...</td>\n      <td>fut ofc the corners they where fantastic</td>\n      <td>[fut, ofc, corner, fantastic]</td>\n      <td>7</td>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.745567</td>\n      <td>-0.661025</td>\n      <td>-1.299867</td>\n      <td>-0.733037</td>\n      <td>-0.440542</td>\n      <td>0.246756</td>\n      <td>-1.088922</td>\n      <td>-0.354721</td>\n      <td>-1.329997</td>\n      <td>-0.539491</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>1493</td>\n      <td>1</td>\n      <td>Welcome to twitter @orafa2 myself n @SlimDandy...</td>\n      <td>welcome to twitter myself at ur service as ur ...</td>\n      <td>[welcome, twitter, ur, service, ur, twitter, c...</td>\n      <td>11</td>\n      <td>51</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.703266</td>\n      <td>-0.638952</td>\n      <td>-1.253360</td>\n      <td>-0.704631</td>\n      <td>-0.435985</td>\n      <td>0.229188</td>\n      <td>-1.043825</td>\n      <td>-0.335427</td>\n      <td>-1.280024</td>\n      <td>-0.526104</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>2286</td>\n      <td>0</td>\n      <td>95% of my clothes are black. #noshame :bomb:\\r</td>\n      <td>of my clothes are black noshame</td>\n      <td>[clothe, black, noshame]</td>\n      <td>6</td>\n      <td>26</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.741880</td>\n      <td>-0.660962</td>\n      <td>-1.329347</td>\n      <td>-0.741173</td>\n      <td>-0.402545</td>\n      <td>0.233630</td>\n      <td>-1.107008</td>\n      <td>-0.364313</td>\n      <td>-1.379796</td>\n      <td>-0.560553</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>2372</td>\n      <td>0</td>\n      <td>This pressure is unreal. Keep it up\\r</td>\n      <td>this pressure is unreal keep it up</td>\n      <td>[pressure, unreal]</td>\n      <td>7</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.738192</td>\n      <td>-0.660898</td>\n      <td>-1.358828</td>\n      <td>-0.749309</td>\n      <td>-0.364549</td>\n      <td>0.220503</td>\n      <td>-1.125094</td>\n      <td>-0.373904</td>\n      <td>-1.429596</td>\n      <td>-0.581615</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>940</td>\n      <td>0</td>\n      <td>Wings? I don't have wings! |\"Of course not... ...</td>\n      <td>wings don have wings course not you re boy</td>\n      <td>[wing, don, wing, course, boy]</td>\n      <td>9</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-0.739118</td>\n      <td>-0.657139</td>\n      <td>-1.282313</td>\n      <td>-0.720612</td>\n      <td>-0.445044</td>\n      <td>0.245276</td>\n      <td>-1.075079</td>\n      <td>-0.345795</td>\n      <td>-1.310360</td>\n      <td>-0.521672</td>\n    </tr>\n    <tr>\n      <th>2671</th>\n      <td>678</td>\n      <td>0</td>\n      <td>#finalsweekgotmelike Ugh. Make it stop. #final...</td>\n      <td>ugh make it stop finals finalsweek ewu</td>\n      <td>[ugh, stop, final, finalsweek, ewu]</td>\n      <td>7</td>\n      <td>32</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-0.728200</td>\n      <td>-0.646012</td>\n      <td>-1.287697</td>\n      <td>-0.713609</td>\n      <td>-0.380788</td>\n      <td>0.231520</td>\n      <td>-1.068907</td>\n      <td>-0.361190</td>\n      <td>-1.338058</td>\n      <td>-0.542515</td>\n    </tr>\n  </tbody>\n</table>\n<p>2672 rows × 315 columns</p>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1447d136",
   "metadata": {
    "id": "1447d136",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg_words_train, neg_word_count_train, pos_words_train, pos_word_count_train = pos_neg_words(df_train, 0.2)\n",
    "neg_words_val, neg_word_count_val, pos_words_val, pos_word_count_val = pos_neg_words(df_validation, 0.2)\n",
    "neg_words_test, neg_word_count_text, pos_words_test, pos_word_count_test = pos_neg_words(df_test, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c4c71a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "5c4c71a3",
    "outputId": "1738c41e-41dc-4738-b858-0d45ce47976b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n0            1226      1  I'm a Victoria Secret model. It's such a secre...   \n1             419      1  Left my lunch at home. Swansea canteen outdoin...   \n2            2061      0  @tweetkumud we are here to fight endlessly and...   \n3             427      1  love, love, love how the left side of my face ...   \n4            2015      1  @MattHDGamer FUT14 ofc, the corners <3___<3 th...   \n...           ...    ...                                                ...   \n2667         1493      1  Welcome to twitter @orafa2 myself n @SlimDandy...   \n2668         2286      0     95% of my clothes are black. #noshame :bomb:\\r   \n2669         2372      0              This pressure is unreal. Keep it up\\r   \n2670          940      0  Wings? I don't have wings! |\"Of course not... ...   \n2671          678      0  #finalsweekgotmelike Ugh. Make it stop. #final...   \n\n                                             clean_text  \\\n0     victoria secret model it such secret not even ...   \n1     left my lunch at home swansea canteen outdoing...   \n2     we are here to fight endlessly and change the ...   \n3     love love love how the left side of my face is...   \n4              fut ofc the corners they where fantastic   \n...                                                 ...   \n2667  welcome to twitter myself at ur service as ur ...   \n2668                    of my clothes are black noshame   \n2669                 this pressure is unreal keep it up   \n2670         wings don have wings course not you re boy   \n2671             ugh make it stop finals finalsweek ewu   \n\n                                                 lemmas  word_count  \\\n0     [victoria, secret, model, secret, victoria, know]          10   \n1     [leave, lunch, home, swansea, canteen, outdo, ...          18   \n2     [fight, endlessly, change, system, atleast, ir...          20   \n3               [love, love, love, left, face, swollen]          12   \n4                         [fut, ofc, corner, fantastic]           7   \n...                                                 ...         ...   \n2667  [welcome, twitter, ur, service, ur, twitter, c...          11   \n2668                           [clothe, black, noshame]           6   \n2669                                 [pressure, unreal]           7   \n2670                     [wing, don, wing, course, boy]           9   \n2671                [ugh, stop, final, finalsweek, ewu]           7   \n\n      char_count  tag_count  hashtag_count  link_count  ...       292  \\\n0             51          0              0           1  ... -1.247315   \n1             91          0              0           1  ... -1.135379   \n2             95          1              1           0  ... -1.158139   \n3             43          0              0           0  ... -1.275515   \n4             34          1              0           0  ... -1.299867   \n...          ...        ...            ...         ...  ...       ...   \n2667          51          2              0           0  ... -1.253360   \n2668          26          0              1           0  ... -1.329347   \n2669          28          0              0           0  ... -1.358828   \n2670          34          0              0           0  ... -1.282313   \n2671          32          0              4           1  ... -1.287697   \n\n           293       294       295       296       297       298       299  \\\n0    -0.715902 -0.472768  0.245063 -1.055144 -0.341575 -1.258775 -0.501747   \n1    -0.688410 -0.585451  0.292726 -0.978633 -0.313849 -1.094880 -0.424406   \n2    -0.682845 -0.588523  0.297747 -1.003470 -0.309303 -1.111962 -0.444621   \n3    -0.717432 -0.334869  0.194307 -1.042801 -0.342240 -1.329015 -0.548489   \n4    -0.733037 -0.440542  0.246756 -1.088922 -0.354721 -1.329997 -0.539491   \n...        ...       ...       ...       ...       ...       ...       ...   \n2667 -0.704631 -0.435985  0.229188 -1.043825 -0.335427 -1.280024 -0.526104   \n2668 -0.741173 -0.402545  0.233630 -1.107008 -0.364313 -1.379796 -0.560553   \n2669 -0.749309 -0.364549  0.220503 -1.125094 -0.373904 -1.429596 -0.581615   \n2670 -0.720612 -0.445044  0.245276 -1.075079 -0.345795 -1.310360 -0.521672   \n2671 -0.713609 -0.380788  0.231520 -1.068907 -0.361190 -1.338058 -0.542515   \n\n      neg_word_count  pos_word_count  \n0                  0               0  \n1                  0               1  \n2                  1               0  \n3                  0               3  \n4                  0               1  \n...              ...             ...  \n2667               0               1  \n2668               0               0  \n2669               1               0  \n2670               0               0  \n2671               2               0  \n\n[2672 rows x 317 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>tag_count</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>...</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n      <th>neg_word_count</th>\n      <th>pos_word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1226</td>\n      <td>1</td>\n      <td>I'm a Victoria Secret model. It's such a secre...</td>\n      <td>victoria secret model it such secret not even ...</td>\n      <td>[victoria, secret, model, secret, victoria, know]</td>\n      <td>10</td>\n      <td>51</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-1.247315</td>\n      <td>-0.715902</td>\n      <td>-0.472768</td>\n      <td>0.245063</td>\n      <td>-1.055144</td>\n      <td>-0.341575</td>\n      <td>-1.258775</td>\n      <td>-0.501747</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>419</td>\n      <td>1</td>\n      <td>Left my lunch at home. Swansea canteen outdoin...</td>\n      <td>left my lunch at home swansea canteen outdoing...</td>\n      <td>[leave, lunch, home, swansea, canteen, outdo, ...</td>\n      <td>18</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-1.135379</td>\n      <td>-0.688410</td>\n      <td>-0.585451</td>\n      <td>0.292726</td>\n      <td>-0.978633</td>\n      <td>-0.313849</td>\n      <td>-1.094880</td>\n      <td>-0.424406</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2061</td>\n      <td>0</td>\n      <td>@tweetkumud we are here to fight endlessly and...</td>\n      <td>we are here to fight endlessly and change the ...</td>\n      <td>[fight, endlessly, change, system, atleast, ir...</td>\n      <td>20</td>\n      <td>95</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.158139</td>\n      <td>-0.682845</td>\n      <td>-0.588523</td>\n      <td>0.297747</td>\n      <td>-1.003470</td>\n      <td>-0.309303</td>\n      <td>-1.111962</td>\n      <td>-0.444621</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>427</td>\n      <td>1</td>\n      <td>love, love, love how the left side of my face ...</td>\n      <td>love love love how the left side of my face is...</td>\n      <td>[love, love, love, left, face, swollen]</td>\n      <td>12</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.275515</td>\n      <td>-0.717432</td>\n      <td>-0.334869</td>\n      <td>0.194307</td>\n      <td>-1.042801</td>\n      <td>-0.342240</td>\n      <td>-1.329015</td>\n      <td>-0.548489</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015</td>\n      <td>1</td>\n      <td>@MattHDGamer FUT14 ofc, the corners &lt;3___&lt;3 th...</td>\n      <td>fut ofc the corners they where fantastic</td>\n      <td>[fut, ofc, corner, fantastic]</td>\n      <td>7</td>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.299867</td>\n      <td>-0.733037</td>\n      <td>-0.440542</td>\n      <td>0.246756</td>\n      <td>-1.088922</td>\n      <td>-0.354721</td>\n      <td>-1.329997</td>\n      <td>-0.539491</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>1493</td>\n      <td>1</td>\n      <td>Welcome to twitter @orafa2 myself n @SlimDandy...</td>\n      <td>welcome to twitter myself at ur service as ur ...</td>\n      <td>[welcome, twitter, ur, service, ur, twitter, c...</td>\n      <td>11</td>\n      <td>51</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.253360</td>\n      <td>-0.704631</td>\n      <td>-0.435985</td>\n      <td>0.229188</td>\n      <td>-1.043825</td>\n      <td>-0.335427</td>\n      <td>-1.280024</td>\n      <td>-0.526104</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>2286</td>\n      <td>0</td>\n      <td>95% of my clothes are black. #noshame :bomb:\\r</td>\n      <td>of my clothes are black noshame</td>\n      <td>[clothe, black, noshame]</td>\n      <td>6</td>\n      <td>26</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.329347</td>\n      <td>-0.741173</td>\n      <td>-0.402545</td>\n      <td>0.233630</td>\n      <td>-1.107008</td>\n      <td>-0.364313</td>\n      <td>-1.379796</td>\n      <td>-0.560553</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>2372</td>\n      <td>0</td>\n      <td>This pressure is unreal. Keep it up\\r</td>\n      <td>this pressure is unreal keep it up</td>\n      <td>[pressure, unreal]</td>\n      <td>7</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.358828</td>\n      <td>-0.749309</td>\n      <td>-0.364549</td>\n      <td>0.220503</td>\n      <td>-1.125094</td>\n      <td>-0.373904</td>\n      <td>-1.429596</td>\n      <td>-0.581615</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>940</td>\n      <td>0</td>\n      <td>Wings? I don't have wings! |\"Of course not... ...</td>\n      <td>wings don have wings course not you re boy</td>\n      <td>[wing, don, wing, course, boy]</td>\n      <td>9</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1.282313</td>\n      <td>-0.720612</td>\n      <td>-0.445044</td>\n      <td>0.245276</td>\n      <td>-1.075079</td>\n      <td>-0.345795</td>\n      <td>-1.310360</td>\n      <td>-0.521672</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2671</th>\n      <td>678</td>\n      <td>0</td>\n      <td>#finalsweekgotmelike Ugh. Make it stop. #final...</td>\n      <td>ugh make it stop finals finalsweek ewu</td>\n      <td>[ugh, stop, final, finalsweek, ewu]</td>\n      <td>7</td>\n      <td>32</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-1.287697</td>\n      <td>-0.713609</td>\n      <td>-0.380788</td>\n      <td>0.231520</td>\n      <td>-1.068907</td>\n      <td>-0.361190</td>\n      <td>-1.338058</td>\n      <td>-0.542515</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2672 rows × 317 columns</p>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['neg_word_count'] = neg_word_count_train\n",
    "#df_train['neg_word'] = neg_words_train\n",
    "df_train['pos_word_count'] = pos_word_count_train\n",
    "#df_train['pos_word'] = pos_words_train\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5ba809d",
   "metadata": {
    "id": "d5ba809d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_validation['neg_word_count'] = neg_word_count_val\n",
    "df_validation['pos_word_count'] = pos_word_count_val\n",
    "df_test['neg_word_count'] = neg_word_count_text\n",
    "df_test['pos_word_count'] = pos_word_count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "['-1_topic',\n '0_topic',\n '1_topic',\n '2_topic',\n '3_topic',\n '4_topic',\n '5_topic',\n '6_topic',\n '7_topic']"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dummies = []\n",
    "for i in range(9):\n",
    "    topic_dummies.append(str(i-1)+'_topic')\n",
    "topic_dummies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "      Tweet index  Label                                         Tweet text  \\\n0            1226      1  I'm a Victoria Secret model. It's such a secre...   \n1             419      1  Left my lunch at home. Swansea canteen outdoin...   \n2            2061      0  @tweetkumud we are here to fight endlessly and...   \n3             427      1  love, love, love how the left side of my face ...   \n4            2015      1  @MattHDGamer FUT14 ofc, the corners <3___<3 th...   \n...           ...    ...                                                ...   \n2667         1493      1  Welcome to twitter @orafa2 myself n @SlimDandy...   \n2668         2286      0     95% of my clothes are black. #noshame :bomb:\\r   \n2669         2372      0              This pressure is unreal. Keep it up\\r   \n2670          940      0  Wings? I don't have wings! |\"Of course not... ...   \n2671          678      0  #finalsweekgotmelike Ugh. Make it stop. #final...   \n\n                                             clean_text  \\\n0     victoria secret model it such secret not even ...   \n1     left my lunch at home swansea canteen outdoing...   \n2     we are here to fight endlessly and change the ...   \n3     love love love how the left side of my face is...   \n4              fut ofc the corners they where fantastic   \n...                                                 ...   \n2667  welcome to twitter myself at ur service as ur ...   \n2668                    of my clothes are black noshame   \n2669                 this pressure is unreal keep it up   \n2670         wings don have wings course not you re boy   \n2671             ugh make it stop finals finalsweek ewu   \n\n                                                 lemmas  word_count  \\\n0     [victoria, secret, model, secret, victoria, know]          10   \n1     [leave, lunch, home, swansea, canteen, outdo, ...          18   \n2     [fight, endlessly, change, system, atleast, ir...          20   \n3               [love, love, love, left, face, swollen]          12   \n4                         [fut, ofc, corner, fantastic]           7   \n...                                                 ...         ...   \n2667  [welcome, twitter, ur, service, ur, twitter, c...          11   \n2668                           [clothe, black, noshame]           6   \n2669                                 [pressure, unreal]           7   \n2670                     [wing, don, wing, course, boy]           9   \n2671                [ugh, stop, final, finalsweek, ewu]           7   \n\n      char_count  tag_count  hashtag_count  link_count  ...  pos_word_count  \\\n0             51          0              0           1  ...               0   \n1             91          0              0           1  ...               1   \n2             95          1              1           0  ...               0   \n3             43          0              0           0  ...               3   \n4             34          1              0           0  ...               1   \n...          ...        ...            ...         ...  ...             ...   \n2667          51          2              0           0  ...               1   \n2668          26          0              1           0  ...               0   \n2669          28          0              0           0  ...               0   \n2670          34          0              0           0  ...               0   \n2671          32          0              4           1  ...               0   \n\n      -1_topic  0_topic 1_topic  2_topic  3_topic  4_topic  5_topic  6_topic  \\\n0            0        1       0        0        0        0        0        0   \n1            1        0       0        0        0        0        0        0   \n2            1        0       0        0        0        0        0        0   \n3            1        0       0        0        0        0        0        0   \n4            1        0       0        0        0        0        0        0   \n...        ...      ...     ...      ...      ...      ...      ...      ...   \n2667         0        0       0        0        0        0        0        0   \n2668         0        1       0        0        0        0        0        0   \n2669         1        0       0        0        0        0        0        0   \n2670         1        0       0        0        0        0        0        0   \n2671         1        0       0        0        0        0        0        0   \n\n      7_topic  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n...       ...  \n2667        1  \n2668        0  \n2669        0  \n2670        0  \n2671        0  \n\n[2672 rows x 326 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet index</th>\n      <th>Label</th>\n      <th>Tweet text</th>\n      <th>clean_text</th>\n      <th>lemmas</th>\n      <th>word_count</th>\n      <th>char_count</th>\n      <th>tag_count</th>\n      <th>hashtag_count</th>\n      <th>link_count</th>\n      <th>...</th>\n      <th>pos_word_count</th>\n      <th>-1_topic</th>\n      <th>0_topic</th>\n      <th>1_topic</th>\n      <th>2_topic</th>\n      <th>3_topic</th>\n      <th>4_topic</th>\n      <th>5_topic</th>\n      <th>6_topic</th>\n      <th>7_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1226</td>\n      <td>1</td>\n      <td>I'm a Victoria Secret model. It's such a secre...</td>\n      <td>victoria secret model it such secret not even ...</td>\n      <td>[victoria, secret, model, secret, victoria, know]</td>\n      <td>10</td>\n      <td>51</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>419</td>\n      <td>1</td>\n      <td>Left my lunch at home. Swansea canteen outdoin...</td>\n      <td>left my lunch at home swansea canteen outdoing...</td>\n      <td>[leave, lunch, home, swansea, canteen, outdo, ...</td>\n      <td>18</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2061</td>\n      <td>0</td>\n      <td>@tweetkumud we are here to fight endlessly and...</td>\n      <td>we are here to fight endlessly and change the ...</td>\n      <td>[fight, endlessly, change, system, atleast, ir...</td>\n      <td>20</td>\n      <td>95</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>427</td>\n      <td>1</td>\n      <td>love, love, love how the left side of my face ...</td>\n      <td>love love love how the left side of my face is...</td>\n      <td>[love, love, love, left, face, swollen]</td>\n      <td>12</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015</td>\n      <td>1</td>\n      <td>@MattHDGamer FUT14 ofc, the corners &lt;3___&lt;3 th...</td>\n      <td>fut ofc the corners they where fantastic</td>\n      <td>[fut, ofc, corner, fantastic]</td>\n      <td>7</td>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>1493</td>\n      <td>1</td>\n      <td>Welcome to twitter @orafa2 myself n @SlimDandy...</td>\n      <td>welcome to twitter myself at ur service as ur ...</td>\n      <td>[welcome, twitter, ur, service, ur, twitter, c...</td>\n      <td>11</td>\n      <td>51</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>2286</td>\n      <td>0</td>\n      <td>95% of my clothes are black. #noshame :bomb:\\r</td>\n      <td>of my clothes are black noshame</td>\n      <td>[clothe, black, noshame]</td>\n      <td>6</td>\n      <td>26</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2669</th>\n      <td>2372</td>\n      <td>0</td>\n      <td>This pressure is unreal. Keep it up\\r</td>\n      <td>this pressure is unreal keep it up</td>\n      <td>[pressure, unreal]</td>\n      <td>7</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>940</td>\n      <td>0</td>\n      <td>Wings? I don't have wings! |\"Of course not... ...</td>\n      <td>wings don have wings course not you re boy</td>\n      <td>[wing, don, wing, course, boy]</td>\n      <td>9</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2671</th>\n      <td>678</td>\n      <td>0</td>\n      <td>#finalsweekgotmelike Ugh. Make it stop. #final...</td>\n      <td>ugh make it stop finals finalsweek ewu</td>\n      <td>[ugh, stop, final, finalsweek, ewu]</td>\n      <td>7</td>\n      <td>32</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2672 rows × 326 columns</p>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[topic_dummies] = pd.get_dummies(df_train['topic'])\n",
    "df_validation[topic_dummies] = pd.get_dummies(df_validation['topic'])\n",
    "df_test[topic_dummies] = pd.get_dummies(df_test['topic'])\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f49072c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f49072c8",
    "outputId": "0f34e424-9c9a-4c8c-ac9d-379d9f001dec",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score\n",
      "0.6347305389221557\n",
      "Validation score\n",
      "0.611353711790393\n",
      "Test score\n",
      "0.6033163265306123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_ = df_train[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "               '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "y_ = df_train['Label']\n",
    "clf = LogisticRegression(random_state=0, solver='liblinear').fit(X_, y_)\n",
    "print('Train score')\n",
    "print(clf.score(X_, y_))\n",
    "print('Validation score')\n",
    "X_val = df_validation[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "               '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "y_val = df_validation['Label']\n",
    "print(clf.score(X_val, y_val))\n",
    "print('Test score')\n",
    "X_val = df_test[['word_count', 'char_count', 'tag_count', 'hashtag_count', 'link_count', 'smiley_count', 'mark_count', 'has_emoji', 'neg_word_count', 'pos_word_count',\n",
    "               '-1_topic', '0_topic', '1_topic', '2_topic', '3_topic', '4_topic', '5_topic', '6_topic', '7_topic']]\n",
    "y_val = df_test['Label']\n",
    "print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4718f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9f4718f",
    "outputId": "f53f55da-c8b7-426a-ef27-c85cc0f0b1b4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Pos and neg words within 4 words\n",
    "df_train[['clean_text', 'pos_word', 'pos_word_count', 'neg_word', 'neg_word_count']].iloc[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf4766",
   "metadata": {
    "id": "fadf4766",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pos_neg_within_n(df, n=4):\n",
    "\n",
    "    ret_array = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['pos_word_count'] > 0 and row['neg_word_count'] > 0:\n",
    "            doc = nlp(row['clean_text'])\n",
    "            lemmas = []\n",
    "            for token in doc:\n",
    "                lemmas.append(token.lemma_)\n",
    "\n",
    "            pos_indexes = np.array([])\n",
    "            for word in row['pos_word']:\n",
    "                pos_indexes = np.append(pos_indexes, np.where(np.array(lemmas) == word))\n",
    "            neg_indexes = np.array([])\n",
    "            for word in row['neg_word']:\n",
    "                neg_indexes = np.append(neg_indexes, np.where(np.array(lemmas) == word))\n",
    "\n",
    "            bool_val = 0\n",
    "            for idx in pos_indexes:\n",
    "                if (abs(neg_indexes-idx) < n).any():\n",
    "                    bool_val = 1\n",
    "            ret_array.append(copy.copy(bool_val))\n",
    "        else:\n",
    "            ret_array.append(0)\n",
    "    return ret_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9367e01",
   "metadata": {
    "id": "a9367e01",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "within_5_train = pos_neg_within_n(df_train, n=5)\n",
    "within_5_val = pos_neg_within_n(df_validation, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f78a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "071f78a5",
    "outputId": "3ba0ff1a-9b49-4143-e83c-c112a67c5c5c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train['pos_neg_within_5'] = within_5_train\n",
    "df_validation['pos_neg_within_5'] = within_5_val\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a917861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a917861",
    "outputId": "38f3877f-6bef-45bf-cb9d-c1ec2f433743",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_ = df_train[[ 'pos_neg_within_5']]\n",
    "y_ = df_train['Label']\n",
    "clf = LogisticRegression().fit(X_, y_)\n",
    "print('Train score')\n",
    "print(clf.score(X_, y_))\n",
    "print('Validation score')\n",
    "X_val = df_validation[[ 'pos_neg_within_5']]\n",
    "y_val = df_validation['Label']\n",
    "print(clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2617de",
   "metadata": {
    "id": "5e2617de",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "within_5_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HJvVNgSQGXBE",
   "metadata": {
    "id": "HJvVNgSQGXBE"
   },
   "source": [
    "### Grid search\n",
    "\n",
    "1. treba koristit tweet_text_train, tweet_text_validation i tweet_text_test s kombiniranim znacajkama iz df_train, df_validation, df_test, a ne da samo koristi embeddinge - ako ne radi bolje tako onda cemo odjebat embeddinge\n",
    "2. grid mora koristit minimalno feature selekciju, skaliranje i model (za dtc ne treba skalirat, lr moze i polynomial transform)\n",
    "3. pazi na data leakove, nemres fitat search i onda evaluirat na njemu test, treba natrenirat novi model s tim najboljim hiperparametrima na trainu i onda predictat na testu\n",
    "4. povecat broj parametara, ovo je jako slabo, grid se vrti po 30ak sekundi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "393bd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = df_train.drop(columns=['Tweet index', 'Label', 'Tweet text', 'clean_text'], axis=1)\n",
    "y_ = df_train['Label']\n",
    "\n",
    "X_val = df_validation.drop(['Tweet index', 'Label', 'Tweet text', 'clean_text'], axis=1)\n",
    "y_val = df_validation['Label']\n",
    "\n",
    "X_test = df_test.drop(['Tweet index', 'Label', 'Tweet text'], axis=1)\n",
    "y_test = df_test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "y7QInQCTGXF9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7QInQCTGXF9",
    "outputId": "09832e27-4753-481a-c53a-9ff38ae7507b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6051566383142568\n",
      "Best score: {'lr__C': 0.08858667904100823, 'lr__penalty': 'l2', 'lr__solver': 'newton-cg', 'selection__k': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('selection', SelectKBest()), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'selection__k': list(range(2, 20, 3)),\n",
    "    'lr__penalty': ['l1', 'l2', 'elasticnet'], \n",
    "    'lr__C': np.logspace(-4, 4, 20), \n",
    "    'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipeline_lr, param_grid=params, cv=5)\n",
    "search.fit(X_, y_)\n",
    "\n",
    "print(f'Best score: {search.best_score_}')\n",
    "print(f'Best score: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "9c94000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.64      0.57      1073\n",
      "           1       0.71      0.58      0.64      1599\n",
      "\n",
      "    accuracy                           0.60      2672\n",
      "   macro avg       0.61      0.61      0.60      2672\n",
      "weighted avg       0.63      0.60      0.61      2672\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.63      0.58       467\n",
      "           1       0.71      0.62      0.66       678\n",
      "\n",
      "    accuracy                           0.62      1145\n",
      "   macro avg       0.62      0.63      0.62      1145\n",
      "weighted avg       0.64      0.62      0.63      1145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr_best = Pipeline([\n",
    "    ('selection', SelectKBest(k=8)), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(C=0.0885, penalty='l2', solver='newton-cg'))\n",
    "])\n",
    "\n",
    "pipeline_lr_best.fit(X_, y_)\n",
    "y_train_pred = pipeline_lr_best.predict(X_)\n",
    "print(classification_report(y_train_pred, y_))\n",
    "\n",
    "y_validation_pred = pipeline_lr_best.predict(X_val)\n",
    "print(classification_report(y_validation_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w7iw-WmyGXKC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7iw-WmyGXKC",
    "outputId": "89cd15f7-2fa4-4922-ce48-c55af7211d87"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "params = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'ccp_alpha': [0.1, .01, .001],\n",
    "              'max_depth' : [5, 6, 7, 8, 9],\n",
    "              'criterion' : ['gini', 'entropy', 'log_loss']\n",
    "             }\n",
    "\n",
    "\n",
    "X_ = df_train.drop(['Tweet index', 'Label', 'Tweet text', 'clean_text', 'pos_word', 'neg_word'], axis=1)\n",
    "y_ = df_train['Label']\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "model = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "\n",
    "model.fit(X_, y_)\n",
    "\n",
    "print('Train score')\n",
    "print(model.score(X_, y_))\n",
    "print('Validation score')\n",
    "X_val = df_validation.drop(['Tweet index', 'Label', 'Tweet text', 'clean_text', 'pos_word', 'neg_word'], axis=1)\n",
    "y_val = df_validation['Label']\n",
    "print(model.score(X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}